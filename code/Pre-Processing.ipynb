{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "\n",
        "# local dirve path to  JSON.\n",
        "CLIENT_JSON = \"client_secret_Final.json\"   #Google Dirve API\n",
        "gauth = GoogleAuth()\n",
        "gauth.LoadClientConfigFile(CLIENT_JSON)\n",
        "\n",
        "# Command-line auth -> \n",
        "gauth.CommandLineAuth()\n",
        "\n",
        "# Save token \n",
        "gauth.SaveCredentialsFile('token.json')\n",
        "\n",
        "drive = GoogleDrive(gauth)\n",
        "print(\"‚úÖ Google Drive connected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The default GEE tif files are mutiple tiles per country\n",
        "Thsi code help to mosaic multiple tif rasters for a given country to have a single file per each varaiable for a country\n",
        "\"\"\"\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.io import MemoryFile\n",
        "from rasterio.shutil import copy as rio_copy\n",
        "from rasterio.warp import reproject, Resampling as WarpResampling\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Google Drive setup\n",
        "# -------------------------------------------------------------------\n",
        "# NOTE: 'drive' must be an authenticated PyDrive2 client in your session.\n",
        "ROOT_FOLDER_ID = \"18pQKnMMnLramhHRZSNwUJrLqG5DXNMmS\"  # folder id from gogole dirve\n",
        "\n",
        "# Variables \n",
        "VARIABLES = [\n",
        "    \"NDVI_mean\", \"NDVI_max\", \"NDVI_min\",\n",
        "    \"NDWI_mean\", \"NDWI_max\", \"NDWI_min\",\n",
        "    \"GI_mean\", \"GI_max\", \"GI_min\",\n",
        "    \"elevation\", \"slope\",\n",
        "    \"ET\", \"PET\",\n",
        "\n",
        "]\n",
        "\n",
        "# Provinces to process\n",
        "PROVINCES = [\n",
        "    \"Philippines\",\n",
        "    # ...\n",
        "]\n",
        "\n",
        "# Optional synonyms (case-insensitive)\n",
        "COUNTRY_SYNONYMS = {\n",
        "    \"Philippines\": \"philippines\",\n",
        "}\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Helpers\n",
        "# -------------------------------------------------------------------\n",
        "VARIABLE_TOKENS = {v.lower(): v for v in VARIABLES}  # canonicalize\n",
        "\n",
        "\n",
        "def _squash_spaces(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "\n",
        "def normalize_country(name: str) -> str:\n",
        "    \"\"\"Normalize 'Africa_Inner_Mongolia' -> 'Africa Inner Mongolia', apply synonyms.\"\"\"\n",
        "    if not name:\n",
        "        return \"\"\n",
        "    n = name.strip().replace(\"_\", \" \")\n",
        "    n = _squash_spaces(n)\n",
        "    lower_syn = {k.lower(): v for k, v in COUNTRY_SYNONYMS.items()}\n",
        "    if n.lower() in lower_syn:\n",
        "        n = lower_syn[n.lower()]\n",
        "    return n\n",
        "\n",
        "\n",
        "def _safe_basename(s: str) -> str:\n",
        "    \"\"\"ASCII slug for filenames.\"\"\"\n",
        "    norm = unicodedata.normalize(\"NFKD\", s)\n",
        "    ascii_only = norm.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    slug = re.sub(r\"[^A-Za-z0-9]+\", \"_\", ascii_only).strip(\"_\")\n",
        "    return slug or \"untitled\"\n",
        "\n",
        "\n",
        "def _drive_q_escape(s: str) -> str:\n",
        "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Drive helpers\n",
        "# -------------------------------------------------------------------\n",
        "def get_or_create_folder(drive, parent_id, name):\n",
        "    safe = _drive_q_escape(name)\n",
        "    q = (\n",
        "        f\"'{parent_id}' in parents and trashed=false and \"\n",
        "        f\"mimeType='application/vnd.google-apps.folder' and title='{safe}'\"\n",
        "    )\n",
        "    res = drive.ListFile({'q': q}).GetList()\n",
        "    if res:\n",
        "        return res[0]['id']\n",
        "    f = drive.CreateFile({\n",
        "        'title': name,\n",
        "        'parents': [{'id': parent_id}],\n",
        "        'mimeType': 'application/vnd.google-apps.folder'\n",
        "    })\n",
        "    f.Upload()\n",
        "    return f['id']\n",
        "\n",
        "\n",
        "def list_all_tifs_recursive(drive, root_id):\n",
        "    \"\"\"Recursively list all .tif/.tiff under root_id.\"\"\"\n",
        "    tifs, stack = [], [root_id]\n",
        "    while stack:\n",
        "        folder_id = stack.pop()\n",
        "        q = f\"'{folder_id}' in parents and trashed=false\"\n",
        "        for it in drive.ListFile({'q': q}).GetList():\n",
        "            mime = it.get('mimeType', '')\n",
        "            if mime == 'application/vnd.google-apps.folder':\n",
        "                stack.append(it['id'])\n",
        "            else:\n",
        "                title = it.get('title', '')\n",
        "                if title and title.lower().endswith(('.tif', '.tiff')):\n",
        "                    tifs.append(it)\n",
        "    return tifs\n",
        "\n",
        "\n",
        "def download_many(drive, files, dst_dir, max_workers=8):\n",
        "    \"\"\"Download Drive files concurrently to dst_dir, return local paths.\"\"\"\n",
        "\n",
        "    def _dl(f):\n",
        "        local = os.path.join(dst_dir, f['title'])\n",
        "        f.GetContentFile(local)\n",
        "        return local\n",
        "\n",
        "    paths = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futs = [ex.submit(_dl, f) for f in files]\n",
        "        for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Downloading\"):\n",
        "            paths.append(fut.result())\n",
        "    return paths\n",
        "\n",
        "\n",
        "def upload_tif(drive, local_path, parent_id, title):\n",
        "    f = drive.CreateFile({'title': title, 'parents': [{'id': parent_id}]})\n",
        "    f.SetContentFile(local_path)\n",
        "    f.Upload()\n",
        "    return f['id']\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Parsing\n",
        "# -------------------------------------------------------------------\n",
        "def _filename_core(title: str) -> str:\n",
        "    \"\"\"\n",
        "    Take a Drive title and return the 'logical' base name:\n",
        "\n",
        "    e.g.\n",
        "    'New_York_GI_max-0000000000-0000000000.tif' -> 'New_York_GI_max'\n",
        "    \"\"\"\n",
        "    base = os.path.splitext(os.path.basename(title))[0]\n",
        "    # Drop anything after the first '-' (tile ID, date, etc.)\n",
        "    if '-' in base:\n",
        "        base = base.split('-', 1)[0]\n",
        "    return base\n",
        "\n",
        "\n",
        "def _split_tokens(path_title: str):\n",
        "    \"\"\"\n",
        "    Split the core filename into '_' tokens.\n",
        "\n",
        "    'New_York_GI_max-0000000000-0000000000.tif'\n",
        "      -> core 'New_York_GI_max'\n",
        "      -> ['New', 'York', 'GI', 'max']\n",
        "    \"\"\"\n",
        "    core = _filename_core(path_title)\n",
        "    return [t for t in core.split('_') if t]\n",
        "\n",
        "\n",
        "def parse_title(title: str):\n",
        "    \"\"\"\n",
        "    Return (country_string_like_'Africa_Anhui', canonical_variable) or (None, None)\n",
        "    \"\"\"\n",
        "    if not title.lower().endswith(('.tif', '.tiff')):\n",
        "        return None, None\n",
        "\n",
        "    toks = _split_tokens(title)\n",
        "    if not toks:\n",
        "        return None, None\n",
        "\n",
        "    lower = [t.lower() for t in toks]\n",
        "\n",
        "    # Match country: pick the longest province whose tokenized name matches the start\n",
        "    best_country = None\n",
        "    start_len = 0\n",
        "    for prov in PROVINCES:\n",
        "        ptoks = prov.split('_')\n",
        "        if len(ptoks) <= len(lower) and [t.lower() for t in ptoks] == lower[:len(ptoks)]:\n",
        "            if len(ptoks) > start_len:\n",
        "                best_country = prov\n",
        "                start_len = len(ptoks)\n",
        "\n",
        "    if not best_country:\n",
        "        return None, None\n",
        "\n",
        "    # Find variable starting right after the country tokens\n",
        "    best = None  # (i, j, canonical_var)\n",
        "    for i in range(start_len, min(start_len + 4, len(lower))):\n",
        "        for j in range(i, min(i + 3, len(lower))):\n",
        "            cand = '_'.join(lower[i:j + 1])\n",
        "            if cand in VARIABLE_TOKENS:\n",
        "                # prefer longer match\n",
        "                if best is None or (j - i) > (best[1] - best[0]):\n",
        "                    best = (i, j, VARIABLE_TOKENS[cand])\n",
        "\n",
        "    if not best:\n",
        "        return None, None\n",
        "\n",
        "    _, _, variable = best\n",
        "    return best_country, variable\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Size helpers / dtype\n",
        "# -------------------------------------------------------------------\n",
        "def _bytes_per_pixel(dtype):\n",
        "    return {\n",
        "        \"uint8\": 1, \"int8\": 1,\n",
        "        \"uint16\": 2, \"int16\": 2,\n",
        "        \"uint32\": 4, \"int32\": 4,\n",
        "        \"float32\": 4, \"float64\": 8\n",
        "    }.get(dtype, 4)\n",
        "\n",
        "\n",
        "def _should_bigtiff(width, height, count, dtype):\n",
        "    est = width * height * count * _bytes_per_pixel(dtype) * 1.05\n",
        "    return est >= (4 * 1024 ** 3)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Grid/reference helpers\n",
        "# -------------------------------------------------------------------\n",
        "def _pick_reference(datasets):\n",
        "    # Choose the most frequent (xres, yres); break ties by largest pixel area\n",
        "    reslist = [(ds.transform.a, -ds.transform.e) for ds in datasets]\n",
        "    counts = Counter(reslist)\n",
        "    best_res = max(counts.items(), key=lambda kv: (kv[1], kv[0][0] * kv[0][1]))[0]\n",
        "    for ds in datasets:\n",
        "        if (ds.transform.a, -ds.transform.e) == best_res:\n",
        "            return ds\n",
        "    return datasets[0]\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# COG writer\n",
        "# -------------------------------------------------------------------\n",
        "def write_cog_from_array(mosaic, profile, cog_path):\n",
        "    force_bigtiff = _should_bigtiff(\n",
        "        width=profile[\"width\"],\n",
        "        height=profile[\"height\"],\n",
        "        count=profile[\"count\"],\n",
        "        dtype=profile[\"dtype\"]\n",
        "    )\n",
        "\n",
        "    src_profile = profile.copy()\n",
        "    src_profile.update({\n",
        "        \"driver\": \"GTiff\",\n",
        "        \"tiled\": True,\n",
        "        \"blockxsize\": profile.get(\"blockxsize\", 512),\n",
        "        \"blockysize\": profile.get(\"blockysize\", 512),\n",
        "        \"compress\": profile.get(\"compress\", \"LZW\"),\n",
        "        \"predictor\": profile.get(\"predictor\", 3),\n",
        "        \"interleave\": \"band\",\n",
        "        \"BIGTIFF\": \"YES\" if force_bigtiff else \"IF_SAFER\",\n",
        "    })\n",
        "\n",
        "    with MemoryFile() as memfile:\n",
        "        with memfile.open(**src_profile) as tmp:\n",
        "            tmp.write(mosaic)\n",
        "        with memfile.open() as src_ds:\n",
        "            rio_copy(\n",
        "                src_ds,\n",
        "                cog_path,\n",
        "                driver=\"COG\",\n",
        "                COMPRESS=\"LZW\",\n",
        "                PREDICTOR=src_profile[\"predictor\"],\n",
        "                BLOCKSIZE=512,\n",
        "                OVERVIEW_LEVELS=\"2,4,8,16\",\n",
        "                OVERVIEW_RESAMPLING=\"AVERAGE\",\n",
        "                NUM_THREADS=\"ALL_CPUS\",\n",
        "                BIGTIFF=\"YES\" if force_bigtiff else \"IF_SAFER\",\n",
        "                RESAMPLING=\"NEAREST\",\n",
        "                DST_NODATA=profile.get(\"nodata\", None),\n",
        "            )\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Mosaic core\n",
        "# -------------------------------------------------------------------\n",
        "def _reducer_for_variable(var: str):\n",
        "    \"\"\"Pick a per-pixel reducer.\"\"\"\n",
        "    vl = var.lower()\n",
        "    if vl.endswith(\"_min\"):\n",
        "        return np.fmin\n",
        "    # default to fmax for max/mean/masks, etc.\n",
        "    return np.fmax\n",
        "\n",
        "\n",
        "def mosaic_files_to_array(datasets, reducer, default_nodata=-9999.0):\n",
        "    \"\"\"\n",
        "    Mosaic single-band rasters to float32 using NaN as working nodata.\n",
        "    Handles mixed CRS and pixel sizes (aligned to common grid).\n",
        "    \"\"\"\n",
        "    # Common CRS: take the first non-None\n",
        "    common_crs = next((ds.crs for ds in datasets if ds.crs is not None), None)\n",
        "    if common_crs is None:\n",
        "        raise RuntimeError(\n",
        "            \"None of the input tiles has a CRS defined; cannot reproject to a common grid.\"\n",
        "        )\n",
        "\n",
        "    ref = _pick_reference(datasets)\n",
        "    ref_transform = ref.transform\n",
        "    ref_res = (ref_transform.a, -ref_transform.e)\n",
        "\n",
        "    # Union bounds in common CRS\n",
        "    from rasterio.warp import transform_bounds\n",
        "    minx = miny = float(\"inf\")\n",
        "    maxx = maxy = float(\"-inf\")\n",
        "    for ds in datasets:\n",
        "        b = ds.bounds\n",
        "        if ds.crs is not None and ds.crs != common_crs:\n",
        "            b = transform_bounds(ds.crs, common_crs, *b, densify_pts=21)\n",
        "        minx, miny = min(minx, b[0]), min(miny, b[1])\n",
        "        maxx, maxy = max(maxx, b[2]), max(maxy, b[3])\n",
        "\n",
        "    # Destination grid at the reference resolution\n",
        "    from rasterio.transform import from_origin\n",
        "    dst_transform = from_origin(minx, maxy, ref_res[0], ref_res[1])\n",
        "    dst_w = int(np.ceil((maxx - minx) / ref_res[0]))\n",
        "    dst_h = int(np.ceil((maxy - miny) / ref_res[1]))\n",
        "\n",
        "    acc = np.full((dst_h, dst_w), np.nan, dtype=np.float32)\n",
        "\n",
        "    for ds in datasets:\n",
        "        temp = np.full((dst_h, dst_w), np.nan, dtype=np.float32)\n",
        "        src_nodata = ds.nodata\n",
        "        reproject(\n",
        "            source=rasterio.band(ds, 1),\n",
        "            destination=temp,\n",
        "            src_transform=ds.transform,\n",
        "            src_crs=(ds.crs if ds.crs is not None else common_crs),\n",
        "            dst_transform=dst_transform,\n",
        "            dst_crs=common_crs,\n",
        "            src_nodata=src_nodata,\n",
        "            dst_nodata=np.nan,\n",
        "            resampling=WarpResampling.nearest,\n",
        "        )\n",
        "        if np.isnan(acc).all():\n",
        "            acc = temp\n",
        "        else:\n",
        "            acc = reducer(acc, temp)  # NaN-aware fmin/fmax\n",
        "\n",
        "    mosaic = np.where(np.isnan(acc), default_nodata, acc).astype(np.float32)[None, ...]\n",
        "    return mosaic, dst_transform, common_crs, float(default_nodata)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Country + variable mosaicking and upload\n",
        "# -------------------------------------------------------------------\n",
        "def mosaic_country_variable_to_drive(drive, parent_id, country, variable, by_country_id):\n",
        "    \"\"\"\n",
        "    Find all tiles for (country, variable), mosaic, write COG, upload.\n",
        "    \"\"\"\n",
        "    tifs = list_all_tifs_recursive(drive, parent_id)\n",
        "\n",
        "    selected = []\n",
        "    c_norm = country  # already province token form\n",
        "    v_norm = variable.lower()\n",
        "\n",
        "    for it in tifs:\n",
        "        title = it.get('title', '')\n",
        "        ctry, var = parse_title(title)\n",
        "        if ctry is None:\n",
        "            continue\n",
        "        if ctry == c_norm and var.lower() == v_norm:\n",
        "            selected.append(it)\n",
        "\n",
        "    if not selected:\n",
        "        print(f\"‚ö†Ô∏è No files for {country} / {variable}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n‚ñ∂Ô∏è {country} / {variable} | {len(selected)} file(s)\")\n",
        "\n",
        "    tmpdir = tempfile.mkdtemp(\n",
        "        prefix=f\"{_safe_basename(country)}_{_safe_basename(variable)}_\"\n",
        "    )\n",
        "    local_paths, datasets = [], []\n",
        "    try:\n",
        "        # Download tiles\n",
        "        local_paths = download_many(drive, selected, tmpdir, max_workers=8)\n",
        "\n",
        "        # Open datasets (require single band)\n",
        "        for p in local_paths:\n",
        "            ds = rasterio.open(p)\n",
        "            if ds.count != 1:\n",
        "                raise RuntimeError(\n",
        "                    f\"Only single-band rasters supported. \"\n",
        "                    f\"{os.path.basename(p)} has {ds.count} bands.\"\n",
        "                )\n",
        "            datasets.append(ds)\n",
        "\n",
        "        reducer = _reducer_for_variable(variable)\n",
        "\n",
        "        # Mosaic to float32\n",
        "        mosaic, out_transform, out_crs, out_nodata = mosaic_files_to_array(\n",
        "            datasets, reducer=reducer\n",
        "        )\n",
        "\n",
        "        profile = {\n",
        "            \"dtype\": \"float32\",\n",
        "            \"height\": mosaic.shape[1],\n",
        "            \"width\": mosaic.shape[2],\n",
        "            \"count\": 1,\n",
        "            \"transform\": out_transform,\n",
        "            \"crs\": out_crs,\n",
        "            \"nodata\": out_nodata,\n",
        "            \"blockxsize\": 512,\n",
        "            \"blockysize\": 512,\n",
        "            \"compress\": \"LZW\",\n",
        "            \"predictor\": 3,\n",
        "        }\n",
        "\n",
        "        # Output folder + filename\n",
        "        display_country = normalize_country(country)\n",
        "        country_folder_id = get_or_create_folder(drive, by_country_id, display_country)\n",
        "\n",
        "        safe_country = _safe_basename(display_country)\n",
        "        safe_variable = _safe_basename(variable)\n",
        "        out_name = f\"{safe_country}_{safe_variable}.tif\"\n",
        "        cog_local = os.path.join(tmpdir, out_name)\n",
        "\n",
        "        print(\"   ‚Ä¢ writing COG ‚Ä¶\")\n",
        "        write_cog_from_array(mosaic, profile, cog_local)\n",
        "\n",
        "        print(f\"   ‚Ä¢ uploading to Drive as {out_name} ‚Ä¶\")\n",
        "        out_id = upload_tif(drive, cog_local, country_folder_id, out_name)\n",
        "        print(f\"‚úÖ Uploaded: {out_name} (file id: {out_id})\")\n",
        "\n",
        "    finally:\n",
        "        for ds in datasets:\n",
        "            try:\n",
        "                ds.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "        shutil.rmtree(tmpdir, ignore_errors=True)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Batch runner\n",
        "# -------------------------------------------------------------------\n",
        "def run_all_provinces_variables(drive):\n",
        "    by_country_id = get_or_create_folder(drive, ROOT_FOLDER_ID, \"By Country\")\n",
        "    # Ensure country folders exist\n",
        "    for c in PROVINCES:\n",
        "        get_or_create_folder(drive, by_country_id, normalize_country(c))\n",
        "\n",
        "    for c in PROVINCES:\n",
        "        for v in VARIABLES:\n",
        "            try:\n",
        "                mosaic_country_variable_to_drive(drive, ROOT_FOLDER_ID, c, v, by_country_id)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå {c} / {v}: {e}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Execute\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_provinces_variables(drive)\n",
        "    print(\"\\nüéâ Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This script precomputes Agro-Ecological Zone (AEZ) data by:\n",
        "1) generating per-state/country AEZ raster tiles aligned to existing state grids, and\n",
        "2) extracting the corresponding AEZ pixel values for each ground-truth training point.\n",
        "The AEZ value is added as a new column in the CSV, alongside the existing predictor values,\n",
        "for each corresponding GTPS.\n",
        "\"\"\"\n",
        "\n",
        "import os, re, json, math, tempfile, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import rasterio\n",
        "from rasterio.enums import Resampling as RioResampling\n",
        "from rasterio.warp import reproject, transform as rio_transform, transform_bounds as rio_transform_bounds\n",
        "from rasterio.transform import Affine\n",
        "from rasterio.crs import CRS\n",
        "\n",
        "try:\n",
        "    from googleapiclient.errors import HttpError\n",
        "except Exception:\n",
        "    HttpError = Exception\n",
        "try:\n",
        "    from pydrive2.files import ApiRequestError\n",
        "except Exception:\n",
        "    ApiRequestError = Exception\n",
        "\n",
        "# ---------- reuse your config ----------\n",
        "ROOT_FOLDER_ID = os.environ.get(\"ROOT_FOLDER_ID\", \"1hqMIyDYEFKnpS8KLxC4bqHmF_9dHXImG\")\n",
        "BY_COUNTRY_NAME = \"By Country\"\n",
        "MODEL_FOLDER = \"Model Training\"\n",
        "LOCAL_BASE_DIR = os.path.join(\"./\", \"US\")\n",
        "LOCAL_BY_COUNTRY_DIR = os.path.join(LOCAL_BASE_DIR, \"By Country\")\n",
        "LOCAL_MODEL_DIR = os.path.join(LOCAL_BASE_DIR, MODEL_FOLDER)\n",
        "\n",
        "AEZ_FILE_ID   = os.environ.get(\"AEZ_FILE_ID\", \"1te3nKn8vyt2AECmk8NM_xBgpEFXp84RX\")\n",
        "AEZ_FILE_NAME = os.environ.get(\"AEZ_FILE_NAME\", \"AEZ_2020s.tif\")\n",
        "\n",
        "POINTS_BASENAME = \"US\"\n",
        "LONCOL = \"longitude\"; LATCOL = \"latitude\"\n",
        "AEZ_COL = \"AEZ\"\n",
        "\n",
        "DRIVE_MAX_RETRIES = int(os.environ.get(\"DRIVE_MAX_RETRIES\", 6))\n",
        "DRIVE_RETRY_BASE  = float(os.environ.get(\"DRIVE_RETRY_BASE\", 0.8))\n",
        "\n",
        "# ---------- Drive helpers (same behavior as your main file) ----------\n",
        "def _drive_retry(callable_fn, *args, **kwargs):\n",
        "    last_err = None\n",
        "    for i in range(DRIVE_MAX_RETRIES):\n",
        "        try:\n",
        "            return callable_fn(*args, **kwargs)\n",
        "        except (HttpError, ApiRequestError) as e:\n",
        "            last_err = e\n",
        "            code = getattr(getattr(e, \"resp\", None), \"status\", None)\n",
        "            if code is None:\n",
        "                msg = str(e).lower()\n",
        "                transient = any(k in msg for k in [\"internal error\",\"backenderror\",\"rate limit\",\"timeout\"])\n",
        "            else:\n",
        "                transient = 500 <= int(code) < 600 or int(code) in (403, 429)\n",
        "            if not transient or i == DRIVE_MAX_RETRIES - 1:\n",
        "                break\n",
        "            time.sleep(DRIVE_RETRY_BASE * (2 ** i) + random.random() * 0.2)\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            break\n",
        "    raise last_err\n",
        "\n",
        "def _drive_query(drive, q):\n",
        "    def _run():\n",
        "        return drive.ListFile({\"q\": q, \"supportsAllDrives\": True, \"includeItemsFromAllDrives\": True, \"maxResults\": 1000}).GetList()\n",
        "    return _drive_retry(_run)\n",
        "\n",
        "def get_subfolder(drive, parent_id, name):\n",
        "    if drive is None:\n",
        "        p = os.path.join(LOCAL_BASE_DIR, name)\n",
        "        return p if os.path.isdir(p) else None\n",
        "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
        "    r = _drive_query(drive, q)\n",
        "    return r[0][\"id\"] if r else None\n",
        "\n",
        "def get_or_create_folder(drive, parent_id, name):\n",
        "    if drive is None:\n",
        "        base = LOCAL_BASE_DIR if not os.path.isabs(parent_id) else parent_id\n",
        "        p = os.path.join(base, name) if os.path.isdir(base) else os.path.join(LOCAL_BASE_DIR, name)\n",
        "        os.makedirs(p, exist_ok=True)\n",
        "        return p\n",
        "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
        "    res = _drive_query(drive, q)\n",
        "    if res:\n",
        "        return res[0][\"id\"]\n",
        "    def _create():\n",
        "        f = drive.CreateFile({\"title\": name, \"parents\": [{\"id\": parent_id}], \"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "        f.Upload(); return f[\"id\"]\n",
        "    return _drive_retry(_create)\n",
        "\n",
        "def list_files(drive, parent_id):\n",
        "    if drive is None:\n",
        "        return sorted([os.path.join(parent_id, p) for p in os.listdir(parent_id)]) if os.path.isdir(parent_id) else []\n",
        "    q = f\"'{parent_id}' in parents and trashed=false\"\n",
        "    try: return _drive_query(drive, q)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def download_to_temp(drive_file, dst_path):\n",
        "    def _dl():\n",
        "        drive_file.GetContentFile(dst_path); return dst_path\n",
        "    return _drive_retry(_dl)\n",
        "\n",
        "def upload_path(drive, local_path, parent_id, title=None):\n",
        "    if drive is None:\n",
        "        os.makedirs(parent_id, exist_ok=True)\n",
        "        import shutil\n",
        "        dst = os.path.join(parent_id, title or os.path.basename(local_path))\n",
        "        shutil.copy2(local_path, dst); return dst\n",
        "    def _up():\n",
        "        f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\": [{\"id\": parent_id}]})\n",
        "        f.SetContentFile(local_path); f.Upload(); return f[\"id\"]\n",
        "    return _drive_retry(_up)\n",
        "\n",
        "def _drive_walk(drive, start_id, max_depth=4):\n",
        "    q = deque([(start_id, 0)])\n",
        "    while q:\n",
        "        fid, d = q.popleft()\n",
        "        items = list_files(drive, fid)\n",
        "        yield fid, items\n",
        "        if d >= max_depth: continue\n",
        "        for it in items:\n",
        "            if isinstance(it, dict) and it.get(\"mimeType\") == \"application/vnd.google-apps.folder\":\n",
        "                q.append((it[\"id\"], d + 1))\n",
        "\n",
        "# ---------- AEZ and points IO ----------\n",
        "def open_aez_path(drive):\n",
        "    if drive is None:\n",
        "        p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
        "        return p if os.path.exists(p) else None\n",
        "\n",
        "    def _resolve_shortcut(file_obj):\n",
        "        if file_obj.get(\"mimeType\") == \"application/vnd.google-apps.shortcut\":\n",
        "            tgt = file_obj.get(\"shortcutDetails\", {}).get(\"targetId\")\n",
        "            if tgt:\n",
        "                g = drive.CreateFile({\"id\": tgt})\n",
        "                g.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); return g\n",
        "        return file_obj\n",
        "\n",
        "    if AEZ_FILE_ID:\n",
        "        f = drive.CreateFile({\"id\": AEZ_FILE_ID})\n",
        "        f.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); f = _resolve_shortcut(f)\n",
        "        mime = f.get(\"mimeType\")\n",
        "        if mime == \"application/vnd.google-apps.folder\":\n",
        "            folder_id = f[\"id\"]\n",
        "        elif mime and mime.startswith(\"application/vnd.google-apps.\"):\n",
        "            raise RuntimeError(f\"AEZ_FILE_ID points to a Google Doc ({mime}), not a TIFF.\")\n",
        "        else:\n",
        "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
        "            f.GetContentFile(tmp); return tmp\n",
        "    else:\n",
        "        folder_id = None\n",
        "\n",
        "    folder_id = folder_id or os.environ.get(\"AEZ_FOLDER_ID\", None)\n",
        "    if folder_id:\n",
        "        q = f\"'{folder_id}' in parents and trashed=false and title='{AEZ_FILE_NAME}'\"\n",
        "        cand = _drive_query(drive, q)\n",
        "        if not cand:\n",
        "            q_any = f\"'{folder_id}' in parents and trashed=false and title contains '.tif'\"\n",
        "            cand = _drive_query(drive, q_any)\n",
        "            cand = [c for c in cand if c.get(\"title\",\"\") == AEZ_FILE_NAME] or cand\n",
        "        if not cand:\n",
        "            raise RuntimeError(f\"Could not find {AEZ_FILE_NAME} in folder id {folder_id}.\")\n",
        "        f = drive.CreateFile({\"id\": cand[0][\"id\"]})\n",
        "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
        "        f.GetContentFile(tmp); return tmp\n",
        "\n",
        "    p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
        "    return p if os.path.exists(p) else None\n",
        "\n",
        "def _find_points_vector_drive(drive):\n",
        "    try: mt = get_subfolder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
        "    except Exception: mt = None\n",
        "    roots = [mt or ROOT_FOLDER_ID, ROOT_FOLDER_ID]\n",
        "    for root in roots:\n",
        "        for fid, items in _drive_walk(drive, root, max_depth=4):\n",
        "            stems={}\n",
        "            for it in items:\n",
        "                if not isinstance(it, dict): continue\n",
        "                title = it.get(\"title\",\"\"); m = re.match(rf\"(.+)\\.(shp|dbf|shx|prj|cpg|qpj)$\", title, re.IGNORECASE)\n",
        "                if not m: continue\n",
        "                stem = m.group(1)\n",
        "                if stem.lower() == POINTS_BASENAME.lower():\n",
        "                    stems.setdefault(stem,[]).append(it)\n",
        "            for stem, parts in stems.items():\n",
        "                if any(p.get(\"title\",\"\").lower().endswith(\".shp\") for p in parts):\n",
        "                    tdir = tempfile.mkdtemp()\n",
        "                    for p in parts: download_to_temp(p, os.path.join(tdir, p.get(\"title\",\"\")))\n",
        "                    shp_path = os.path.join(tdir, f\"{os.path.basename(stem)}.shp\")\n",
        "                    if os.path.exists(shp_path):\n",
        "                        print(f\"Using vector points (shp): {os.path.basename(stem)}.shp\"); return shp_path\n",
        "            for it in items:\n",
        "                if not isinstance(it, dict): continue\n",
        "                title = it.get(\"title\",\"\"); low = title.lower()\n",
        "                if it.get(\"mimeType\") == \"application/vnd.google-apps.folder\": continue\n",
        "                if low.endswith(\".gpkg\") and low == f\"{POINTS_BASENAME.lower()}.gpkg\":\n",
        "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".gpkg\").name\n",
        "                    download_to_temp(it, tmp); print(f\"Using vector points (gpkg): {title}\"); return tmp\n",
        "                if low.endswith(\".zip\") and low == f\"{POINTS_BASENAME.lower()}.zip\":\n",
        "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\").name\n",
        "                    download_to_temp(it, tmp); print(f\"Using vector points (zip): {title}\"); return tmp\n",
        "                if low.endswith(\".csv\") and low == f\"{POINTS_BASENAME.lower()}.csv\":\n",
        "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\").name\n",
        "                    download_to_temp(it, tmp); print(f\"Using vector points (csv): {title}\"); return tmp\n",
        "    return None\n",
        "\n",
        "def _find_points_vector_local():\n",
        "    for root in [os.path.join(LOCAL_MODEL_DIR), LOCAL_BASE_DIR]:\n",
        "        if not os.path.isdir(root): continue\n",
        "        for nm in os.listdir(root):\n",
        "            low = nm.lower()\n",
        "            if low == f\"{POINTS_BASENAME.lower()}.csv\": return os.path.join(root, nm)\n",
        "            if POINTS_BASENAME.lower() in low and low.endswith(\".gpkg\"): return os.path.join(root, nm)\n",
        "            if POINTS_BASENAME.lower() in low and low.endswith(\".zip\"):  return os.path.join(root, nm)\n",
        "            if low.endswith(\".shp\") and POINTS_BASENAME.lower() in low:  return os.path.join(root, nm)\n",
        "    return None\n",
        "\n",
        "def load_points_raw(drive):\n",
        "    import geopandas as gpd, fiona\n",
        "    path = _find_points_vector_drive(drive) if drive is not None else _find_points_vector_local()\n",
        "    if path is None:\n",
        "        raise RuntimeError(\"Could not locate US.(shp/zip/gpkg/csv) in 'Model Training' tree.\")\n",
        "    if path.lower().endswith(\".csv\"):\n",
        "        df = pd.read_csv(path)\n",
        "    else:\n",
        "        with fiona.Env(SHAPE_RESTORE_SHX=\"YES\"):\n",
        "            gdf = gpd.read_file(path)\n",
        "        if gdf.crs is not None and gdf.crs.to_epsg() != 4326:\n",
        "            gdf = gdf.to_crs(4326)\n",
        "        df = pd.DataFrame(gdf.drop(columns=\"geometry\", errors=\"ignore\"))\n",
        "    if LONCOL not in df.columns or LATCOL not in df.columns:\n",
        "        # best-effort case-insensitive pickup\n",
        "        def _ci(cols, name): \n",
        "            for c in cols:\n",
        "                if c.lower()==name.lower(): return c\n",
        "            return name\n",
        "        df.rename(columns={_ci(df.columns,LONCOL):LONCOL, _ci(df.columns,LATCOL):LATCOL}, inplace=True)\n",
        "    return df\n",
        "\n",
        "def _sample_raster_at_lonlat(src, lon_arr, lat_arr):\n",
        "    xs = np.asarray(lon_arr, dtype=float); ys = np.asarray(lat_arr, dtype=float)\n",
        "    if src.crs and (src.crs.to_epsg() != 4326):\n",
        "        tx, ty = rio_transform(CRS.from_epsg(4326), src.crs, xs.tolist(), ys.tolist())\n",
        "        xs = np.asarray(tx, dtype=float); ys = np.asarray(ty, dtype=float)\n",
        "    out = np.full(xs.shape[0], np.nan, dtype=np.float32)\n",
        "    xmin, ymin, xmax, ymax = src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top\n",
        "    inside = (xs >= xmin) & (xs <= xmax) & (ys >= ymin) & (ys <= ymax)\n",
        "    if not np.any(inside): return out\n",
        "    idx = np.where(inside)[0]\n",
        "    coords = list(zip(xs[idx], ys[idx]))\n",
        "    vals = np.array([v[0] for v in src.sample(coords)], dtype=np.float32)\n",
        "    if src.nodata is not None and np.isfinite(src.nodata):\n",
        "        vals = np.where(np.isclose(vals, np.float32(src.nodata)), np.float32(np.nan), vals)\n",
        "    out[idx] = vals; return out\n",
        "\n",
        "# ---------- Per-state AEZ tiles ----------\n",
        "def _pick_reference_raster_for_bounds(drive, state):\n",
        "    # use any state raster present (CLE or NDVI_mean etc.)\n",
        "    if drive is None:\n",
        "        pdir = os.path.join(LOCAL_BY_COUNTRY_DIR, state)\n",
        "        if not os.path.isdir(pdir):\n",
        "            return None\n",
        "        for nm in os.listdir(pdir):\n",
        "            if nm.endswith(\".tif\") and ((\"_CLE\" in nm) or (\"NDVI_mean\" in nm) or (\"NDVI\" in nm)):\n",
        "                return os.path.join(pdir, nm)\n",
        "        # fallback to any tif\n",
        "        for nm in os.listdir(pdir):\n",
        "            if nm.endswith(\".tif\"):\n",
        "                return os.path.join(pdir, nm)\n",
        "        return None\n",
        "    byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
        "    if not byc: return None\n",
        "    q = f\"'{byc}' in parents and trashed=false and title='{state}' and mimeType='application/vnd.google-apps.folder'\"\n",
        "    res = _drive_query(drive, q)\n",
        "    if not res: return None\n",
        "    sid = res[0][\"id\"]\n",
        "    files = list_files(drive, sid)\n",
        "    pick = None\n",
        "    for it in files:\n",
        "        if not isinstance(it, dict): continue\n",
        "        t = it.get(\"title\",\"\")\n",
        "        if t.endswith(\".tif\") and ((\"_CLE\" in t) or (\"NDVI_mean\" in t) or (\"NDVI\" in t)):\n",
        "            pick = it; break\n",
        "    if not pick:\n",
        "        for it in files:\n",
        "            if isinstance(it, dict) and it.get(\"title\",\"\").endswith(\".tif\"):\n",
        "                pick = it; break\n",
        "    if not pick: return None\n",
        "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
        "    return download_to_temp(pick, tmp)\n",
        "\n",
        "def _target_wgs84_grid_from_bounds(west, south, east, north, pixel_m=30.0):\n",
        "    mid_lat = 0.5 * (south + north)\n",
        "    m_per_deg_lat = 111_132.0\n",
        "    m_per_deg_lon = 111_320.0 * math.cos(math.radians(mid_lat))\n",
        "    dy = pixel_m / m_per_deg_lat; dx = pixel_m / m_per_deg_lon\n",
        "    width = int(math.ceil((east - west) / dx)); height = int(math.ceil((north - south) / dy))\n",
        "    transform = Affine(dx, 0, west, 0, -dy, north)\n",
        "    return height, width, transform, CRS.from_epsg(4326)\n",
        "\n",
        "def _clip_bounds_from_reference(raster_path):\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        b = rio_transform_bounds(src.crs, CRS.from_epsg(4326),\n",
        "                                 src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top, densify_pts=8)\n",
        "    return b\n",
        "\n",
        "def write_state_aez_tif(drive, state):\n",
        "    aez_src_path = open_aez_path(drive)\n",
        "    if not aez_src_path:\n",
        "        raise RuntimeError(\"AEZ raster not found.\")\n",
        "    ref = _pick_reference_raster_for_bounds(drive, state)\n",
        "    if not ref:\n",
        "        print(f\"   ! skip {state}: no reference raster to define grid\"); return\n",
        "    w,s,e,n = _clip_bounds_from_reference(ref)\n",
        "    H,W,transform,crs = _target_wgs84_grid_from_bounds(w,s,e,n, pixel_m=30.0)\n",
        "\n",
        "    base_profile = {\n",
        "        \"driver\":\"GTiff\",\"height\":H,\"width\":W,\"count\":1,\"crs\":crs,\"transform\":transform,\n",
        "        \"compress\":\"LZW\",\"tiled\":True,\"blockxsize\":512,\"blockysize\":512,\n",
        "        \"dtype\":\"float32\",\"nodata\":-9999.0\n",
        "    }\n",
        "    with tempfile.TemporaryDirectory() as tdir:\n",
        "        out_tmp = os.path.join(tdir, f\"{state}_AEZ.tif\")\n",
        "        with rasterio.open(aez_src_path) as src, rasterio.open(out_tmp, \"w\", **base_profile) as dst:\n",
        "            dst_arr = np.full((H,W), -9999.0, dtype=np.float32)\n",
        "            reproject(source=rasterio.band(src,1), destination=dst_arr,\n",
        "                      src_transform=src.transform, src_crs=src.crs,\n",
        "                      dst_transform=transform, dst_crs=crs,\n",
        "                      dst_nodata=-9999.0, resampling=RioResampling.nearest)\n",
        "            dst.write(dst_arr, 1)\n",
        "\n",
        "        # upload/write to By Country/<State>/<State>_AEZ.tif\n",
        "        if drive is None:\n",
        "            out_dir = os.path.join(LOCAL_BY_COUNTRY_DIR, state)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "        else:\n",
        "            byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME) or get_or_create_folder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
        "            # ensure state folder\n",
        "            if drive is None:\n",
        "                out_dir = os.path.join(byc, state); os.makedirs(out_dir, exist_ok=True)\n",
        "            else:\n",
        "                q = f\"'{byc}' in parents and trashed=false and title='{state}' and mimeType='application/vnd.google-apps.folder'\"\n",
        "                res = _drive_query(drive, q)\n",
        "                state_id = res[0][\"id\"] if res else get_or_create_folder(drive, byc, state)\n",
        "                out_dir = state_id\n",
        "        upload_path(drive, out_tmp, out_dir, f\"{state}_AEZ.tif\")\n",
        "    print(f\"   ‚Ä¢ wrote {state}_AEZ.tif\")\n",
        "\n",
        "def list_states(drive):\n",
        "    names=set()\n",
        "    if drive is None:\n",
        "        if os.path.isdir(LOCAL_BY_COUNTRY_DIR):\n",
        "            for d in os.listdir(LOCAL_BY_COUNTRY_DIR):\n",
        "                if os.path.isdir(os.path.join(LOCAL_BY_COUNTRY_DIR,d)):\n",
        "                    names.add(d)\n",
        "    else:\n",
        "        byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
        "        if byc:\n",
        "            subs = _drive_query(drive, f\"'{byc}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
        "            for it in subs: names.add(it[\"title\"])\n",
        "    return sorted(names)\n",
        "\n",
        "# ---------- Points cache ----------\n",
        "def augment_points_with_aez(drive):\n",
        "    df = load_points_raw(drive)\n",
        "    if AEZ_COL in df.columns and df[AEZ_COL].notna().any():\n",
        "        print(\"   ‚Ä¢ points already have AEZ; writing cache anyway\")\n",
        "    aez_path = open_aez_path(drive)\n",
        "    if not aez_path:\n",
        "        raise RuntimeError(\"AEZ raster not found.\")\n",
        "    with rasterio.open(aez_path) as src:\n",
        "        vals = _sample_raster_at_lonlat(src, df[LONCOL].values, df[LATCOL].values)\n",
        "    df[AEZ_COL] = np.rint(pd.to_numeric(vals, errors=\"coerce\")).astype(\"float64\")\n",
        "\n",
        "    # stable rounding keys for merge\n",
        "    df[\"_lonr\"] = np.round(df[LONCOL].astype(float), 6)\n",
        "    df[\"_latr\"] = np.round(df[LATCOL].astype(float), 6)\n",
        "\n",
        "    # write cache\n",
        "    if drive is None:\n",
        "        out_dir = LOCAL_MODEL_DIR\n",
        "    else:\n",
        "        out_dir = get_or_create_folder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
        "    with tempfile.TemporaryDirectory() as tdir:\n",
        "        pq = os.path.join(tdir, \"US_with_AEZ.parquet\")\n",
        "        cs = os.path.join(tdir, \"US_with_AEZ.csv\")\n",
        "        df[[LONCOL,LATCOL,\"_lonr\",\"_latr\",AEZ_COL]].to_parquet(pq, index=False)\n",
        "        df[[LONCOL,LATCOL,\"_lonr\",\"_latr\",AEZ_COL]].to_csv(cs, index=False)\n",
        "        upload_path(drive, pq, out_dir, \"US_with_AEZ.parquet\")\n",
        "        upload_path(drive, cs, out_dir, \"US_with_AEZ.csv\")\n",
        "    print(\"   ‚Ä¢ wrote Model Training/US_with_AEZ.parquet (+ .csv)\")\n",
        "\n",
        "# ---------- Runner ----------\n",
        "def run(use_drive=True, states=None):\n",
        "    if use_drive:\n",
        "        try:\n",
        "            drive  # noqa\n",
        "            _drive = drive\n",
        "            try: _drive.auth.service.http.timeout = 120\n",
        "            except Exception: pass\n",
        "        except NameError:\n",
        "            raise RuntimeError(\"PyDrive 'drive' not found. Authenticate and expose 'drive', or set use_drive=False.\")\n",
        "    else:\n",
        "        _drive = None\n",
        "        os.makedirs(LOCAL_BY_COUNTRY_DIR, exist_ok=True)\n",
        "        os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    # A) per-state AEZ tiles\n",
        "    if not states:\n",
        "        states = list_states(_drive)\n",
        "    print(\"=== Build per-state AEZ tiles ===\")\n",
        "    for s in states:\n",
        "        try:\n",
        "            write_state_aez_tif(_drive, s)\n",
        "        except Exception as e:\n",
        "            print(f\"   ! {s}: {e}\")\n",
        "\n",
        "    # B) points AEZ cache\n",
        "    print(\"=== Build points AEZ cache ===\")\n",
        "    augment_points_with_aez(_drive)\n",
        "    print(\"‚úÖ done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run(use_drive=True, states=None)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
