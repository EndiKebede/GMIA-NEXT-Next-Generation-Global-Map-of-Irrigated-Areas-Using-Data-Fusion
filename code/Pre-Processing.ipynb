{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "\n",
        "# local dirve path to  JSON.\n",
        "CLIENT_JSON = \"client_secret_Final.json\"   #Google Dirve API\n",
        "gauth = GoogleAuth()\n",
        "gauth.LoadClientConfigFile(CLIENT_JSON)\n",
        "\n",
        "# Command-line auth -> \n",
        "gauth.CommandLineAuth()\n",
        "\n",
        "# Save token \n",
        "gauth.SaveCredentialsFile('token.json')\n",
        "\n",
        "drive = GoogleDrive(gauth)\n",
        "print(\"‚úÖ Google Drive connected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Mosaic multiple tif rasters for a given country to have a single file per each varaiable for a country\n",
        "\"\"\"\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.io import MemoryFile\n",
        "from rasterio.shutil import copy as rio_copy\n",
        "from rasterio.warp import reproject, Resampling as WarpResampling\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Google Drive setup\n",
        "# -------------------------------------------------------------------\n",
        "# NOTE: 'drive' must be an authenticated PyDrive2 client in your session.\n",
        "ROOT_FOLDER_ID = \"18pQKnMMnLramhHRZSNwUJrLqG5DXNMmS\"  # folder id from gogole dirve\n",
        "\n",
        "# Variables \n",
        "VARIABLES = [\n",
        "    \"NDVI_mean\", \"NDVI_max\", \"NDVI_min\",\n",
        "    \"NDWI_mean\", \"NDWI_max\", \"NDWI_min\",\n",
        "    \"GI_mean\", \"GI_max\", \"GI_min\",\n",
        "    \"elevation\", \"slope\",\n",
        "    \"ET\", \"PET\",\n",
        "\n",
        "]\n",
        "\n",
        "# Provinces to process\n",
        "PROVINCES = [\n",
        "    \"Philippines\",\n",
        "    # ...\n",
        "]\n",
        "\n",
        "# Optional synonyms (case-insensitive)\n",
        "COUNTRY_SYNONYMS = {\n",
        "    \"Philippines\": \"philippines\",\n",
        "}\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Helpers\n",
        "# -------------------------------------------------------------------\n",
        "VARIABLE_TOKENS = {v.lower(): v for v in VARIABLES}  # canonicalize\n",
        "\n",
        "\n",
        "def _squash_spaces(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "\n",
        "def normalize_country(name: str) -> str:\n",
        "    \"\"\"Normalize 'Africa_Inner_Mongolia' -> 'Africa Inner Mongolia', apply synonyms.\"\"\"\n",
        "    if not name:\n",
        "        return \"\"\n",
        "    n = name.strip().replace(\"_\", \" \")\n",
        "    n = _squash_spaces(n)\n",
        "    lower_syn = {k.lower(): v for k, v in COUNTRY_SYNONYMS.items()}\n",
        "    if n.lower() in lower_syn:\n",
        "        n = lower_syn[n.lower()]\n",
        "    return n\n",
        "\n",
        "\n",
        "def _safe_basename(s: str) -> str:\n",
        "    \"\"\"ASCII slug for filenames.\"\"\"\n",
        "    norm = unicodedata.normalize(\"NFKD\", s)\n",
        "    ascii_only = norm.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    slug = re.sub(r\"[^A-Za-z0-9]+\", \"_\", ascii_only).strip(\"_\")\n",
        "    return slug or \"untitled\"\n",
        "\n",
        "\n",
        "def _drive_q_escape(s: str) -> str:\n",
        "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Drive helpers\n",
        "# -------------------------------------------------------------------\n",
        "def get_or_create_folder(drive, parent_id, name):\n",
        "    safe = _drive_q_escape(name)\n",
        "    q = (\n",
        "        f\"'{parent_id}' in parents and trashed=false and \"\n",
        "        f\"mimeType='application/vnd.google-apps.folder' and title='{safe}'\"\n",
        "    )\n",
        "    res = drive.ListFile({'q': q}).GetList()\n",
        "    if res:\n",
        "        return res[0]['id']\n",
        "    f = drive.CreateFile({\n",
        "        'title': name,\n",
        "        'parents': [{'id': parent_id}],\n",
        "        'mimeType': 'application/vnd.google-apps.folder'\n",
        "    })\n",
        "    f.Upload()\n",
        "    return f['id']\n",
        "\n",
        "\n",
        "def list_all_tifs_recursive(drive, root_id):\n",
        "    \"\"\"Recursively list all .tif/.tiff under root_id.\"\"\"\n",
        "    tifs, stack = [], [root_id]\n",
        "    while stack:\n",
        "        folder_id = stack.pop()\n",
        "        q = f\"'{folder_id}' in parents and trashed=false\"\n",
        "        for it in drive.ListFile({'q': q}).GetList():\n",
        "            mime = it.get('mimeType', '')\n",
        "            if mime == 'application/vnd.google-apps.folder':\n",
        "                stack.append(it['id'])\n",
        "            else:\n",
        "                title = it.get('title', '')\n",
        "                if title and title.lower().endswith(('.tif', '.tiff')):\n",
        "                    tifs.append(it)\n",
        "    return tifs\n",
        "\n",
        "\n",
        "def download_many(drive, files, dst_dir, max_workers=8):\n",
        "    \"\"\"Download Drive files concurrently to dst_dir, return local paths.\"\"\"\n",
        "\n",
        "    def _dl(f):\n",
        "        local = os.path.join(dst_dir, f['title'])\n",
        "        f.GetContentFile(local)\n",
        "        return local\n",
        "\n",
        "    paths = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futs = [ex.submit(_dl, f) for f in files]\n",
        "        for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Downloading\"):\n",
        "            paths.append(fut.result())\n",
        "    return paths\n",
        "\n",
        "\n",
        "def upload_tif(drive, local_path, parent_id, title):\n",
        "    f = drive.CreateFile({'title': title, 'parents': [{'id': parent_id}]})\n",
        "    f.SetContentFile(local_path)\n",
        "    f.Upload()\n",
        "    return f['id']\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Parsing\n",
        "# -------------------------------------------------------------------\n",
        "def _filename_core(title: str) -> str:\n",
        "    \"\"\"\n",
        "    Take a Drive title and return the 'logical' base name:\n",
        "\n",
        "    e.g.\n",
        "    'New_York_GI_max-0000000000-0000000000.tif' -> 'New_York_GI_max'\n",
        "    \"\"\"\n",
        "    base = os.path.splitext(os.path.basename(title))[0]\n",
        "    # Drop anything after the first '-' (tile ID, date, etc.)\n",
        "    if '-' in base:\n",
        "        base = base.split('-', 1)[0]\n",
        "    return base\n",
        "\n",
        "\n",
        "def _split_tokens(path_title: str):\n",
        "    \"\"\"\n",
        "    Split the core filename into '_' tokens.\n",
        "\n",
        "    'New_York_GI_max-0000000000-0000000000.tif'\n",
        "      -> core 'New_York_GI_max'\n",
        "      -> ['New', 'York', 'GI', 'max']\n",
        "    \"\"\"\n",
        "    core = _filename_core(path_title)\n",
        "    return [t for t in core.split('_') if t]\n",
        "\n",
        "\n",
        "def parse_title(title: str):\n",
        "    \"\"\"\n",
        "    Return (country_string_like_'Africa_Anhui', canonical_variable) or (None, None)\n",
        "    \"\"\"\n",
        "    if not title.lower().endswith(('.tif', '.tiff')):\n",
        "        return None, None\n",
        "\n",
        "    toks = _split_tokens(title)\n",
        "    if not toks:\n",
        "        return None, None\n",
        "\n",
        "    lower = [t.lower() for t in toks]\n",
        "\n",
        "    # Match country: pick the longest province whose tokenized name matches the start\n",
        "    best_country = None\n",
        "    start_len = 0\n",
        "    for prov in PROVINCES:\n",
        "        ptoks = prov.split('_')\n",
        "        if len(ptoks) <= len(lower) and [t.lower() for t in ptoks] == lower[:len(ptoks)]:\n",
        "            if len(ptoks) > start_len:\n",
        "                best_country = prov\n",
        "                start_len = len(ptoks)\n",
        "\n",
        "    if not best_country:\n",
        "        return None, None\n",
        "\n",
        "    # Find variable starting right after the country tokens\n",
        "    best = None  # (i, j, canonical_var)\n",
        "    for i in range(start_len, min(start_len + 4, len(lower))):\n",
        "        for j in range(i, min(i + 3, len(lower))):\n",
        "            cand = '_'.join(lower[i:j + 1])\n",
        "            if cand in VARIABLE_TOKENS:\n",
        "                # prefer longer match\n",
        "                if best is None or (j - i) > (best[1] - best[0]):\n",
        "                    best = (i, j, VARIABLE_TOKENS[cand])\n",
        "\n",
        "    if not best:\n",
        "        return None, None\n",
        "\n",
        "    _, _, variable = best\n",
        "    return best_country, variable\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Size helpers / dtype\n",
        "# -------------------------------------------------------------------\n",
        "def _bytes_per_pixel(dtype):\n",
        "    return {\n",
        "        \"uint8\": 1, \"int8\": 1,\n",
        "        \"uint16\": 2, \"int16\": 2,\n",
        "        \"uint32\": 4, \"int32\": 4,\n",
        "        \"float32\": 4, \"float64\": 8\n",
        "    }.get(dtype, 4)\n",
        "\n",
        "\n",
        "def _should_bigtiff(width, height, count, dtype):\n",
        "    est = width * height * count * _bytes_per_pixel(dtype) * 1.05\n",
        "    return est >= (4 * 1024 ** 3)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Grid/reference helpers\n",
        "# -------------------------------------------------------------------\n",
        "def _pick_reference(datasets):\n",
        "    # Choose the most frequent (xres, yres); break ties by largest pixel area\n",
        "    reslist = [(ds.transform.a, -ds.transform.e) for ds in datasets]\n",
        "    counts = Counter(reslist)\n",
        "    best_res = max(counts.items(), key=lambda kv: (kv[1], kv[0][0] * kv[0][1]))[0]\n",
        "    for ds in datasets:\n",
        "        if (ds.transform.a, -ds.transform.e) == best_res:\n",
        "            return ds\n",
        "    return datasets[0]\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# COG writer\n",
        "# -------------------------------------------------------------------\n",
        "def write_cog_from_array(mosaic, profile, cog_path):\n",
        "    force_bigtiff = _should_bigtiff(\n",
        "        width=profile[\"width\"],\n",
        "        height=profile[\"height\"],\n",
        "        count=profile[\"count\"],\n",
        "        dtype=profile[\"dtype\"]\n",
        "    )\n",
        "\n",
        "    src_profile = profile.copy()\n",
        "    src_profile.update({\n",
        "        \"driver\": \"GTiff\",\n",
        "        \"tiled\": True,\n",
        "        \"blockxsize\": profile.get(\"blockxsize\", 512),\n",
        "        \"blockysize\": profile.get(\"blockysize\", 512),\n",
        "        \"compress\": profile.get(\"compress\", \"LZW\"),\n",
        "        \"predictor\": profile.get(\"predictor\", 3),\n",
        "        \"interleave\": \"band\",\n",
        "        \"BIGTIFF\": \"YES\" if force_bigtiff else \"IF_SAFER\",\n",
        "    })\n",
        "\n",
        "    with MemoryFile() as memfile:\n",
        "        with memfile.open(**src_profile) as tmp:\n",
        "            tmp.write(mosaic)\n",
        "        with memfile.open() as src_ds:\n",
        "            rio_copy(\n",
        "                src_ds,\n",
        "                cog_path,\n",
        "                driver=\"COG\",\n",
        "                COMPRESS=\"LZW\",\n",
        "                PREDICTOR=src_profile[\"predictor\"],\n",
        "                BLOCKSIZE=512,\n",
        "                OVERVIEW_LEVELS=\"2,4,8,16\",\n",
        "                OVERVIEW_RESAMPLING=\"AVERAGE\",\n",
        "                NUM_THREADS=\"ALL_CPUS\",\n",
        "                BIGTIFF=\"YES\" if force_bigtiff else \"IF_SAFER\",\n",
        "                RESAMPLING=\"NEAREST\",\n",
        "                DST_NODATA=profile.get(\"nodata\", None),\n",
        "            )\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Mosaic core\n",
        "# -------------------------------------------------------------------\n",
        "def _reducer_for_variable(var: str):\n",
        "    \"\"\"Pick a per-pixel reducer.\"\"\"\n",
        "    vl = var.lower()\n",
        "    if vl.endswith(\"_min\"):\n",
        "        return np.fmin\n",
        "    # default to fmax for max/mean/masks, etc.\n",
        "    return np.fmax\n",
        "\n",
        "\n",
        "def mosaic_files_to_array(datasets, reducer, default_nodata=-9999.0):\n",
        "    \"\"\"\n",
        "    Mosaic single-band rasters to float32 using NaN as working nodata.\n",
        "    Handles mixed CRS and pixel sizes (aligned to common grid).\n",
        "    \"\"\"\n",
        "    # Common CRS: take the first non-None\n",
        "    common_crs = next((ds.crs for ds in datasets if ds.crs is not None), None)\n",
        "    if common_crs is None:\n",
        "        raise RuntimeError(\n",
        "            \"None of the input tiles has a CRS defined; cannot reproject to a common grid.\"\n",
        "        )\n",
        "\n",
        "    ref = _pick_reference(datasets)\n",
        "    ref_transform = ref.transform\n",
        "    ref_res = (ref_transform.a, -ref_transform.e)\n",
        "\n",
        "    # Union bounds in common CRS\n",
        "    from rasterio.warp import transform_bounds\n",
        "    minx = miny = float(\"inf\")\n",
        "    maxx = maxy = float(\"-inf\")\n",
        "    for ds in datasets:\n",
        "        b = ds.bounds\n",
        "        if ds.crs is not None and ds.crs != common_crs:\n",
        "            b = transform_bounds(ds.crs, common_crs, *b, densify_pts=21)\n",
        "        minx, miny = min(minx, b[0]), min(miny, b[1])\n",
        "        maxx, maxy = max(maxx, b[2]), max(maxy, b[3])\n",
        "\n",
        "    # Destination grid at the reference resolution\n",
        "    from rasterio.transform import from_origin\n",
        "    dst_transform = from_origin(minx, maxy, ref_res[0], ref_res[1])\n",
        "    dst_w = int(np.ceil((maxx - minx) / ref_res[0]))\n",
        "    dst_h = int(np.ceil((maxy - miny) / ref_res[1]))\n",
        "\n",
        "    acc = np.full((dst_h, dst_w), np.nan, dtype=np.float32)\n",
        "\n",
        "    for ds in datasets:\n",
        "        temp = np.full((dst_h, dst_w), np.nan, dtype=np.float32)\n",
        "        src_nodata = ds.nodata\n",
        "        reproject(\n",
        "            source=rasterio.band(ds, 1),\n",
        "            destination=temp,\n",
        "            src_transform=ds.transform,\n",
        "            src_crs=(ds.crs if ds.crs is not None else common_crs),\n",
        "            dst_transform=dst_transform,\n",
        "            dst_crs=common_crs,\n",
        "            src_nodata=src_nodata,\n",
        "            dst_nodata=np.nan,\n",
        "            resampling=WarpResampling.nearest,\n",
        "        )\n",
        "        if np.isnan(acc).all():\n",
        "            acc = temp\n",
        "        else:\n",
        "            acc = reducer(acc, temp)  # NaN-aware fmin/fmax\n",
        "\n",
        "    mosaic = np.where(np.isnan(acc), default_nodata, acc).astype(np.float32)[None, ...]\n",
        "    return mosaic, dst_transform, common_crs, float(default_nodata)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Country + variable mosaicking and upload\n",
        "# -------------------------------------------------------------------\n",
        "def mosaic_country_variable_to_drive(drive, parent_id, country, variable, by_country_id):\n",
        "    \"\"\"\n",
        "    Find all tiles for (country, variable), mosaic, write COG, upload.\n",
        "    \"\"\"\n",
        "    tifs = list_all_tifs_recursive(drive, parent_id)\n",
        "\n",
        "    selected = []\n",
        "    c_norm = country  # already province token form\n",
        "    v_norm = variable.lower()\n",
        "\n",
        "    for it in tifs:\n",
        "        title = it.get('title', '')\n",
        "        ctry, var = parse_title(title)\n",
        "        if ctry is None:\n",
        "            continue\n",
        "        if ctry == c_norm and var.lower() == v_norm:\n",
        "            selected.append(it)\n",
        "\n",
        "    if not selected:\n",
        "        print(f\"‚ö†Ô∏è No files for {country} / {variable}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n‚ñ∂Ô∏è {country} / {variable} | {len(selected)} file(s)\")\n",
        "\n",
        "    tmpdir = tempfile.mkdtemp(\n",
        "        prefix=f\"{_safe_basename(country)}_{_safe_basename(variable)}_\"\n",
        "    )\n",
        "    local_paths, datasets = [], []\n",
        "    try:\n",
        "        # Download tiles\n",
        "        local_paths = download_many(drive, selected, tmpdir, max_workers=8)\n",
        "\n",
        "        # Open datasets (require single band)\n",
        "        for p in local_paths:\n",
        "            ds = rasterio.open(p)\n",
        "            if ds.count != 1:\n",
        "                raise RuntimeError(\n",
        "                    f\"Only single-band rasters supported. \"\n",
        "                    f\"{os.path.basename(p)} has {ds.count} bands.\"\n",
        "                )\n",
        "            datasets.append(ds)\n",
        "\n",
        "        reducer = _reducer_for_variable(variable)\n",
        "\n",
        "        # Mosaic to float32\n",
        "        mosaic, out_transform, out_crs, out_nodata = mosaic_files_to_array(\n",
        "            datasets, reducer=reducer\n",
        "        )\n",
        "\n",
        "        profile = {\n",
        "            \"dtype\": \"float32\",\n",
        "            \"height\": mosaic.shape[1],\n",
        "            \"width\": mosaic.shape[2],\n",
        "            \"count\": 1,\n",
        "            \"transform\": out_transform,\n",
        "            \"crs\": out_crs,\n",
        "            \"nodata\": out_nodata,\n",
        "            \"blockxsize\": 512,\n",
        "            \"blockysize\": 512,\n",
        "            \"compress\": \"LZW\",\n",
        "            \"predictor\": 3,\n",
        "        }\n",
        "\n",
        "        # Output folder + filename\n",
        "        display_country = normalize_country(country)\n",
        "        country_folder_id = get_or_create_folder(drive, by_country_id, display_country)\n",
        "\n",
        "        safe_country = _safe_basename(display_country)\n",
        "        safe_variable = _safe_basename(variable)\n",
        "        out_name = f\"{safe_country}_{safe_variable}.tif\"\n",
        "        cog_local = os.path.join(tmpdir, out_name)\n",
        "\n",
        "        print(\"   ‚Ä¢ writing COG ‚Ä¶\")\n",
        "        write_cog_from_array(mosaic, profile, cog_local)\n",
        "\n",
        "        print(f\"   ‚Ä¢ uploading to Drive as {out_name} ‚Ä¶\")\n",
        "        out_id = upload_tif(drive, cog_local, country_folder_id, out_name)\n",
        "        print(f\"‚úÖ Uploaded: {out_name} (file id: {out_id})\")\n",
        "\n",
        "    finally:\n",
        "        for ds in datasets:\n",
        "            try:\n",
        "                ds.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "        shutil.rmtree(tmpdir, ignore_errors=True)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Batch runner\n",
        "# -------------------------------------------------------------------\n",
        "def run_all_provinces_variables(drive):\n",
        "    by_country_id = get_or_create_folder(drive, ROOT_FOLDER_ID, \"By Country\")\n",
        "    # Ensure country folders exist\n",
        "    for c in PROVINCES:\n",
        "        get_or_create_folder(drive, by_country_id, normalize_country(c))\n",
        "\n",
        "    for c in PROVINCES:\n",
        "        for v in VARIABLES:\n",
        "            try:\n",
        "                mosaic_country_variable_to_drive(drive, ROOT_FOLDER_ID, c, v, by_country_id)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå {c} / {v}: {e}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Execute\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_provinces_variables(drive)\n",
        "    print(\"\\nüéâ Done.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
