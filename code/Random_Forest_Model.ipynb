{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cd337-e695-4236-b46b-1adce6d61e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google Drive connected\n"
     ]
    }
   ],
   "source": [
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "# path to  JSON.\n",
    "CLIENT_JSON = \"client_secret_Final.json\"   # Google Drive API\n",
    "gauth = GoogleAuth()\n",
    "gauth.LoadClientConfigFile(CLIENT_JSON)\n",
    "\n",
    "# Command-line auth \n",
    "gauth.CommandLineAuth()\n",
    "\n",
    "# Save token \n",
    "gauth.SaveCredentialsFile('token.json')\n",
    "\n",
    "drive = GoogleDrive(gauth)\n",
    "print(\"✅ Google Drive connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd2afd-9f0b-4960-9922-f856e514bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# match Slurm allocation (\"64\") for maximizing efficency \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"64\"        \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"64\"        \n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"64\"    \n",
    "os.environ[\"GDAL_NUM_THREADS\"] = \"64\"       \n",
    "os.environ[\"RASTERIO_NUM_THREADS\"] = \"64\"   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List only folders in Google Drive root\n",
    "folder_list = drive.ListFile({\n",
    "    'q': \"'root' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
    "}).GetList()\n",
    "\n",
    "for folder in folder_list:\n",
    "    print(f\"Folder: {folder['title']}  |  ID: {folder['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_aez_cache_and_tiles.py\n",
    "# Pre-compute AEZ (\"GTPS\") for:\n",
    "#   (A) per-state tiles:   By Country/<State>/<State>_AEZ.tif\n",
    "#   (B) training points:   Model Training/US_with_AEZ.parquet (+ .csv)\n",
    "#\n",
    "# Run:  python prep_aez_cache_and_tiles.py\n",
    "#       (with PyDrive 'drive' in scope if using Drive; otherwise local)\n",
    "\n",
    "import os, re, json, math, tempfile, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling as RioResampling\n",
    "from rasterio.warp import reproject, transform as rio_transform, transform_bounds as rio_transform_bounds\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "try:\n",
    "    from googleapiclient.errors import HttpError\n",
    "except Exception:\n",
    "    HttpError = Exception\n",
    "try:\n",
    "    from pydrive2.files import ApiRequestError\n",
    "except Exception:\n",
    "    ApiRequestError = Exception\n",
    "\n",
    "# ---------- reuse your config ----------\n",
    "ROOT_FOLDER_ID = os.environ.get(\"ROOT_FOLDER_ID\", \"1hqMIyDYEFKnpS8KLxC4bqHmF_9dHXImG\")\n",
    "BY_COUNTRY_NAME = \"By Country\"\n",
    "MODEL_FOLDER = \"Model Training\"\n",
    "LOCAL_BASE_DIR = os.path.join(\"./\", \"US\")\n",
    "LOCAL_BY_COUNTRY_DIR = os.path.join(LOCAL_BASE_DIR, \"By Country\")\n",
    "LOCAL_MODEL_DIR = os.path.join(LOCAL_BASE_DIR, MODEL_FOLDER)\n",
    "\n",
    "AEZ_FILE_ID   = os.environ.get(\"AEZ_FILE_ID\", \"1te3nKn8vyt2AECmk8NM_xBgpEFXp84RX\")\n",
    "AEZ_FILE_NAME = os.environ.get(\"AEZ_FILE_NAME\", \"AEZ_2020s.tif\")\n",
    "\n",
    "POINTS_BASENAME = \"US\"\n",
    "LONCOL = \"longitude\"; LATCOL = \"latitude\"\n",
    "AEZ_COL = \"AEZ\"\n",
    "\n",
    "DRIVE_MAX_RETRIES = int(os.environ.get(\"DRIVE_MAX_RETRIES\", 6))\n",
    "DRIVE_RETRY_BASE  = float(os.environ.get(\"DRIVE_RETRY_BASE\", 0.8))\n",
    "\n",
    "# ---------- Drive helpers (same behavior as your main file) ----------\n",
    "def _drive_retry(callable_fn, *args, **kwargs):\n",
    "    last_err = None\n",
    "    for i in range(DRIVE_MAX_RETRIES):\n",
    "        try:\n",
    "            return callable_fn(*args, **kwargs)\n",
    "        except (HttpError, ApiRequestError) as e:\n",
    "            last_err = e\n",
    "            code = getattr(getattr(e, \"resp\", None), \"status\", None)\n",
    "            if code is None:\n",
    "                msg = str(e).lower()\n",
    "                transient = any(k in msg for k in [\"internal error\",\"backenderror\",\"rate limit\",\"timeout\"])\n",
    "            else:\n",
    "                transient = 500 <= int(code) < 600 or int(code) in (403, 429)\n",
    "            if not transient or i == DRIVE_MAX_RETRIES - 1:\n",
    "                break\n",
    "            time.sleep(DRIVE_RETRY_BASE * (2 ** i) + random.random() * 0.2)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            break\n",
    "    raise last_err\n",
    "\n",
    "def _drive_query(drive, q):\n",
    "    def _run():\n",
    "        return drive.ListFile({\"q\": q, \"supportsAllDrives\": True, \"includeItemsFromAllDrives\": True, \"maxResults\": 1000}).GetList()\n",
    "    return _drive_retry(_run)\n",
    "\n",
    "def get_subfolder(drive, parent_id, name):\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BASE_DIR, name)\n",
    "        return p if os.path.isdir(p) else None\n",
    "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    r = _drive_query(drive, q)\n",
    "    return r[0][\"id\"] if r else None\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    if drive is None:\n",
    "        base = LOCAL_BASE_DIR if not os.path.isabs(parent_id) else parent_id\n",
    "        p = os.path.join(base, name) if os.path.isdir(base) else os.path.join(LOCAL_BASE_DIR, name)\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "        return p\n",
    "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    res = _drive_query(drive, q)\n",
    "    if res:\n",
    "        return res[0][\"id\"]\n",
    "    def _create():\n",
    "        f = drive.CreateFile({\"title\": name, \"parents\": [{\"id\": parent_id}], \"mimeType\": \"application/vnd.google-apps.folder\"})\n",
    "        f.Upload(); return f[\"id\"]\n",
    "    return _drive_retry(_create)\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    if drive is None:\n",
    "        return sorted([os.path.join(parent_id, p) for p in os.listdir(parent_id)]) if os.path.isdir(parent_id) else []\n",
    "    q = f\"'{parent_id}' in parents and trashed=false\"\n",
    "    try: return _drive_query(drive, q)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def download_to_temp(drive_file, dst_path):\n",
    "    def _dl():\n",
    "        drive_file.GetContentFile(dst_path); return dst_path\n",
    "    return _drive_retry(_dl)\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    if drive is None:\n",
    "        os.makedirs(parent_id, exist_ok=True)\n",
    "        import shutil\n",
    "        dst = os.path.join(parent_id, title or os.path.basename(local_path))\n",
    "        shutil.copy2(local_path, dst); return dst\n",
    "    def _up():\n",
    "        f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\": [{\"id\": parent_id}]})\n",
    "        f.SetContentFile(local_path); f.Upload(); return f[\"id\"]\n",
    "    return _drive_retry(_up)\n",
    "\n",
    "def _drive_walk(drive, start_id, max_depth=4):\n",
    "    q = deque([(start_id, 0)])\n",
    "    while q:\n",
    "        fid, d = q.popleft()\n",
    "        items = list_files(drive, fid)\n",
    "        yield fid, items\n",
    "        if d >= max_depth: continue\n",
    "        for it in items:\n",
    "            if isinstance(it, dict) and it.get(\"mimeType\") == \"application/vnd.google-apps.folder\":\n",
    "                q.append((it[\"id\"], d + 1))\n",
    "\n",
    "# ---------- AEZ and points IO ----------\n",
    "def open_aez_path(drive):\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
    "        return p if os.path.exists(p) else None\n",
    "\n",
    "    def _resolve_shortcut(file_obj):\n",
    "        if file_obj.get(\"mimeType\") == \"application/vnd.google-apps.shortcut\":\n",
    "            tgt = file_obj.get(\"shortcutDetails\", {}).get(\"targetId\")\n",
    "            if tgt:\n",
    "                g = drive.CreateFile({\"id\": tgt})\n",
    "                g.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); return g\n",
    "        return file_obj\n",
    "\n",
    "    if AEZ_FILE_ID:\n",
    "        f = drive.CreateFile({\"id\": AEZ_FILE_ID})\n",
    "        f.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); f = _resolve_shortcut(f)\n",
    "        mime = f.get(\"mimeType\")\n",
    "        if mime == \"application/vnd.google-apps.folder\":\n",
    "            folder_id = f[\"id\"]\n",
    "        elif mime and mime.startswith(\"application/vnd.google-apps.\"):\n",
    "            raise RuntimeError(f\"AEZ_FILE_ID points to a Google Doc ({mime}), not a TIFF.\")\n",
    "        else:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            f.GetContentFile(tmp); return tmp\n",
    "    else:\n",
    "        folder_id = None\n",
    "\n",
    "    folder_id = folder_id or os.environ.get(\"AEZ_FOLDER_ID\", None)\n",
    "    if folder_id:\n",
    "        q = f\"'{folder_id}' in parents and trashed=false and title='{AEZ_FILE_NAME}'\"\n",
    "        cand = _drive_query(drive, q)\n",
    "        if not cand:\n",
    "            q_any = f\"'{folder_id}' in parents and trashed=false and title contains '.tif'\"\n",
    "            cand = _drive_query(drive, q_any)\n",
    "            cand = [c for c in cand if c.get(\"title\",\"\") == AEZ_FILE_NAME] or cand\n",
    "        if not cand:\n",
    "            raise RuntimeError(f\"Could not find {AEZ_FILE_NAME} in folder id {folder_id}.\")\n",
    "        f = drive.CreateFile({\"id\": cand[0][\"id\"]})\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "        f.GetContentFile(tmp); return tmp\n",
    "\n",
    "    p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "def _find_points_vector_drive(drive):\n",
    "    try: mt = get_subfolder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "    except Exception: mt = None\n",
    "    roots = [mt or ROOT_FOLDER_ID, ROOT_FOLDER_ID]\n",
    "    for root in roots:\n",
    "        for fid, items in _drive_walk(drive, root, max_depth=4):\n",
    "            stems={}\n",
    "            for it in items:\n",
    "                if not isinstance(it, dict): continue\n",
    "                title = it.get(\"title\",\"\"); m = re.match(rf\"(.+)\\.(shp|dbf|shx|prj|cpg|qpj)$\", title, re.IGNORECASE)\n",
    "                if not m: continue\n",
    "                stem = m.group(1)\n",
    "                if stem.lower() == POINTS_BASENAME.lower():\n",
    "                    stems.setdefault(stem,[]).append(it)\n",
    "            for stem, parts in stems.items():\n",
    "                if any(p.get(\"title\",\"\").lower().endswith(\".shp\") for p in parts):\n",
    "                    tdir = tempfile.mkdtemp()\n",
    "                    for p in parts: download_to_temp(p, os.path.join(tdir, p.get(\"title\",\"\")))\n",
    "                    shp_path = os.path.join(tdir, f\"{os.path.basename(stem)}.shp\")\n",
    "                    if os.path.exists(shp_path):\n",
    "                        print(f\"Using vector points (shp): {os.path.basename(stem)}.shp\"); return shp_path\n",
    "            for it in items:\n",
    "                if not isinstance(it, dict): continue\n",
    "                title = it.get(\"title\",\"\"); low = title.lower()\n",
    "                if it.get(\"mimeType\") == \"application/vnd.google-apps.folder\": continue\n",
    "                if low.endswith(\".gpkg\") and low == f\"{POINTS_BASENAME.lower()}.gpkg\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".gpkg\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (gpkg): {title}\"); return tmp\n",
    "                if low.endswith(\".zip\") and low == f\"{POINTS_BASENAME.lower()}.zip\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (zip): {title}\"); return tmp\n",
    "                if low.endswith(\".csv\") and low == f\"{POINTS_BASENAME.lower()}.csv\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (csv): {title}\"); return tmp\n",
    "    return None\n",
    "\n",
    "def _find_points_vector_local():\n",
    "    for root in [os.path.join(LOCAL_MODEL_DIR), LOCAL_BASE_DIR]:\n",
    "        if not os.path.isdir(root): continue\n",
    "        for nm in os.listdir(root):\n",
    "            low = nm.lower()\n",
    "            if low == f\"{POINTS_BASENAME.lower()}.csv\": return os.path.join(root, nm)\n",
    "            if POINTS_BASENAME.lower() in low and low.endswith(\".gpkg\"): return os.path.join(root, nm)\n",
    "            if POINTS_BASENAME.lower() in low and low.endswith(\".zip\"):  return os.path.join(root, nm)\n",
    "            if low.endswith(\".shp\") and POINTS_BASENAME.lower() in low:  return os.path.join(root, nm)\n",
    "    return None\n",
    "\n",
    "def load_points_raw(drive):\n",
    "    import geopandas as gpd, fiona\n",
    "    path = _find_points_vector_drive(drive) if drive is not None else _find_points_vector_local()\n",
    "    if path is None:\n",
    "        raise RuntimeError(\"Could not locate US.(shp/zip/gpkg/csv) in 'Model Training' tree.\")\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        with fiona.Env(SHAPE_RESTORE_SHX=\"YES\"):\n",
    "            gdf = gpd.read_file(path)\n",
    "        if gdf.crs is not None and gdf.crs.to_epsg() != 4326:\n",
    "            gdf = gdf.to_crs(4326)\n",
    "        df = pd.DataFrame(gdf.drop(columns=\"geometry\", errors=\"ignore\"))\n",
    "    if LONCOL not in df.columns or LATCOL not in df.columns:\n",
    "        # best-effort case-insensitive pickup\n",
    "        def _ci(cols, name): \n",
    "            for c in cols:\n",
    "                if c.lower()==name.lower(): return c\n",
    "            return name\n",
    "        df.rename(columns={_ci(df.columns,LONCOL):LONCOL, _ci(df.columns,LATCOL):LATCOL}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def _sample_raster_at_lonlat(src, lon_arr, lat_arr):\n",
    "    xs = np.asarray(lon_arr, dtype=float); ys = np.asarray(lat_arr, dtype=float)\n",
    "    if src.crs and (src.crs.to_epsg() != 4326):\n",
    "        tx, ty = rio_transform(CRS.from_epsg(4326), src.crs, xs.tolist(), ys.tolist())\n",
    "        xs = np.asarray(tx, dtype=float); ys = np.asarray(ty, dtype=float)\n",
    "    out = np.full(xs.shape[0], np.nan, dtype=np.float32)\n",
    "    xmin, ymin, xmax, ymax = src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top\n",
    "    inside = (xs >= xmin) & (xs <= xmax) & (ys >= ymin) & (ys <= ymax)\n",
    "    if not np.any(inside): return out\n",
    "    idx = np.where(inside)[0]\n",
    "    coords = list(zip(xs[idx], ys[idx]))\n",
    "    vals = np.array([v[0] for v in src.sample(coords)], dtype=np.float32)\n",
    "    if src.nodata is not None and np.isfinite(src.nodata):\n",
    "        vals = np.where(np.isclose(vals, np.float32(src.nodata)), np.float32(np.nan), vals)\n",
    "    out[idx] = vals; return out\n",
    "\n",
    "# ---------- Per-state AEZ tiles ----------\n",
    "def _pick_reference_raster_for_bounds(drive, state):\n",
    "    # use any state raster present (CLE or NDVI_mean etc.)\n",
    "    if drive is None:\n",
    "        pdir = os.path.join(LOCAL_BY_COUNTRY_DIR, state)\n",
    "        if not os.path.isdir(pdir):\n",
    "            return None\n",
    "        for nm in os.listdir(pdir):\n",
    "            if nm.endswith(\".tif\") and ((\"_CLE\" in nm) or (\"NDVI_mean\" in nm) or (\"NDVI\" in nm)):\n",
    "                return os.path.join(pdir, nm)\n",
    "        # fallback to any tif\n",
    "        for nm in os.listdir(pdir):\n",
    "            if nm.endswith(\".tif\"):\n",
    "                return os.path.join(pdir, nm)\n",
    "        return None\n",
    "    byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
    "    if not byc: return None\n",
    "    q = f\"'{byc}' in parents and trashed=false and title='{state}' and mimeType='application/vnd.google-apps.folder'\"\n",
    "    res = _drive_query(drive, q)\n",
    "    if not res: return None\n",
    "    sid = res[0][\"id\"]\n",
    "    files = list_files(drive, sid)\n",
    "    pick = None\n",
    "    for it in files:\n",
    "        if not isinstance(it, dict): continue\n",
    "        t = it.get(\"title\",\"\")\n",
    "        if t.endswith(\".tif\") and ((\"_CLE\" in t) or (\"NDVI_mean\" in t) or (\"NDVI\" in t)):\n",
    "            pick = it; break\n",
    "    if not pick:\n",
    "        for it in files:\n",
    "            if isinstance(it, dict) and it.get(\"title\",\"\").endswith(\".tif\"):\n",
    "                pick = it; break\n",
    "    if not pick: return None\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "    return download_to_temp(pick, tmp)\n",
    "\n",
    "def _target_wgs84_grid_from_bounds(west, south, east, north, pixel_m=30.0):\n",
    "    mid_lat = 0.5 * (south + north)\n",
    "    m_per_deg_lat = 111_132.0\n",
    "    m_per_deg_lon = 111_320.0 * math.cos(math.radians(mid_lat))\n",
    "    dy = pixel_m / m_per_deg_lat; dx = pixel_m / m_per_deg_lon\n",
    "    width = int(math.ceil((east - west) / dx)); height = int(math.ceil((north - south) / dy))\n",
    "    transform = Affine(dx, 0, west, 0, -dy, north)\n",
    "    return height, width, transform, CRS.from_epsg(4326)\n",
    "\n",
    "def _clip_bounds_from_reference(raster_path):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        b = rio_transform_bounds(src.crs, CRS.from_epsg(4326),\n",
    "                                 src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top, densify_pts=8)\n",
    "    return b\n",
    "\n",
    "def write_state_aez_tif(drive, state):\n",
    "    aez_src_path = open_aez_path(drive)\n",
    "    if not aez_src_path:\n",
    "        raise RuntimeError(\"AEZ raster not found.\")\n",
    "    ref = _pick_reference_raster_for_bounds(drive, state)\n",
    "    if not ref:\n",
    "        print(f\"   ! skip {state}: no reference raster to define grid\"); return\n",
    "    w,s,e,n = _clip_bounds_from_reference(ref)\n",
    "    H,W,transform,crs = _target_wgs84_grid_from_bounds(w,s,e,n, pixel_m=30.0)\n",
    "\n",
    "    base_profile = {\n",
    "        \"driver\":\"GTiff\",\"height\":H,\"width\":W,\"count\":1,\"crs\":crs,\"transform\":transform,\n",
    "        \"compress\":\"LZW\",\"tiled\":True,\"blockxsize\":512,\"blockysize\":512,\n",
    "        \"dtype\":\"float32\",\"nodata\":-9999.0\n",
    "    }\n",
    "    with tempfile.TemporaryDirectory() as tdir:\n",
    "        out_tmp = os.path.join(tdir, f\"{state}_AEZ.tif\")\n",
    "        with rasterio.open(aez_src_path) as src, rasterio.open(out_tmp, \"w\", **base_profile) as dst:\n",
    "            dst_arr = np.full((H,W), -9999.0, dtype=np.float32)\n",
    "            reproject(source=rasterio.band(src,1), destination=dst_arr,\n",
    "                      src_transform=src.transform, src_crs=src.crs,\n",
    "                      dst_transform=transform, dst_crs=crs,\n",
    "                      dst_nodata=-9999.0, resampling=RioResampling.nearest)\n",
    "            dst.write(dst_arr, 1)\n",
    "\n",
    "        # upload/write to By Country/<State>/<State>_AEZ.tif\n",
    "        if drive is None:\n",
    "            out_dir = os.path.join(LOCAL_BY_COUNTRY_DIR, state)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "        else:\n",
    "            byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME) or get_or_create_folder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
    "            # ensure state folder\n",
    "            if drive is None:\n",
    "                out_dir = os.path.join(byc, state); os.makedirs(out_dir, exist_ok=True)\n",
    "            else:\n",
    "                q = f\"'{byc}' in parents and trashed=false and title='{state}' and mimeType='application/vnd.google-apps.folder'\"\n",
    "                res = _drive_query(drive, q)\n",
    "                state_id = res[0][\"id\"] if res else get_or_create_folder(drive, byc, state)\n",
    "                out_dir = state_id\n",
    "        upload_path(drive, out_tmp, out_dir, f\"{state}_AEZ.tif\")\n",
    "    print(f\"   • wrote {state}_AEZ.tif\")\n",
    "\n",
    "def list_states(drive):\n",
    "    names=set()\n",
    "    if drive is None:\n",
    "        if os.path.isdir(LOCAL_BY_COUNTRY_DIR):\n",
    "            for d in os.listdir(LOCAL_BY_COUNTRY_DIR):\n",
    "                if os.path.isdir(os.path.join(LOCAL_BY_COUNTRY_DIR,d)):\n",
    "                    names.add(d)\n",
    "    else:\n",
    "        byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
    "        if byc:\n",
    "            subs = _drive_query(drive, f\"'{byc}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "            for it in subs: names.add(it[\"title\"])\n",
    "    return sorted(names)\n",
    "\n",
    "# ---------- Points cache ----------\n",
    "def augment_points_with_aez(drive):\n",
    "    df = load_points_raw(drive)\n",
    "    if AEZ_COL in df.columns and df[AEZ_COL].notna().any():\n",
    "        print(\"   • points already have AEZ; writing cache anyway\")\n",
    "    aez_path = open_aez_path(drive)\n",
    "    if not aez_path:\n",
    "        raise RuntimeError(\"AEZ raster not found.\")\n",
    "    with rasterio.open(aez_path) as src:\n",
    "        vals = _sample_raster_at_lonlat(src, df[LONCOL].values, df[LATCOL].values)\n",
    "    df[AEZ_COL] = np.rint(pd.to_numeric(vals, errors=\"coerce\")).astype(\"float64\")\n",
    "\n",
    "    # stable rounding keys for merge\n",
    "    df[\"_lonr\"] = np.round(df[LONCOL].astype(float), 6)\n",
    "    df[\"_latr\"] = np.round(df[LATCOL].astype(float), 6)\n",
    "\n",
    "    # write cache\n",
    "    if drive is None:\n",
    "        out_dir = LOCAL_MODEL_DIR\n",
    "    else:\n",
    "        out_dir = get_or_create_folder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "    with tempfile.TemporaryDirectory() as tdir:\n",
    "        pq = os.path.join(tdir, \"US_with_AEZ.parquet\")\n",
    "        cs = os.path.join(tdir, \"US_with_AEZ.csv\")\n",
    "        df[[LONCOL,LATCOL,\"_lonr\",\"_latr\",AEZ_COL]].to_parquet(pq, index=False)\n",
    "        df[[LONCOL,LATCOL,\"_lonr\",\"_latr\",AEZ_COL]].to_csv(cs, index=False)\n",
    "        upload_path(drive, pq, out_dir, \"US_with_AEZ.parquet\")\n",
    "        upload_path(drive, cs, out_dir, \"US_with_AEZ.csv\")\n",
    "    print(\"   • wrote Model Training/US_with_AEZ.parquet (+ .csv)\")\n",
    "\n",
    "# ---------- Runner ----------\n",
    "def run(use_drive=True, states=None):\n",
    "    if use_drive:\n",
    "        try:\n",
    "            drive  # noqa\n",
    "            _drive = drive\n",
    "            try: _drive.auth.service.http.timeout = 120\n",
    "            except Exception: pass\n",
    "        except NameError:\n",
    "            raise RuntimeError(\"PyDrive 'drive' not found. Authenticate and expose 'drive', or set use_drive=False.\")\n",
    "    else:\n",
    "        _drive = None\n",
    "        os.makedirs(LOCAL_BY_COUNTRY_DIR, exist_ok=True)\n",
    "        os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    # A) per-state AEZ tiles\n",
    "    if not states:\n",
    "        states = list_states(_drive)\n",
    "    print(\"=== Build per-state AEZ tiles ===\")\n",
    "    for s in states:\n",
    "        try:\n",
    "            write_state_aez_tif(_drive, s)\n",
    "        except Exception as e:\n",
    "            print(f\"   ! {s}: {e}\")\n",
    "\n",
    "    # B) points AEZ cache\n",
    "    print(\"=== Build points AEZ cache ===\")\n",
    "    augment_points_with_aez(_drive)\n",
    "    print(\"✅ done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(use_drive=True, states=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ba20c-7c6c-4665-821c-0c30fc57fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vector points (shp): Africa.shp\n",
      "   • AEZ filled from cache for 265/15126 points\n",
      "=== Train regional model: Africa ===\n",
      "   • AEZ sampled from tiles/global for 14861 points\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Regional RF irrigation model with:\n",
    "- Farm-aware split via DBSCAN (no leakage)\n",
    "- AEZ categorical (one-hot)\n",
    "- OUT-OF-FOLD probability calibration (isotonic by default, Platt optional)\n",
    "- Regional OOF metrics + artifacts\n",
    "- Per-country OOF metrics & thresholds saved\n",
    "- Country predictions on ~30m WGS84 using calibrated probabilities\n",
    "\n",
    "  • Read AEZ per-point from Model Training/Continent_GTPS_per_point.(parquet|csv) when available\n",
    "  • Prefer per-state AEZ tiles: By Country/<State>/<State>_AEZ.tif (fallback: global AEZ)\n",
    "\n",
    "Outputs:\n",
    "  CountryModelPredicted/<Country>_RF_probability_percent.tif\n",
    "  CountryModelPredicted/<Country>_RF_binary_0_1_cropland.tif\n",
    "  CountryModelPredicted/<Country>_RF_predictors_count.tif\n",
    "\n",
    "  Model Training/Regional Models/<Region>/\n",
    "    - metrics.json\n",
    "    - confusion.png\n",
    "    - feature_importance.csv\n",
    "    - per_country_test_metrics.csv\n",
    "    - model.joblib\n",
    "\n",
    "  Model Training/Country Models/<Country>/\n",
    "    - threshold.json\n",
    "    - test_metrics.json\n",
    "    - test_confusion.png\n",
    "\"\"\"\n",
    "import os, re, json, math, tempfile, joblib, time, random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, average_precision_score, roc_auc_score,\n",
    "    precision_recall_fscore_support, confusion_matrix, roc_curve,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import pyproj\n",
    "from sklearn.base import clone\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling as RioResampling\n",
    "from rasterio.warp import reproject, transform as rio_transform, transform_bounds as rio_transform_bounds\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.windows import Window\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from googleapiclient.errors import HttpError\n",
    "except Exception:\n",
    "    HttpError = Exception\n",
    "try:\n",
    "    from pydrive2.files import ApiRequestError\n",
    "except Exception:\n",
    "    ApiRequestError = Exception\n",
    "\n",
    "os.environ.setdefault(\"SHAPE_RESTORE_SHX\", \"YES\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "# ------------------------ CONFIG ------------------------\n",
    "ROOT_FOLDER_ID = os.environ.get(\"ROOT_FOLDER_ID\", \"1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\")\n",
    "BY_COUNTRY_NAME = \"By Country\"\n",
    "CLE_FOLDER_ID = \"1pOpf-Zy5la4SKTIcrmLX6tGlxVBejKYX\"\n",
    "MODEL_FOLDER = \"Model Training\"\n",
    "REGIONAL_MODELS_FOLDER = \"Regional Models\"  # used per country now: region_name == country\n",
    "COUNTRY_MODELS_FOLDER = \"Country Models\"\n",
    "OUTPUT_FOLDER = \"CountryModelPredicted_Cropland\"\n",
    "\n",
    "# River Network (continental fallback for distance-to-river)\n",
    "RIVER_NET_FOLDER_ID = \"1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\"\n",
    "RIVER_DIST_PREFIX = \"Dist_\"\n",
    "RIVER_DIST_SUFFIX = \"_river.tif\"\n",
    "\n",
    "# Country column auto-detection; we copy into this name\n",
    "COUNTRY_COL = os.environ.get(\"COUNTRY_COL\", \"country_hint\")\n",
    "\n",
    "# DBSCAN farm grouping (meters)\n",
    "DBSCAN_EPS_M = float(os.environ.get(\"DBSCAN_EPS_M\", 45))\n",
    "DBSCAN_MIN_SAMPLES = int(os.environ.get(\"DBSCAN_MIN_SAMPLES\", 1))\n",
    "\n",
    "# AEZ (global raster)\n",
    "AEZ_FILE_ID   = os.environ.get(\"AEZ_FILE_ID\", \"1te3nKn8vyt2AECmk8NM_xBgpEFXp84RX\")\n",
    "AEZ_FILE_NAME = os.environ.get(\"AEZ_FILE_NAME\", \"AEZ_2020s.tif\")\n",
    "AEZ_COL       = \"AEZ\"\n",
    "\n",
    "# Local fallbacks\n",
    "LOCAL_BASE_DIR = os.path.join(\"./\", \"Africa_VIs_Env\")\n",
    "LOCAL_BY_COUNTRY_DIR = os.path.join(LOCAL_BASE_DIR, \"By Country\")\n",
    "LOCAL_CLE_DIR = os.path.join(LOCAL_BASE_DIR, \"CLE\")\n",
    "LOCAL_MODEL_DIR = os.path.join(LOCAL_BASE_DIR, MODEL_FOLDER)\n",
    "LOCAL_OUTPUT_DIR = os.path.join(LOCAL_BASE_DIR, OUTPUT_FOLDER)\n",
    "\n",
    "# GTPS/AEZ cache made earlier\n",
    "GTPS_CACHE_PARQUET = \"Africa_GTPS_per_point.parquet\"\n",
    "GTPS_CACHE_CSV     = \"Africa_GTPS_per_point.csv\"\n",
    "\n",
    "# Drive retry knobs\n",
    "DRIVE_MAX_RETRIES = int(os.environ.get(\"DRIVE_MAX_RETRIES\", 6))\n",
    "DRIVE_RETRY_BASE = float(os.environ.get(\"DRIVE_RETRY_BASE\", 0.8))\n",
    "\n",
    "POINTS_BASENAME = \"Africa\"  # pooled training points\n",
    "\n",
    "ALL_PREDICTORS = [\n",
    "    \"NDVI_mean\",\"NDWI_mean\",\"GI_mean\",\n",
    "    \"NDVI_max\",\"NDWI_max\",\"GI_max\",\n",
    "    \"NDVI_min\",\"NDWI_min\",\"GI_min\",\n",
    "    \"elevation\",\"slope\",\n",
    "    # \"ET\",\"PET\",\n",
    "    \"dist_to_river\",\n",
    "    AEZ_COL,\n",
    "]\n",
    "TARGET = \"irrigated\"\n",
    "LONCOL = \"longitude\"\n",
    "LATCOL = \"latitude\"\n",
    "\n",
    "\n",
    "# RF + data\n",
    "RF_TREES = int(os.environ.get(\"RF_TREES\", 400))\n",
    "RANDOM_SEED = 42\n",
    "TEST_FRACTION = 0.30\n",
    "\n",
    "BORROW_MAX = 2000\n",
    "\n",
    "# ---- Calibration & thresholds ----\n",
    "CALIBRATION_METHOD = os.environ.get(\"CALIBRATION_METHOD\", \"isotonic\").lower()  # \"isotonic\" | \"platt\"\n",
    "THRESHOLD_STRATEGY = os.environ.get(\"THRESHOLD_STRATEGY\", \"auto\").lower()      # \"auto\"|\"j\"|\"f1\"|\"fixed\"|\"precision_at\"|\"recall_at\"\n",
    "FIXED_THRESHOLD = float(os.environ.get(\"FIXED_THRESHOLD\", 0.50))\n",
    "PRECISION_TARGET = os.environ.get(\"PRECISION_TARGET\")\n",
    "RECALL_TARGET = os.environ.get(\"RECALL_TARGET\")\n",
    "PRECISION_TARGET = None if PRECISION_TARGET in (None, \"\", \"None\") else float(PRECISION_TARGET)\n",
    "RECALL_TARGET    = None if RECALL_TARGET    in (None, \"\", \"None\") else float(RECALL_TARGET)\n",
    "\n",
    "# Per-country threshold mode: \"global\" | \"country\" | \"hybrid\"\n",
    "THRESHOLD_MODE = os.environ.get(\"THRESHOLD_MODE\", \"hybrid\").lower()\n",
    "MIN_POS_NEG_FOR_COUNTRY = int(os.environ.get(\"MIN_POS_NEG_FOR_COUNTRY\", 30))\n",
    "MIN_TOTAL_FOR_COUNTRY   = int(os.environ.get(\"MIN_TOTAL_FOR_COUNTRY\", 100))\n",
    "\n",
    "# Prediction\n",
    "TILE_SIZE = 1024\n",
    "RESAMPLING = RioResampling.bilinear\n",
    "PER_BAND_RESAMPLING = {AEZ_COL: RioResampling.nearest}\n",
    "STRICT_ALL_BANDS = False\n",
    "MIN_PREDICTORS = 9\n",
    "\n",
    "# ------------------ UTILS / HELPERS ---------------------\n",
    "def _meters_per_degree(lat_deg: float):\n",
    "    m_per_deg_lat = 111_132.0\n",
    "    m_per_deg_lon = 111_320.0 * math.cos(math.radians(lat_deg))\n",
    "    return m_per_deg_lat, m_per_deg_lon\n",
    "\n",
    "def _target_wgs84_grid_from_bounds(west, south, east, north, pixel_m=30.0):\n",
    "    mid_lat = 0.5 * (south + north)\n",
    "    m_lat, m_lon = _meters_per_degree(mid_lat)\n",
    "    dy = pixel_m / m_lat; dx = pixel_m / m_lon\n",
    "    width = int(math.ceil((east - west) / dx)); height = int(math.ceil((north - south) / dy))\n",
    "    transform = Affine(dx, 0, west, 0, -dy, north)\n",
    "    return height, width, transform, CRS.from_epsg(4326)\n",
    "\n",
    "def _iter_tiles(H, W, tile):\n",
    "    for r0 in range(0, H, tile):\n",
    "        for c0 in range(0, W, tile):\n",
    "            r1 = min(r0 + tile, H); c1 = min(c0 + tile, W)\n",
    "            yield Window.from_slices((r0, r1), (c0, c1))\n",
    "\n",
    "def _tile_count(H, W, tile):\n",
    "    return ((H + tile - 1) // tile) * ((W + tile - 1) // tile)\n",
    "\n",
    "def _compute_basic_metrics(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    try: roc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception: roc = float(\"nan\")\n",
    "    try: prauc = average_precision_score(y_true, y_prob)\n",
    "    except Exception: prauc = float(\"nan\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(pr),\n",
    "        \"recall\": float(rc),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"pr_auc\": float(prauc),\n",
    "        \"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]),\n",
    "        \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1]),\n",
    "    }, cm\n",
    "\n",
    "def _save_confusion_png(cm, title, path):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xlabel(\"Pred\"); ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title)\n",
    "    for (r,c),v in np.ndenumerate(cm):\n",
    "        ax.text(c, r, f\"{v}\", ha=\"center\", va=\"center\")\n",
    "    fig.savefig(path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def assign_farm_ids(df: pd.DataFrame, lon_col=LONCOL, lat_col=LATCOL) -> pd.Series:\n",
    "    if df.empty:\n",
    "        return pd.Series([], dtype=\"int64\")\n",
    "    proj = pyproj.Transformer.from_crs(4326, 3857, always_xy=True)\n",
    "    x, y = proj.transform(df[lon_col].values, df[lat_col].values)\n",
    "    coords = np.column_stack([x, y])\n",
    "    db = DBSCAN(eps=DBSCAN_EPS_M, min_samples=DBSCAN_MIN_SAMPLES, metric=\"euclidean\", algorithm=\"ball_tree\")\n",
    "    labels = db.fit_predict(coords)\n",
    "    if (labels == -1).any():\n",
    "        max_lab = labels[labels >= 0].max() if np.any(labels >= 0) else -1\n",
    "        noise_idx = np.where(labels == -1)[0]\n",
    "        labels[noise_idx] = np.arange(max_lab + 1, max_lab + 1 + len(noise_idx))\n",
    "    return pd.Series(labels.astype(\"int64\"), index=df.index, name=\"farm_id\")\n",
    "\n",
    "def _pick_country_candidate(cols):\n",
    "    cand = [\"country_hint\",\"country\",\"Country\",\"COUNTRY\",\"admin\",\"ADMIN\",\"state\",\"STATE\",\"STATE_NAME\",\"State\",\"province\",\"Province\"]\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for c in cand:\n",
    "        if c.lower() in low:\n",
    "            return low[c.lower()]\n",
    "    return None\n",
    "\n",
    "# ---------------------- DRIVE HELPERS -------------------\n",
    "def _drive_retry(callable_fn, *args, **kwargs):\n",
    "    last_err = None\n",
    "    for i in range(DRIVE_MAX_RETRIES):\n",
    "        try:\n",
    "            return callable_fn(*args, **kwargs)\n",
    "        except (HttpError, ApiRequestError) as e:\n",
    "            last_err = e\n",
    "            code = getattr(getattr(e, \"resp\", None), \"status\", None)\n",
    "            if code is None:\n",
    "                msg = str(e).lower()\n",
    "                transient = any(k in msg for k in [\"internal error\",\"backenderror\",\"rate limit\",\"timeout\"])\n",
    "            else:\n",
    "                transient = 500 <= int(code) < 600 or int(code) in (403, 429)\n",
    "            if not transient or i == DRIVE_MAX_RETRIES - 1:\n",
    "                break\n",
    "            sleep_s = DRIVE_RETRY_BASE * (2 ** i) + random.random() * 0.2\n",
    "            print(f\"[Drive Retry] attempt {i+1}/{DRIVE_MAX_RETRIES} after error {e}; sleeping {sleep_s:.2f}s\")\n",
    "            time.sleep(sleep_s)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            break\n",
    "    raise last_err\n",
    "\n",
    "def _drive_query(drive, q):\n",
    "    def _run():\n",
    "        return drive.ListFile({\"q\": q, \"supportsAllDrives\": True, \"includeItemsFromAllDrives\": True, \"maxResults\": 1000}).GetList()\n",
    "    return _drive_retry(_run)\n",
    "\n",
    "def get_subfolder(drive, parent_id, name):\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BASE_DIR, name)\n",
    "        return p if os.path.isdir(p) else None\n",
    "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    r = _drive_query(drive, q)\n",
    "    return r[0][\"id\"] if r else None\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    if drive is None:\n",
    "        base = LOCAL_BASE_DIR if not os.path.isabs(parent_id) else parent_id\n",
    "        p = os.path.join(base, name) if os.path.isdir(base) else os.path.join(LOCAL_BASE_DIR, name)\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "        return p\n",
    "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    res = _drive_query(drive, q)\n",
    "    if res:\n",
    "        return res[0][\"id\"]\n",
    "    def _create():\n",
    "        f = drive.CreateFile({\"title\": name, \"parents\": [{\"id\": parent_id}], \"mimeType\": \"application/vnd.google-apps.folder\"})\n",
    "        f.Upload(); return f[\"id\"]\n",
    "    return _drive_retry(_create)\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    if drive is None:\n",
    "        return sorted([os.path.join(parent_id, p) for p in os.listdir(parent_id)]) if os.path.isdir(parent_id) else []\n",
    "    q = f\"'{parent_id}' in parents and trashed=false\"\n",
    "    try: return _drive_query(drive, q)\n",
    "    except Exception as e:\n",
    "        print(f\"[Drive Warning] list_files failed for parent {parent_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_to_temp(drive_file, dst_path):\n",
    "    def _dl():\n",
    "        drive_file.GetContentFile(dst_path); return dst_path\n",
    "    return _drive_retry(_dl)\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    if drive is None:\n",
    "        os.makedirs(parent_id, exist_ok=True)\n",
    "        import shutil\n",
    "        dst = os.path.join(parent_id, title or os.path.basename(local_path))\n",
    "        shutil.copy2(local_path, dst); return dst\n",
    "    def _up():\n",
    "        f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\": [{\"id\": parent_id}]})\n",
    "        f.SetContentFile(local_path); f.Upload(); return f[\"id\"]\n",
    "    return _drive_retry(_up)\n",
    "\n",
    "def _drive_walk(drive, start_id, max_depth=4):\n",
    "    q = deque([(start_id, 0)])\n",
    "    while q:\n",
    "        fid, d = q.popleft()\n",
    "        items = list_files(drive, fid)\n",
    "        yield fid, items\n",
    "        if d >= max_depth: continue\n",
    "        for it in items:\n",
    "            if isinstance(it, dict) and it.get(\"mimeType\") == \"application/vnd.google-apps.folder\":\n",
    "                q.append((it[\"id\"], d + 1))\n",
    "\n",
    "# ---------------------- AEZ SOURCES ----------------------\n",
    "def open_aez_path(drive):\n",
    "    \"\"\"Global AEZ.tif (fallback if per-country tile missing).\"\"\"\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
    "        return p if os.path.exists(p) else None\n",
    "\n",
    "    def _resolve_shortcut(file_obj):\n",
    "        if file_obj.get(\"mimeType\") == \"application/vnd.google-apps.shortcut\":\n",
    "            tgt = file_obj.get(\"shortcutDetails\", {}).get(\"targetId\")\n",
    "            if tgt:\n",
    "                g = drive.CreateFile({\"id\": tgt})\n",
    "                g.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); return g\n",
    "        return file_obj\n",
    "\n",
    "    folder_id = None\n",
    "    if AEZ_FILE_ID:\n",
    "        f = drive.CreateFile({\"id\": AEZ_FILE_ID})\n",
    "        f.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); f = _resolve_shortcut(f)\n",
    "        mime = f.get(\"mimeType\")\n",
    "        if mime == \"application/vnd.google-apps.folder\":\n",
    "            folder_id = f[\"id\"]\n",
    "        elif mime and mime.startswith(\"application/vnd.google-apps.\"):\n",
    "            raise RuntimeError(f\"AEZ_FILE_ID points to a Google Doc ({mime}), not a TIFF.\")\n",
    "        else:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            f.GetContentFile(tmp); return tmp\n",
    "\n",
    "    folder_id = folder_id or os.environ.get(\"AEZ_FOLDER_ID\", None)\n",
    "    if folder_id:\n",
    "        q = f\"'{folder_id}' in parents and trashed=false and title='{AEZ_FILE_NAME}'\"\n",
    "        cand = _drive_query(drive, q)\n",
    "        if not cand:\n",
    "            q_any = f\"'{folder_id}' in parents and trashed=false and title contains '.tif'\"\n",
    "            cand = _drive_query(drive, q_any)\n",
    "            cand = [c for c in cand if c.get(\"title\",\"\") == AEZ_FILE_NAME] or cand\n",
    "        if not cand:\n",
    "            raise RuntimeError(f\"Could not find {AEZ_FILE_NAME} in folder id {folder_id}.\")\n",
    "        f = drive.CreateFile({\"id\": cand[0][\"id\"]})\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "        f.GetContentFile(tmp); return tmp\n",
    "\n",
    "    p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "# ---------------------- POINTS / CACHE -------------------\n",
    "def _find_points_vector_drive(drive):\n",
    "    try: mt = get_subfolder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "    except Exception: mt = None\n",
    "    if mt:\n",
    "        items = list_files(drive, mt); parts=[]; has_shp=False\n",
    "        for it in items:\n",
    "            if not isinstance(it, dict): continue\n",
    "            title = it.get(\"title\",\"\"); low = title.lower()\n",
    "            if not low.startswith(\"US.\"): continue\n",
    "            if low.endswith(\".shp\"): has_shp=True\n",
    "            if re.search(r\"\\.(shp|dbf|shx|prj|cpg|qpj)$\", low): parts.append(it)\n",
    "        if has_shp and parts:\n",
    "            tdir = tempfile.mkdtemp()\n",
    "            for p in parts: download_to_temp(p, os.path.join(tdir, p.get(\"title\",\"\")))\n",
    "            shp_path = os.path.join(tdir, \"US.shp\")\n",
    "            if os.path.exists(shp_path):\n",
    "                print(\"Using vector points (shp): Model Training/US.shp\"); return shp_path\n",
    "    roots = [mt or ROOT_FOLDER_ID, ROOT_FOLDER_ID]\n",
    "    for root in roots:\n",
    "        for fid, items in _drive_walk(drive, root, max_depth=4):\n",
    "            stems={}\n",
    "            for it in items:\n",
    "                if not isinstance(it, dict): continue\n",
    "                title = it.get(\"title\",\"\"); m = re.match(rf\"(.+)\\.(shp|dbf|shx|prj|cpg|qpj)$\", title, re.IGNORECASE)\n",
    "                if not m: continue\n",
    "                stem = m.group(1)\n",
    "                if stem.lower() == POINTS_BASENAME.lower():\n",
    "                    stems.setdefault(stem,[]).append(it)\n",
    "            for stem, parts in stems.items():\n",
    "                if any(p.get(\"title\",\"\").lower().endswith(\".shp\") for p in parts):\n",
    "                    tdir = tempfile.mkdtemp()\n",
    "                    for p in parts: download_to_temp(p, os.path.join(tdir, p.get(\"title\",\"\")))\n",
    "                    shp_path = os.path.join(tdir, f\"{os.path.basename(stem)}.shp\")\n",
    "                    if os.path.exists(shp_path):\n",
    "                        print(f\"Using vector points (shp): {os.path.basename(stem)}.shp\"); return shp_path\n",
    "            for it in items:\n",
    "                if not isinstance(it, dict): continue\n",
    "                title = it.get(\"title\",\"\"); low = title.lower()\n",
    "                if it.get(\"mimeType\") == \"application/vnd.google-apps.folder\": continue\n",
    "                if low.endswith(\".gpkg\") and low == f\"{POINTS_BASENAME.lower()}.gpkg\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".gpkg\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (gpkg): {title}\"); return tmp\n",
    "                if low.endswith(\".zip\") and low == f\"{POINTS_BASENAME.lower()}.zip\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (zip): {title}\"); return tmp\n",
    "                if low.endswith(\".csv\") and low == f\"{POINTS_BASENAME.lower()}.csv\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (csv): {title}\"); return tmp\n",
    "    return None\n",
    "\n",
    "def _find_points_vector_local():\n",
    "    search_roots = [os.path.join(LOCAL_BASE_DIR, MODEL_FOLDER), LOCAL_BASE_DIR]\n",
    "    for root in search_roots:\n",
    "        if not os.path.isdir(root): continue\n",
    "        for nm in os.listdir(root):\n",
    "            low = nm.lower()\n",
    "            if low == f\"{POINTS_BASENAME.lower()}.csv\":\n",
    "                print(f\"Using vector points (csv): {nm}\"); return os.path.join(root, nm)\n",
    "            if POINTS_BASENAME.lower() in low and low.endswith(\".gpkg\"):\n",
    "                print(f\"Using vector points (gpkg): {nm}\"); return os.path.join(root, nm)\n",
    "            if POINTS_BASENAME.lower() in low and low.endswith(\".zip\"):\n",
    "                print(f\"Using vector points (zip): {nm}\"); return os.path.join(root, nm)\n",
    "    for root in search_roots:\n",
    "        if not os.path.isdir(root): continue\n",
    "        for nm in os.listdir(root):\n",
    "            if nm.lower().endswith(\".shp\") and POINTS_BASENAME.lower() in nm.lower():\n",
    "                print(f\"Using vector points (shp): {nm}\"); return os.path.join(root, nm)\n",
    "    return None\n",
    "\n",
    "def _ci_lookup(cols, name):\n",
    "    name_l = name.lower()\n",
    "    for c in cols:\n",
    "        if c.lower() == name_l: return c\n",
    "    return None\n",
    "\n",
    "def _round6(x):\n",
    "    return np.round(pd.to_numeric(x, errors=\"coerce\"), 6)\n",
    "\n",
    "def _load_gtps_cache_df(drive):\n",
    "    \"\"\"Load US_GTPS_per_point.(parquet|csv) from Model Training if present.\"\"\"\n",
    "    def _from_drive():\n",
    "        mt = get_or_create_folder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "        # prefer parquet\n",
    "        res = _drive_query(drive, f\"'{mt}' in parents and trashed=false and title='{GTPS_CACHE_PARQUET}'\")\n",
    "        if res:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".parquet\").name\n",
    "            download_to_temp(res[0], tmp)\n",
    "            try:\n",
    "                return pd.read_parquet(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "        res = _drive_query(drive, f\"'{mt}' in parents and trashed=false and title='{GTPS_CACHE_CSV}'\")\n",
    "        if res:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\").name\n",
    "            download_to_temp(res[0], tmp)\n",
    "            return pd.read_csv(tmp)\n",
    "        return None\n",
    "\n",
    "    def _from_local():\n",
    "        p1 = os.path.join(LOCAL_MODEL_DIR, GTPS_CACHE_PARQUET)\n",
    "        p2 = os.path.join(LOCAL_MODEL_DIR, GTPS_CACHE_CSV)\n",
    "        if os.path.exists(p1):\n",
    "            try:\n",
    "                return pd.read_parquet(p1)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if os.path.exists(p2):\n",
    "            return pd.read_csv(p2)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return _from_drive() if drive is not None else _from_local()\n",
    "    except Exception as e:\n",
    "        print(f\"   ! AEZ cache read error: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_points_df(drive):\n",
    "    import geopandas as gpd, fiona\n",
    "    path = _find_points_vector_drive(drive) if drive is not None else _find_points_vector_local()\n",
    "    if path is None:\n",
    "        raise RuntimeError(\"Could not locate US.(shp/zip/gpkg/csv) in 'Model Training' tree.\")\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        with fiona.Env(SHAPE_RESTORE_SHX=\"YES\"):\n",
    "            gdf = gpd.read_file(path)\n",
    "        if gdf.crs is not None and gdf.crs.to_epsg() != 4326:\n",
    "            gdf = gdf.to_crs(4326)\n",
    "        df = pd.DataFrame(gdf.drop(columns=\"geometry\", errors=\"ignore\"))\n",
    "\n",
    "    lon_col = _ci_lookup(df.columns, LONCOL); lat_col = _ci_lookup(df.columns, LATCOL)\n",
    "    if lon_col is None or lat_col is None:\n",
    "        raise RuntimeError(\"Points must contain longitude and latitude columns.\")\n",
    "    if lon_col != LONCOL: df[LONCOL] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "    if lat_col != LATCOL: df[LATCOL] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "\n",
    "    tgt_col = _ci_lookup(df.columns, TARGET)\n",
    "    if tgt_col is None: raise RuntimeError(f\"Points file missing column: {TARGET}\")\n",
    "    if tgt_col != TARGET: df[TARGET] = pd.to_numeric(df[tgt_col], errors=\"coerce\")\n",
    "\n",
    "    # pre-create rounders for cache join\n",
    "    df[\"_lonr\"] = _round6(df[LONCOL]); df[\"_latr\"] = _round6(df[LATCOL])\n",
    "\n",
    "    # copy a country-like column\n",
    "    if COUNTRY_COL not in df.columns:\n",
    "        cand = _pick_country_candidate(df.columns)\n",
    "        df[COUNTRY_COL] = df[cand] if cand else \"Unknown\"\n",
    "\n",
    "    for c in [TARGET, LONCOL, LATCOL]:\n",
    "        if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[TARGET, LONCOL, LATCOL]).reset_index(drop=True)\n",
    "    df[TARGET] = df[TARGET].astype(int)\n",
    "    df[COUNTRY_COL] = df[COUNTRY_COL].astype(str)\n",
    "\n",
    "    # Bring AEZ from cache if available\n",
    "    if AEZ_COL not in df.columns or df[AEZ_COL].isna().all():\n",
    "        cache = _load_gtps_cache_df(drive)\n",
    "        if cache is not None:\n",
    "            # normalize cache cols\n",
    "            cc = {c.lower(): c for c in cache.columns}\n",
    "            lonc = cc.get(LONCOL.lower(), None) or cc.get(\"x\") or cc.get(\"lon\") or list(cache.columns)[1]\n",
    "            latc = cc.get(LATCOL.lower(), None) or cc.get(\"y\") or cc.get(\"lat\") or list(cache.columns)[2]\n",
    "            aezc = cc.get(AEZ_COL.lower(), None) or cc.get(\"gtps\") or cc.get(\"aez\")\n",
    "            if aezc is not None:\n",
    "                cache[\"_lonr\"] = _round6(cache[lonc]); cache[\"_latr\"] = _round6(cache[latc])\n",
    "                df = df.merge(cache[[\"_lonr\",\"_latr\",aezc]].rename(columns={aezc:AEZ_COL}),\n",
    "                              on=[\"_lonr\",\"_latr\"], how=\"left\")\n",
    "                if AEZ_COL in df.columns:\n",
    "                    df[AEZ_COL] = pd.to_numeric(df[AEZ_COL], errors=\"coerce\")\n",
    "                    df[AEZ_COL] = np.rint(df[AEZ_COL]).astype(\"float64\")\n",
    "                    print(f\"   • AEZ filled from cache for {np.isfinite(df[AEZ_COL]).sum()}/{len(df)} points\")\n",
    "    return df\n",
    "\n",
    "# ---------------------- RASTERS -------------------------\n",
    "def open_country_var_path(drive, country, variable):\n",
    "    \"\"\"Any By Country/<Country>/<Country>_<variable>.tif (case-sensitive on names passed in).\"\"\"\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BY_COUNTRY_DIR, country, f\"{country}_{variable}.tif\")\n",
    "        return p if os.path.exists(p) else None\n",
    "    byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
    "    if not byc: return None\n",
    "    q = f\"'{byc}' in parents and trashed=false and title='{country}' and mimeType='application/vnd.google-apps.folder'\"\n",
    "    res = _drive_query(drive, q)\n",
    "    if not res: return None\n",
    "    cid = res[0][\"id\"]\n",
    "    q2 = f\"'{cid}' in parents and trashed=false and title='{country}_{variable}.tif'\"\n",
    "    files = _drive_query(drive, q2)\n",
    "    if files:\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "        return download_to_temp(files[0], tmp)\n",
    "    for it in list_files(drive, cid):\n",
    "        if isinstance(it, dict) and re.match(rf\"^{re.escape(country)}_{re.escape(variable)}.*\\.tif$\", it.get(\"title\",\"\"), re.IGNORECASE):\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            return download_to_temp(it, tmp)\n",
    "    return None\n",
    "\n",
    "def open_CroplandNE(drive):\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_CLE_DIR, \"Cropland_NE.tif\")\n",
    "        return p if os.path.exists(p) else None\n",
    "    items = list_files(drive, CLE_FOLDER_ID)\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict): continue\n",
    "        if it.get(\"title\",\"\").lower().startswith(\"Cropland_NE\") and it.get(\"title\",\"\").lower().endswith(\".tif\"):\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            return download_to_temp(it, tmp)\n",
    "    return None\n",
    "\n",
    "def _clip_bounds_from_reference(raster_path):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        b = rio_transform_bounds(src.crs, CRS.from_epsg(4326),\n",
    "                                 src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top, densify_pts=8)\n",
    "    return b\n",
    "\n",
    "def _pick_reference_raster_for_bounds(drive, country):\n",
    "    # Prefer any available per-country raster (including AEZ tile) to set the grid.\n",
    "    # IMPORTANT: do NOT fall back to global AEZ here, to avoid predicting outside the country.\n",
    "    for v in [\"CLE\"] + ALL_PREDICTORS + [AEZ_COL]:\n",
    "        if v == AEZ_COL:\n",
    "            p = open_country_var_path(drive, country, AEZ_COL)\n",
    "        else:\n",
    "            p = open_country_var_path(drive, country, v)\n",
    "        if p is not None:\n",
    "            return p\n",
    "    raise RuntimeError(f\"No per-country reference raster found for bounds in '{country}'.\")\n",
    "\n",
    "def country_grid_and_mask(drive, country):\n",
    "    ref_path = _pick_reference_raster_for_bounds(drive, country)\n",
    "    west, south, east, north = _clip_bounds_from_reference(ref_path)\n",
    "    H, W, transform, crs = _target_wgs84_grid_from_bounds(west, south, east, north, pixel_m=30.0)\n",
    "\n",
    "    # Prefer per-country cropland, but fall back to global CroplandNE if needed.\n",
    "    country_cropland = open_country_var_path(drive, country, \"CLE\")\n",
    "    if country_cropland is not None:\n",
    "        cropland = clip_cle_to_grid(drive, H, W, transform, crs, source_path=country_cropland)\n",
    "    else:\n",
    "        print(f\"   ! No per-country CLE for '{country}'; using global CroplandNE.tif as cropland mask.\")\n",
    "        cropland = clip_cle_to_grid(drive, H, W, transform, crs, source_path=None)\n",
    "\n",
    "    return (H, W, transform, crs, cropland, (west, south, east, north))\n",
    "\n",
    "def clip_cle_to_grid(drive, out_h, out_w, out_transform, out_crs, source_path=None):\n",
    "    cle = source_path or open_CroplandNE(drive)\n",
    "    if cle is None: return np.ones((out_h, out_w), dtype=bool)\n",
    "    arr = np.full((out_h, out_w), -9999.0, dtype=np.float32)\n",
    "    with rasterio.open(cle) as src:\n",
    "        reproject(source=rasterio.band(src, 1), destination=arr,\n",
    "                  src_transform=src.transform, src_crs=src.crs,\n",
    "                  dst_transform=out_transform, dst_crs=out_crs,\n",
    "                  dst_nodata=-9999.0, resampling=RioResampling.nearest)\n",
    "        nd = src.nodata\n",
    "    if nd is not None: arr = np.where(np.isclose(arr, nd), np.nan, arr)\n",
    "    arr = np.where(np.isclose(arr, -9999.0), np.nan, arr)\n",
    "    return np.isfinite(arr) & (arr > 0.5)\n",
    "\n",
    "# ---------------- THRESHOLDS -----------------------------\n",
    "def _thr_by_J(y_true, p):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    if np.unique(y).size < 2: return FIXED_THRESHOLD\n",
    "    fpr, tpr, th = roc_curve(y, p)\n",
    "    if len(th) < 2: return FIXED_THRESHOLD\n",
    "    j = tpr - fpr\n",
    "    return float(th[int(np.argmax(j))])\n",
    "\n",
    "def _thr_by_F1(y_true, p):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    ths = np.linspace(0.01, 0.99, 99); best_t, best_f1 = 0.5, -1\n",
    "    for t in ths:\n",
    "        yhat = (p >= t).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y, yhat, average=\"binary\", zero_division=0)\n",
    "        if f1 > best_f1: best_f1, best_t = f1, t\n",
    "    return float(best_t)\n",
    "\n",
    "def _thr_precision_at(y_true, p, target):\n",
    "    if target is None: return _thr_by_J(y_true, p)\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    cand = thr[prec[1:] >= target]\n",
    "    if len(cand) > 0:\n",
    "        return float(np.min(cand))  \n",
    "    return _thr_by_F1(y_true, p)\n",
    "\n",
    "def _thr_recall_at(y_true, p, target):\n",
    "    if target is None: return _thr_by_J(y_true, p)\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    cand = thr[rec[1:] >= target]\n",
    "    if len(cand) > 0:\n",
    "        return float(np.max(cand))  \n",
    "    return _thr_by_F1(y_true, p)\n",
    "\n",
    "def _pick_threshold(y_true, p):\n",
    "    s = THRESHOLD_STRATEGY.lower()\n",
    "    if s == \"fixed\":       return float(FIXED_THRESHOLD)\n",
    "    if s == \"f1\":          return _thr_by_F1(y_true, p)\n",
    "    if s == \"j\":           return _thr_by_J(y_true, p)\n",
    "    if s == \"precision_at\":return _thr_precision_at(y_true, p, PRECISION_TARGET)\n",
    "    if s == \"recall_at\":   return _thr_recall_at(y_true, p, RECALL_TARGET)\n",
    "    if PRECISION_TARGET is not None:\n",
    "        return _thr_precision_at(y_true, p, PRECISION_TARGET)\n",
    "    if RECALL_TARGET is not None:\n",
    "        return _thr_recall_at(y_true, p, RECALL_TARGET)\n",
    "    return _thr_by_J(y_true, p)\n",
    "\n",
    "# ---------------- Calibration helpers -------------------\n",
    "def _fit_calibrator(raw_pos_probs, y, method=\"isotonic\"):\n",
    "    raw_pos_probs = np.asarray(raw_pos_probs, dtype=float).ravel()\n",
    "    y = np.asarray(y, dtype=int).ravel()\n",
    "    if method == \"platt\":\n",
    "        lr = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "        lr.fit(raw_pos_probs.reshape(-1,1), y)\n",
    "        return {\"method\": \"platt\", \"model\": lr}\n",
    "    ir = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds=\"clip\")\n",
    "    ir.fit(raw_pos_probs, y)\n",
    "    return {\"method\": \"isotonic\", \"model\": ir}\n",
    "\n",
    "def _apply_calibrator(calib, raw_pos_probs):\n",
    "    raw_pos_probs = np.asarray(raw_pos_probs, dtype=float).ravel()\n",
    "    if calib is None:\n",
    "        return raw_pos_probs\n",
    "    if calib.get(\"method\") == \"platt\":\n",
    "        return calib[\"model\"].predict_proba(raw_pos_probs.reshape(-1,1))[:,1]\n",
    "    return calib[\"model\"].predict(raw_pos_probs)\n",
    "\n",
    "# ------------- POINT SAMPLING FROM RASTERS --------------\n",
    "def _sample_raster_at_lonlat(src, lon_arr, lat_arr):\n",
    "    xs = np.asarray(lon_arr, dtype=float); ys = np.asarray(lat_arr, dtype=float)\n",
    "    if src.crs and (src.crs.to_epsg() != 4326):\n",
    "        tx, ty = rio_transform(CRS.from_epsg(4326), src.crs, xs.tolist(), ys.tolist())\n",
    "        xs = np.asarray(tx, dtype=float); ys = np.asarray(ty, dtype=float)\n",
    "    out = np.full(xs.shape[0], np.nan, dtype=np.float32)\n",
    "    xmin, ymin, xmax, ymax = src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top\n",
    "    inside = (xs >= xmin) & (xs <= xmax) & (ys >= ymin) & (ys <= ymax)\n",
    "    if not np.any(inside): return out\n",
    "    idx = np.where(inside)[0]\n",
    "    coords = list(zip(xs[idx], ys[idx]))\n",
    "    vals = np.array([v[0] for v in src.sample(coords)], dtype=np.float32)\n",
    "    if src.nodata is not None and np.isfinite(src.nodata):\n",
    "        vals = np.where(np.isclose(vals, np.float32(src.nodata)), np.float32(np.nan), vals)\n",
    "    out[idx] = vals; return out\n",
    "\n",
    "# ---- TRAINING FILL (num predictors + AEZ) ---------------\n",
    "def _fill_training_predictors_from_country_rasters(drive, df_all, countries):\n",
    "    df = df_all.copy()\n",
    "\n",
    "    # AEZ: if still missing after cache merge, try per-country tiles, else global\n",
    "    if AEZ_COL not in df.columns or df[AEZ_COL].isna().any():\n",
    "        miss = df.index if AEZ_COL not in df.columns else df.index[df[AEZ_COL].isna()]\n",
    "        if len(miss) > 0:\n",
    "            aez_filled = 0\n",
    "            # try per-country tiles first\n",
    "            for c in countries:\n",
    "                try:\n",
    "                    ref = open_country_var_path(drive, c, AEZ_COL) or open_aez_path(drive)\n",
    "                    if not ref: continue\n",
    "                    w, s, e, n = _clip_bounds_from_reference(ref)\n",
    "                    idx = df.loc[miss].index[\n",
    "                        df.loc[miss, LONCOL].between(min(w,e), max(w,e)) &\n",
    "                        df.loc[miss, LATCOL].between(min(s,n), max(s,n))\n",
    "                    ]\n",
    "                    if len(idx) == 0: continue\n",
    "                    with rasterio.open(ref) as aez_src:\n",
    "                        vals = _sample_raster_at_lonlat(aez_src, df.loc[idx, LONCOL].values, df.loc[idx, LATCOL].values)\n",
    "                    df.loc[idx, AEZ_COL] = np.rint(pd.to_numeric(vals, errors=\"coerce\")).astype(\"float64\")\n",
    "                    aez_filled += int(np.isfinite(df.loc[idx, AEZ_COL]).sum())\n",
    "                except Exception as ex:\n",
    "                    print(f\"   ! AEZ fill skip for {c}: {ex}\")\n",
    "            if aez_filled > 0:\n",
    "                print(f\"   • AEZ sampled from tiles/global for {aez_filled} points\")\n",
    "\n",
    "    # Per-country numeric predictors from rasters\n",
    "    for c in countries:\n",
    "        try:\n",
    "            ref = open_country_var_path(drive, c, \"CLE\") or open_country_var_path(drive, c, \"NDVI_mean\")\n",
    "            if not ref: continue\n",
    "            w, s, e, n = _clip_bounds_from_reference(ref)\n",
    "        except Exception as ex:\n",
    "            print(f\"   ! Skip raster fill for {c}: {ex}\"); continue\n",
    "        idx = df.index[df[LONCOL].between(min(w,e), max(w,e)) & df[LATCOL].between(min(s,n), max(s,n))]\n",
    "        if len(idx) == 0: continue\n",
    "        raster_cache = {}; filled_any = False\n",
    "        for p in ALL_PREDICTORS:\n",
    "            if p == AEZ_COL: continue\n",
    "            pth = open_country_var_path(drive, c, p)\n",
    "            if pth is None: continue\n",
    "            if p not in raster_cache: raster_cache[p] = rasterio.open(pth)\n",
    "            vals = _sample_raster_at_lonlat(raster_cache[p], df.loc[idx, LONCOL].values, df.loc[idx, LATCOL].values)\n",
    "            df.loc[idx, p] = pd.to_numeric(vals, errors=\"coerce\").astype(\"float64\")\n",
    "            filled_any = True\n",
    "        if filled_any:\n",
    "            print(f\"   • Training fill from rasters for {c}: {len(idx)} points\")\n",
    "    return df\n",
    "\n",
    "def list_countries_auto(drive):\n",
    "    names = set()\n",
    "    if drive is None:\n",
    "        if os.path.isdir(LOCAL_BY_COUNTRY_DIR):\n",
    "            for d in os.listdir(LOCAL_BY_COUNTRY_DIR):\n",
    "                if os.path.isdir(os.path.join(LOCAL_BY_COUNTRY_DIR, d)): names.add(d)\n",
    "    else:\n",
    "        byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
    "        if byc:\n",
    "            subs = _drive_query(drive, f\"'{byc}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "            for it in subs: names.add(it[\"title\"])\n",
    "    return sorted(names)\n",
    "\n",
    "# ----------------- TRAIN REGIONAL MODEL -----------------\n",
    "def _borrow_neighbors(df_all, base_cols, max_points):\n",
    "    if COUNTRY_COL not in df_all.columns:\n",
    "        df_all[COUNTRY_COL] = \"Unknown\"\n",
    "    keep_cols = list(dict.fromkeys(base_cols))\n",
    "    pooled = df_all[keep_cols].dropna(subset=[TARGET, LONCOL, LATCOL]).copy()\n",
    "    if len(pooled) > max_points:\n",
    "        pooled = pooled.sample(max_points, random_state=RANDOM_SEED)\n",
    "    if pooled[TARGET].nunique() < 2:\n",
    "        other = 1 - int(pooled[TARGET].iloc[0])\n",
    "        extra = df_all[df_all[TARGET] == other]\n",
    "        if not extra.empty:\n",
    "            take = min(1000, len(extra))\n",
    "            pooled = pd.concat([pooled, extra.sample(take, random_state=RANDOM_SEED)[keep_cols]], ignore_index=True)\n",
    "    if COUNTRY_COL not in pooled.columns:\n",
    "        pooled[COUNTRY_COL] = \"Unknown\"\n",
    "    pooled[COUNTRY_COL] = pooled[COUNTRY_COL].astype(str)\n",
    "    return pooled.reset_index(drop=True)\n",
    "\n",
    "def train_region_model(drive, region_name, df_all, countries):\n",
    "    print(f\"=== Train regional model: {region_name} ===\")\n",
    "\n",
    "    # Fill predictors (uses AEZ cache + per-country AEZ tiles)\n",
    "    df_all = _fill_training_predictors_from_country_rasters(drive, df_all, countries)\n",
    "\n",
    "    use_preds = [p for p in ALL_PREDICTORS if p in df_all.columns]\n",
    "    base_cols = [TARGET, LONCOL, LATCOL, COUNTRY_COL] + use_preds\n",
    "    df = _borrow_neighbors(df_all, base_cols, BORROW_MAX)\n",
    "\n",
    "    if len(df) < 200 or df[TARGET].nunique() < 2:\n",
    "        raise RuntimeError(f\"Insufficient pooled points for region training (n={len(df)}, classes={df[TARGET].nunique()}).\")\n",
    "\n",
    "    num_features = [f for f in use_preds if f != AEZ_COL]\n",
    "    cat_features = [AEZ_COL] if AEZ_COL in use_preds else []\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", make_pipeline(SimpleImputer(strategy=\"median\")), num_features),\n",
    "            (\"cat\", make_pipeline(SimpleImputer(strategy=\"most_frequent\"),\n",
    "                                  OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)), cat_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=RF_TREES, max_features=\"sqrt\", class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1, random_state=RANDOM_SEED, bootstrap=True,\n",
    "    )\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"rf\", rf)])\n",
    "\n",
    "    # ---------- Group-aware OOF to fit calibrator ----------\n",
    "    df[\"farm_id\"] = assign_farm_ids(df, lon_col=LONCOL, lat_col=LATCOL)\n",
    "    n_splits = 5 if 0.19 <= TEST_FRACTION <= 0.21 else max(3, int(round(1.0 / max(TEST_FRACTION, 1e-3))))\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    X_all = df[use_preds].copy()\n",
    "    y_all = df[TARGET].astype(int).values\n",
    "    g_all = df[\"farm_id\"].values\n",
    "    countries_all = df[COUNTRY_COL].astype(str).values\n",
    "    lon_all = df[LONCOL].values\n",
    "    lat_all = df[LATCOL].values\n",
    "\n",
    "    # enforce dtypes\n",
    "    for col in [f for f in use_preds if f != AEZ_COL]:\n",
    "        X_all[col] = pd.to_numeric(X_all[col], errors=\"coerce\").astype(\"float64\")\n",
    "    if AEZ_COL in use_preds:\n",
    "        X_all[AEZ_COL] = pd.to_numeric(X_all[AEZ_COL], errors=\"coerce\")\n",
    "        X_all[AEZ_COL] = np.rint(X_all[AEZ_COL]).astype(\"float64\")\n",
    "\n",
    "    oof_raw = np.full(len(X_all), np.nan, dtype=float)\n",
    "    classes_ = None\n",
    "    for tr_idx, te_idx in sgkf.split(X_all, y_all, groups=g_all):\n",
    "        tr_X = X_all.iloc[tr_idx]; tr_y = y_all[tr_idx]\n",
    "        te_X = X_all.iloc[te_idx]\n",
    "        pipe_fold = clone(pipe)\n",
    "        pipe_fold.fit(tr_X, tr_y.astype(int))\n",
    "        if classes_ is None:\n",
    "            classes_ = pipe_fold.named_steps[\"rf\"].classes_\n",
    "        pos_idx_fold = int(np.where(classes_ == 1)[0][0])\n",
    "        oof_raw[te_idx] = pipe_fold.predict_proba(te_X)[:, pos_idx_fold]\n",
    "\n",
    "    # fit final model on all data\n",
    "    pipe.fit(X_all, y_all.astype(int))\n",
    "    classes_ = pipe.named_steps[\"rf\"].classes_\n",
    "    pos_idx = int(np.where(classes_ == 1)[0][0])\n",
    "\n",
    "    # calibrate on OOF\n",
    "    calib = _fit_calibrator(oof_raw, y_all, CALIBRATION_METHOD)\n",
    "    oof_cal = _apply_calibrator(calib, oof_raw)\n",
    "\n",
    "    # ---------- GLOBAL threshold from OOF calibrated ----------\n",
    "    global_thr = _pick_threshold(y_all, oof_cal)\n",
    "\n",
    "    # ---------- Regional OOF metrics & confusion ----------\n",
    "    reg_metrics, reg_cm = _compute_basic_metrics(y_all, oof_cal, global_thr)\n",
    "    reg_metrics.update({\n",
    "        \"region\": region_name,\n",
    "        \"n_oof\": int(len(y_all)),\n",
    "        \"threshold_strategy\": THRESHOLD_STRATEGY,\n",
    "        \"global_threshold\": float(global_thr),\n",
    "        \"calibration\": CALIBRATION_METHOD,\n",
    "    })\n",
    "\n",
    "    # feature importances from final model\n",
    "    out_names = list(pipe.named_steps[\"pre\"].get_feature_names_out())\n",
    "    rf_imp = pipe.named_steps[\"rf\"].feature_importances_\n",
    "    fi_df = pd.DataFrame({\"feature\": out_names, \"importance\": rf_imp}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    # ---------- Prep folders ----------\n",
    "    if drive is None:\n",
    "        reg_base = os.path.join(LOCAL_MODEL_DIR, REGIONAL_MODELS_FOLDER, region_name)\n",
    "        os.makedirs(reg_base, exist_ok=True)\n",
    "        c_base_root = os.path.join(LOCAL_MODEL_DIR, COUNTRY_MODELS_FOLDER)\n",
    "        os.makedirs(c_base_root, exist_ok=True)\n",
    "    else:\n",
    "        mt = get_or_create_folder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "        reg = get_or_create_folder(drive, mt, REGIONAL_MODELS_FOLDER)\n",
    "        reg_base = get_or_create_folder(drive, reg, region_name)\n",
    "        c_base_root = get_or_create_folder(drive, mt, COUNTRY_MODELS_FOLDER)\n",
    "\n",
    "    # ---------- Ensure country tags (bbox fallback) ----------\n",
    "    assigned = countries_all.copy()\n",
    "    unknown_mask = (assigned == \"Unknown\") | (assigned == \"\") | pd.isna(assigned)\n",
    "    if unknown_mask.any():\n",
    "        for ctry in countries:\n",
    "            try:\n",
    "                ref = open_country_var_path(drive, ctry, \"CLE\") or open_country_var_path(drive, ctry, AEZ_COL) or open_aez_path(drive)\n",
    "                if not ref: continue\n",
    "                w, s, e, n = _clip_bounds_from_reference(ref)\n",
    "                inside = unknown_mask & (lon_all >= min(w, e)) & (lon_all <= max(w, e)) \\\n",
    "                                   & (lat_all >= min(s, n)) & (lat_all <= max(s, n))\n",
    "                assigned[inside] = ctry\n",
    "            except Exception:\n",
    "                pass\n",
    "        countries_all = assigned\n",
    "\n",
    "    # ---------- Per-country thresholds & OOF metrics ----------\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"country\": countries_all.astype(str),\n",
    "        \"y\": y_all.astype(int),\n",
    "        \"prob_cal\": oof_cal.astype(float),\n",
    "    })\n",
    "\n",
    "    per_rows = []\n",
    "    per_thr = {}\n",
    "\n",
    "    for ctry in sorted(set(countries)):\n",
    "        sub = oof_df[oof_df[\"country\"] == ctry]\n",
    "        n_total = int(len(sub))\n",
    "        n_pos = int((sub[\"y\"] == 1).sum())\n",
    "        n_neg = n_total - n_pos\n",
    "\n",
    "        if (n_total < MIN_TOTAL_FOR_COUNTRY) or (n_pos < MIN_POS_NEG_FOR_COUNTRY) or (n_neg < MIN_POS_NEG_FOR_COUNTRY):\n",
    "            thr_c = float(global_thr); note = \"fallback_global\"\n",
    "        else:\n",
    "            thr_c = _pick_threshold(sub[\"y\"].values, sub[\"prob_cal\"].values); note = \"oof_calibrated\"\n",
    "\n",
    "        per_thr[ctry] = float(thr_c)\n",
    "        m_c, cm_c = _compute_basic_metrics(sub[\"y\"].values, sub[\"prob_cal\"].values, thr_c)\n",
    "        m_c.update({\n",
    "            \"region\": region_name, \"country\": ctry, \"n_oof\": n_total,\n",
    "            \"n_pos\": n_pos, \"n_neg\": n_neg, \"threshold_used\": float(thr_c),\n",
    "            \"note\": note\n",
    "        })\n",
    "        per_rows.append(m_c)\n",
    "\n",
    "        # save country artifacts (metrics + confusion + threshold.json)\n",
    "        cdir = get_or_create_folder(drive, c_base_root, ctry)\n",
    "        with tempfile.TemporaryDirectory() as tdir:\n",
    "            with open(os.path.join(tdir, \"test_metrics.json\"), \"w\") as f:\n",
    "                json.dump(m_c, f, indent=2)\n",
    "            upload_path(drive, os.path.join(tdir, \"test_metrics.json\"), cdir, \"test_metrics.json\")\n",
    "\n",
    "            png = os.path.join(tdir, \"test_confusion.png\")\n",
    "            _save_confusion_png(cm_c, f\"{ctry} — OOF Confusion (thr={thr_c:.3f})\", png)\n",
    "            upload_path(drive, png, cdir, \"test_confusion.png\")\n",
    "\n",
    "            with open(os.path.join(tdir, \"threshold.json\"), \"w\") as f:\n",
    "                json.dump({\n",
    "                    \"threshold_strategy\": THRESHOLD_STRATEGY,\n",
    "                    \"mode\": THRESHOLD_MODE,\n",
    "                    \"calibration\": CALIBRATION_METHOD,\n",
    "                    \"threshold\": float(thr_c),\n",
    "                    \"n_total\": n_total, \"n_pos\": n_pos, \"n_neg\": n_neg,\n",
    "                    \"note\": note\n",
    "                }, f, indent=2)\n",
    "            upload_path(drive, os.path.join(tdir, \"threshold.json\"), cdir, \"threshold.json\")\n",
    "\n",
    "    # regional CSV summary for per-country\n",
    "    if len(per_rows) > 0:\n",
    "        per_df = pd.DataFrame(per_rows).sort_values(\"country\")\n",
    "    else:\n",
    "        per_df = pd.DataFrame(columns=[\"country\",\"n_oof\",\"precision\",\"recall\",\"f1\",\"roc_auc\",\"pr_auc\",\n",
    "                                       \"tn\",\"fp\",\"fn\",\"tp\",\"threshold_used\",\"note\"])\n",
    "\n",
    "    # ---------- Save regional artifacts ----------\n",
    "    with tempfile.TemporaryDirectory() as tdir:\n",
    "        with open(os.path.join(tdir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(reg_metrics, f, indent=2)\n",
    "        upload_path(drive, os.path.join(tdir, \"metrics.json\"), reg_base, \"metrics.json\")\n",
    "\n",
    "        fi_df.to_csv(os.path.join(tdir, \"feature_importance.csv\"), index=False)\n",
    "        upload_path(drive, os.path.join(tdir, \"feature_importance.csv\"), reg_base, \"feature_importance.csv\")\n",
    "\n",
    "        per_df.to_csv(os.path.join(tdir, \"per_country_test_metrics.csv\"), index=False)\n",
    "        upload_path(drive, os.path.join(tdir, \"per_country_test_metrics.csv\"), reg_base, \"per_country_test_metrics.csv\")\n",
    "\n",
    "        png = os.path.join(tdir, \"confusion.png\")\n",
    "        _save_confusion_png(reg_cm, f\"{region_name} — OOF Confusion ({THRESHOLD_STRATEGY}, thr={global_thr:.3f})\", png)\n",
    "        upload_path(drive, png, reg_base, \"confusion.png\")\n",
    "\n",
    "        joblib.dump(\n",
    "            {\n",
    "                \"pipeline\": pipe,\n",
    "                \"predictors\": use_preds,\n",
    "                \"positive_class_index\": int(pos_idx),\n",
    "                \"calibration\": calib,\n",
    "                \"global_threshold\": float(global_thr),\n",
    "                \"threshold_strategy\": THRESHOLD_STRATEGY,\n",
    "                \"per_country_thresholds\": per_thr,\n",
    "                \"num_features\": [f for f in use_preds if f != AEZ_COL],\n",
    "                \"cat_features\": [AEZ_COL] if AEZ_COL in use_preds else [],\n",
    "            },\n",
    "            os.path.join(tdir, \"model.joblib\"),\n",
    "        )\n",
    "        upload_path(drive, os.path.join(tdir, \"model.joblib\"), reg_base, \"model.joblib\")\n",
    "\n",
    "    model = {\n",
    "        \"pipe\": pipe,\n",
    "        \"use_preds\": use_preds,\n",
    "        \"pos_idx\": int(pos_idx),\n",
    "        \"calib\": calib,\n",
    "        \"global_thr\": float(global_thr),\n",
    "        \"per_thr\": per_thr,\n",
    "        \"reg_base\": reg_base\n",
    "    }\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------- PREDICT ONE COUNTRY ------------------\n",
    "def predict_country(drive, country, model, thr, grid_pack):\n",
    "    out_dir = get_or_create_folder(drive, ROOT_FOLDER_ID, OUTPUT_FOLDER) if drive else LOCAL_OUTPUT_DIR\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    pipe   = model[\"pipe\"]\n",
    "    pos_idx= model[\"pos_idx\"]\n",
    "    calib  = model[\"calib\"]\n",
    "\n",
    "    H, W, transform, crs, cropland = grid_pack\n",
    "    raster_cache = {}\n",
    "\n",
    "    # AEZ: prefer per-country tile, fallback to global AEZ\n",
    "    aez_src_path = open_country_var_path(drive, country, AEZ_COL) or open_aez_path(drive)\n",
    "    if AEZ_COL in model[\"use_preds\"] and aez_src_path is not None:\n",
    "        raster_cache[AEZ_COL] = rasterio.open(aez_src_path)\n",
    "\n",
    "    for v in model[\"use_preds\"]:\n",
    "        if v == AEZ_COL: continue\n",
    "        p = open_country_var_path(drive, country, v)\n",
    "        if p is not None: raster_cache[v] = rasterio.open(p)\n",
    "\n",
    "    base_profile = {\n",
    "        \"driver\": \"GTiff\",\"height\": H,\"width\": W,\"count\": 1,\"crs\": crs,\"transform\": transform,\n",
    "        \"compress\": \"LZW\",\"tiled\": True,\"blockxsize\": 512,\"blockysize\": 512,\n",
    "    }\n",
    "    prob_profile = {**base_profile, \"dtype\": \"float32\", \"nodata\": -9999.0}\n",
    "    bin_profile  = {**base_profile, \"dtype\": \"uint8\", \"nodata\": 255}\n",
    "    cnt_profile  = {**base_profile, \"dtype\": \"uint8\", \"nodata\": 0}\n",
    "\n",
    "    prob_name = f\"{country}_RF_probability_percent.tif\"\n",
    "    bin_name  = f\"{country}_RF_binary_0_1_cropland.tif\"\n",
    "    cnt_name  = f\"{country}_RF_predictors_count.tif\"\n",
    "\n",
    "    eligible_total = 0; positive_total = 0\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tdir:\n",
    "        prob_tmp = os.path.join(tdir, prob_name)\n",
    "        bin_tmp  = os.path.join(tdir, bin_name)\n",
    "        cnt_tmp  = os.path.join(tdir, cnt_name)\n",
    "        with rasterio.open(prob_tmp, \"w\", **prob_profile) as dst_prob, \\\n",
    "             rasterio.open(bin_tmp,  \"w\", **bin_profile)  as dst_bin,  \\\n",
    "             rasterio.open(cnt_tmp,  \"w\", **cnt_profile) as dst_cnt:\n",
    "\n",
    "            for win in tqdm(_iter_tiles(H, W, TILE_SIZE), total=_tile_count(H, W, TILE_SIZE), desc=f\"Predict {country}\"):\n",
    "                r0, r1 = int(win.row_off), int(win.row_off + win.height)\n",
    "                c0, c1 = int(win.col_off), int(win.col_off + win.width)\n",
    "                tile_h, tile_w = (r1 - r0), (c1 - c0)\n",
    "                tile_transform = transform * Affine.translation(c0, r0)\n",
    "\n",
    "                crop_tile = cropland[r0:r1, c0:c1]\n",
    "                if not crop_tile.any():\n",
    "                    dst_prob.write(np.full((1, tile_h, tile_w), prob_profile[\"nodata\"], np.float32), window=win)\n",
    "                    dst_bin.write (np.full((1, tile_h, tile_w), bin_profile[\"nodata\"], np.uint8), window=win)\n",
    "                    dst_cnt.write (np.zeros((1, tile_h, tile_w), np.uint8), window=win)\n",
    "                    continue\n",
    "\n",
    "                stack = np.full((len(model[\"use_preds\"]), tile_h, tile_w), np.nan, dtype=np.float32)\n",
    "                valid_count = np.zeros((tile_h, tile_w), dtype=np.uint8)\n",
    "\n",
    "                for j, v in enumerate(model[\"use_preds\"]):\n",
    "                    src = raster_cache.get(v)\n",
    "                    if src is None:\n",
    "                        p = open_country_var_path(drive, country, v) if v != AEZ_COL else (open_country_var_path(drive, country, AEZ_COL) or open_aez_path(drive))\n",
    "                        if p: src = raster_cache[v] = rasterio.open(p)\n",
    "                        else: continue\n",
    "                    dst_arr = np.full((tile_h, tile_w), np.nan, dtype=np.float32)\n",
    "                    dst_nd  = -9999.0\n",
    "                    resamp  = PER_BAND_RESAMPLING.get(v, RESAMPLING)\n",
    "                    try:\n",
    "                        reproject(source=rasterio.band(src, 1), destination=dst_arr,\n",
    "                                  src_transform=src.transform, src_crs=src.crs,\n",
    "                                  dst_transform=tile_transform, dst_crs=crs,\n",
    "                                  dst_nodata=dst_nd, resampling=resamp)\n",
    "                    except Exception as e:\n",
    "                        print(f\"      ! reprojection failed for {v}: {e}\"); continue\n",
    "                    if src.nodata is not None:\n",
    "                        dst_arr = np.where(np.isclose(dst_arr, src.nodata), np.nan, dst_arr)\n",
    "                    dst_arr = np.where(np.isclose(dst_arr, dst_nd), np.nan, dst_arr)\n",
    "                    stack[j] = dst_arr; valid_count += np.isfinite(dst_arr)\n",
    "\n",
    "                req = (len(model[\"use_preds\"]) if STRICT_ALL_BANDS else min(MIN_PREDICTORS, len(model[\"use_preds\"])))\n",
    "                good = crop_tile & (valid_count >= req)\n",
    "                prob_tile = np.full((tile_h, tile_w), np.nan, dtype=np.float32)\n",
    "\n",
    "                if good.any():\n",
    "                    X = stack.reshape(len(model[\"use_preds\"]), -1).T\n",
    "                    good_flat = good.reshape(-1)\n",
    "                    X_good = X[good_flat].astype(\"float64\")\n",
    "                    X_good_df = pd.DataFrame(X_good, columns=model[\"use_preds\"])\n",
    "\n",
    "                    # enforce dtypes\n",
    "                    num_features = [f for f in model[\"use_preds\"] if f != AEZ_COL]\n",
    "                    for col in num_features:\n",
    "                        X_good_df[col] = pd.to_numeric(X_good_df[col], errors=\"coerce\").astype(\"float64\")\n",
    "                    if AEZ_COL in X_good_df.columns:\n",
    "                        X_good_df[AEZ_COL] = pd.to_numeric(X_good_df[AEZ_COL], errors=\"coerce\")\n",
    "                        X_good_df[AEZ_COL] = np.rint(X_good_df[AEZ_COL]).astype(\"float64\")\n",
    "\n",
    "                    prob_raw = pipe.predict_proba(X_good_df)[:, pos_idx].astype(np.float32)\n",
    "                    prob_vals = _apply_calibrator(calib, prob_raw).astype(np.float32)\n",
    "\n",
    "                    bin_vals  = (prob_vals >= thr).astype(np.uint8)\n",
    "                    prob_tile.reshape(-1)[good_flat] = prob_vals\n",
    "\n",
    "                    mask_arr = np.isfinite(prob_tile)\n",
    "                    prob_out = np.full((1, tile_h, tile_w), prob_profile[\"nodata\"], np.float32)\n",
    "                    prob_out[0][mask_arr] = (prob_tile[mask_arr] * 100.0).astype(np.float32)\n",
    "                    dst_prob.write(prob_out, window=win)\n",
    "\n",
    "                    bin_out = np.full((1, tile_h, tile_w), bin_profile[\"nodata\"], np.uint8)\n",
    "                    bin_out[0].reshape(-1)[good_flat] = bin_vals\n",
    "                    dst_bin.write(bin_out, window=win)\n",
    "\n",
    "                    eligible_total += int(mask_arr.sum()); positive_total += int(bin_vals.sum())\n",
    "                else:\n",
    "                    dst_prob.write(np.full((1, tile_h, tile_w), prob_profile[\"nodata\"], np.float32), window=win)\n",
    "                    dst_bin.write (np.full((1, tile_h, tile_w), bin_profile[\"nodata\"], np.uint8), window=win)\n",
    "\n",
    "                dst_cnt.write(valid_count[np.newaxis], window=win)\n",
    "\n",
    "        upload_path(drive, prob_tmp, out_dir, prob_name)\n",
    "        upload_path(drive, bin_tmp,  out_dir, bin_name)\n",
    "        upload_path(drive, cnt_tmp,  out_dir, cnt_name)\n",
    "\n",
    "    frac = (positive_total / max(1, eligible_total)) * 100.0\n",
    "    print(f\"   • predicted irrigated fraction inside cropland: {frac:.3f}% ({positive_total:,}/{eligible_total:,})\")\n",
    "\n",
    "# ----------------------------- RUNNER -------------------\n",
    "def run_region_then_countries(use_drive=True, countries=None, region_name=\"US\"):\n",
    "    if use_drive:\n",
    "        try:\n",
    "            drive  # noqa: F821\n",
    "            _drive = drive\n",
    "            try: _drive.auth.service.http.timeout = 120\n",
    "            except Exception: pass\n",
    "        except NameError:\n",
    "            raise RuntimeError(\"PyDrive 'drive' not found. Authenticate and expose 'drive', or set use_drive=False.\")\n",
    "    else:\n",
    "        _drive = None\n",
    "        os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    df_all = load_points_df(_drive)\n",
    "    for c in [TARGET, LONCOL, LATCOL] + [p for p in ALL_PREDICTORS if p in df_all.columns and p != AEZ_COL]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    df_all = df_all.dropna(subset=[TARGET, LONCOL, LATCOL]).reset_index(drop=True)\n",
    "    df_all[TARGET] = df_all[TARGET].astype(int)\n",
    "\n",
    "    if not countries:\n",
    "        countries = [\n",
    "            \"Sudan\",\"Chad\",\"Central_African_Republic\", \"Cameroon\", \"Ethiopia\",\n",
    "            # \"Niger\",\n",
    "            # \"Niger\", \n",
    "            # \"Cote_d_Ivoire\" , \n",
    "            # \"Niger\"\n",
    "            # \"Mali\",\n",
    "\n",
    "        ]\n",
    "\n",
    "    model = train_region_model(_drive, region_name, df_all, countries)\n",
    "\n",
    "    # Predict\n",
    "    print(f\"=== PREDICTION per-country (threshold mode: {THRESHOLD_MODE}) ===\")\n",
    "    for c in countries:\n",
    "        try:\n",
    "            H, W, transform, crs, cropland, _ = country_grid_and_mask(_drive, c)\n",
    "            thr_c = model[\"per_thr\"].get(c, model[\"global_thr\"]) if THRESHOLD_MODE in (\"country\",\"hybrid\") else model[\"global_thr\"]\n",
    "            predict_country(_drive, c, model, thr_c, (H, W, transform, crs, cropland))\n",
    "            print(f\"✓ {c}: maps → {OUTPUT_FOLDER}, model → {MODEL_FOLDER}/{REGIONAL_MODELS_FOLDER}/{region_name}, thr={thr_c:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {c}: {e}\")\n",
    "\n",
    "    print(\"✅ Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_region_then_countries(use_drive=True, countries=None, region_name=\"Africa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301e4d6-1dd7-434f-9be3-897c8c6eacb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d33058-d624-42d1-a688-39ae50d70a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76772a-ce71-428f-a27b-2d2ada7ad0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420925f7-715c-4857-be56-09616ba85d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
