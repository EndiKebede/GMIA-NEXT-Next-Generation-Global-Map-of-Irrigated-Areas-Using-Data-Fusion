{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958cd337-e695-4236-b46b-1adce6d61e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google Drive connected\n"
     ]
    }
   ],
   "source": [
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "# Use the exact path to your JSON. If it's next to the notebook, just the filename is fine.\n",
    "CLIENT_JSON = \"client_secret_Final.json\"   # or \"/home/endiabe/client_secrets.json\"\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.LoadClientConfigFile(CLIENT_JSON)\n",
    "\n",
    "# Command-line auth -> opens a URL; you paste back the code\n",
    "gauth.CommandLineAuth()\n",
    "\n",
    "# Save token so you won't need to sign in every time\n",
    "gauth.SaveCredentialsFile('token.json')\n",
    "\n",
    "drive = GoogleDrive(gauth)\n",
    "print(\"✅ Google Drive connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfd2afd-9f0b-4960-9922-f856e514bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# match your Slurm allocation (\"64\")\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"64\"        # OpenMP (NumPy/SciPy, some raster ops)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"64\"        # MKL-backed NumPy / scikit-learn\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"64\"    # if you use numexpr\n",
    "os.environ[\"GDAL_NUM_THREADS\"] = \"64\"       # for internal GDAL threaded ops\n",
    "os.environ[\"RASTERIO_NUM_THREADS\"] = \"64\"   # rasterio’s thread pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fe644d-eb10-4636-9023-cef55817675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: Nigeria_VIs_Env_GrowingSeason  |  ID: 1VYsHaK4CXu3wo_UZIN6Kv44qan8d1smR\n",
      "Folder: Nigeria_VIs_Env_GrowingSeason  |  ID: 1nUAHXZ2zKpT22jIsCVHJahGDa7-7bCoW\n",
      "Folder: .ipynb_checkpoints  |  ID: 1veaaOV_OobM00bScEiQiY8pDoJSxFIC0\n",
      "Folder: Continental Irr Maps  |  ID: 1atiKrbLV7Ifez9AKTuSqCFMsTxkOqOvc\n",
      "Folder: Moldova_VIs_Env_GrowingSeason  |  ID: 183w10O5hg6pS6ou2gT7LE1N4SI1pSn5B\n",
      "Folder: Cuba_VIs_Env_IrrigationSeason  |  ID: 1OrL9zVhyRh57oLfvg84ySJhB0GZSCM6m\n",
      "Folder: Caribbean_VIs_Env_IrrigationSeason  |  ID: 1vsKprJVxyyjHR6zPcfYV1Rc1wkjIw2fn\n",
      "Folder: GMIE  |  ID: 1lVkH_OShkP5n_9CI4bdJgwFuHwmbx57H\n",
      "Folder: CLE_National  |  ID: 1pOpf-Zy5la4SKTIcrmLX6tGlxVBejKYX\n",
      "Folder: AfricaEnv_WA_noPrecip_NoSoil_NoCropMask  |  ID: 1KGWrOMmhnCVvUFr7uh9y2wyjPdLkRFDd\n",
      "Folder: Europe_VIs_Env2  |  ID: 1Rz21QLD1UnspShqUD7jB6Bfsgjk03Z9e\n",
      "Folder: China  |  ID: 19Ee1kUar4oEabBObq7GcA-jDR7F29x2_\n",
      "Folder: Canada  |  ID: 1qXhq3-3-J2YhKjXmMcCOT0exvb12LxhD\n",
      "Folder: Global_Irr_Maps  |  ID: 1XKYX2EPkPmu1el4Q0FpfZMEkgsWvRuYD\n",
      "Folder: Validation Dataset  |  ID: 1yHuAWZZdz5KsWxvEXPw4nxiRcKgYPtp8\n",
      "Folder: SouthAmerica_VIs_Env  |  ID: 1hitOuuj27P1_Lleo4Dhl630Q-TCcS_b6\n",
      "Folder: Canada_Irrigation_RF  |  ID: 1JUJ8xSgSiuccxHd80eiVYfLgwjZcPS5b\n",
      "Folder: US_VIs_Env  |  ID: 12LaYV-UY6G9lMeKMKk7y-RXFsAoClXIh\n",
      "Folder: China_VIs_Env_bySeason-2024  |  ID: 1Ed9_qQ-qYFN-weL1Tb_2B8mFnCXdtJsG\n",
      "Folder: Chile  |  ID: 1jExB0R7pXMH86NdZG_83qAylZqRpi931\n",
      "Folder: China_VIs_Env_bySeason_new  |  ID: 17a8xF2qvnHjw_Neu8IPtP_-VjOtbvIkd\n",
      "Folder: Oceania_VIs_Env_new_2  |  ID: 1vKJ5o-i8V2K3TUswyTtrBvnMGkhURxiW\n",
      "Folder: Global Water Scarcity  |  ID: 1VQiSgaEykgtdP1GUBQ7rwRvaTa_OICxx\n",
      "Folder: John MIRCA-OS 2020  |  ID: 1b13mul-bIexA3uA8ln6QpgWHVxp76FB2\n",
      "Folder: Big Idea Challenge  |  ID: 1iq2UYDu5YZR8BsU9scxEAM9V-lx4wAdv\n",
      "Folder: Presentation Figures (Kyle)  |  ID: 1t3qWNZyWmgXl4fgvaJIeKQU76DawkAHg\n",
      "Folder: Alaska_VIs_Env  |  ID: 1sAJCK-sp5cf12iNVinjup0Mg0pznoFo4\n",
      "Folder: GMIE_GTP_Exports  |  ID: 1NGHXKFhhzGegRMJyL5V9bs7bWIlaCZjX\n",
      "Folder: Europe_VIs_Env  |  ID: 161LpFY8dIGXWrKO6dVNDVQaHDbugYvJA\n",
      "Folder: LANID_GTPS  |  ID: 1gB8pFxtUKkPguTabZ-Tu4s0WPXn75uLU\n",
      "Folder: LANID_GTPS  |  ID: 11h1oxJyob8Bk1idYODFDSY63Z5D-I65t\n",
      "Folder: NorthAmerica_GTPs  |  ID: 1yF93-d9qWa_gsJbHKiW3PYXLGBjQ2R1d\n",
      "Folder: SouthAmerica_GTPs  |  ID: 1W0HNTYGgvwS15Macx3o3Fck8-Zolcp8H\n",
      "Folder: SouthAmerica_GTPs  |  ID: 1rWEjBsC6IliCKtyg3ZTAi8sF_Mga1-O3\n",
      "Folder: Asia_GTP_Exports  |  ID: 1Mb4lCc_SPY6k1E6RUQkQK00aZI-W7ZcU\n",
      "Folder: Asia_GTP_Exports  |  ID: 1lGpJAaJKf4O83Go3wybVc7qxP9OUFjNX\n",
      "Folder: East_Africa_VIs  |  ID: 1nHooOVyzdpwKK4S7VkYqGYJ7HemV8fR5\n",
      "Folder: Continental_LST  |  ID: 1PXneA1vLweiWq3P27JgWtAu_Bicsn63n\n",
      "Folder: Model Training  |  ID: 1riXbA8PYImJ_IAPLMt-1Mp5OxwvhYq5t\n",
      "Folder: Australia_GTP_Exports  |  ID: 1-pZ78HTRKA0xXHNG543w93gy9RmoU1rK\n",
      "Folder: Oceania_GTP_Exports  |  ID: 1r8e3cf34uLFnttX1wLgLZLGAxGZNA08k\n",
      "Folder: River Network  |  ID: 1ZqVgSQzDUvCG8KHrLRjw_Y9eOSVFoKb6\n",
      "Folder: GEE  |  ID: 1pSu1knrs7PCP0y7GRYjpeY7Q9IBIw90s\n",
      "Folder: Asia_VIs_Env_fix  |  ID: 1RidDG3V91GYJzddA01UXoiyQBO7V3tBw\n",
      "Folder: AEZ  |  ID: 1te3nKn8vyt2AECmk8NM_xBgpEFXp84RX\n",
      "Folder: US_VIs_Env_Final  |  ID: 1hqMIyDYEFKnpS8KLxC4bqHmF_9dHXImG\n",
      "Folder: Russia  |  ID: 1t3wGHApm7zc1IzDLDNUpHXjbBo7zAyx7\n",
      "Folder: NorthAmerica_VIs_Env  |  ID: 15EdJ3CkBPm-tsKBVVF3kdgajV2Skp0I3\n",
      "Folder: MIRCA2020  |  ID: 1eP7mxDR9D6hRyN133Y5Db-KOWrbAQF0k\n",
      "Folder: Asia_VIs_Env  |  ID: 18pQKnMMnLramhHRZSNwUJrLqG5DXNMmS\n",
      "Folder: Africa_VIs_Env  |  ID: 1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\n",
      "Folder: Oceania_VIs_Env  |  ID: 1gtVp-x-dIgcuNPcoaxHfmZs0ctoG71PS\n",
      "Folder: Other files  |  ID: 1UF1SbMqszNi0vQSJaD6ExB5_dKq-A-bN\n",
      "Folder: Asia_GPTS_2023  |  ID: 18QKPr8XZIDNGqIdXncbatM8qL90BLBCa\n",
      "Folder: GEE_exports  |  ID: 1iKEC-xrOomn5eFw4CV5HzUNlq7y5-EDC\n",
      "Folder: Global High Res. Map  |  ID: 1wmu_kb5tcxIt6gIPQ6CLDixM-uSbZ5Hc\n",
      "Folder: MIRCA2015_Latest_Version  |  ID: 1fqtG_HEAvK_wKRTF4NuuGgXl8FNBnDvY\n",
      "Folder: MIRCA-OS_New_version  |  ID: 1mxYpSecaSVYuBgkw7FwbUh0nc9-RHqKD\n",
      "Folder: GPTs  |  ID: 1zSZ57xgJa3LfVMRDgz0oGXOxkFoQqrnA\n",
      "Folder: USA  |  ID: 1nmIDoF9d7eN3QvTLkwwTt-wJ7llTREEt\n",
      "Folder: New AEI CA  |  ID: 1PNFVJ803HVSlPbw2S1D7E5iLj_4H8_Du\n",
      "Folder: Progress Reports  |  ID: 1Fk0549LWrBgEKlFf9TzGPMwEaS5mcPfQ\n",
      "Folder: 2015 Water Scarcity Assessment   |  ID: 1kDqp_iSiDCW842h8Ic_dwG9-nV98bGfQ\n",
      "Folder: USA  |  ID: 1YGpsMpV7d0-PE_pElPBzYUMUA_cAgyYm\n",
      "Folder: Colab Notebooks  |  ID: 1SqDk51R_bKFSdcAqT-4zMo6bZTlpat4C\n",
      "Folder: GGE  |  ID: 1B-dRy_XfHBtZgjlP90v77bjjidWrLxNJ\n"
     ]
    }
   ],
   "source": [
    "# List only folders in Google Drive root\n",
    "folder_list = drive.ListFile({\n",
    "    'q': \"'root' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
    "}).GetList()\n",
    "\n",
    "for folder in folder_list:\n",
    "    print(f\"Folder: {folder['title']}  |  ID: {folder['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ba20c-7c6c-4665-821c-0c30fc57fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vector points (shp): Africa.shp\n",
      "   • AEZ filled from cache for 265/15126 points\n",
      "=== Train regional model: Africa ===\n",
      "   • AEZ sampled from tiles/global for 14861 points\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Regional RF irrigation model with:\n",
    "- Farm-aware split via DBSCAN (no leakage)\n",
    "- AEZ categorical (one-hot)\n",
    "- OUT-OF-FOLD probability calibration (isotonic by default, Platt optional)\n",
    "- Regional OOF metrics + artifacts\n",
    "- Per-country OOF metrics & thresholds saved\n",
    "- Country predictions on ~30m WGS84 using calibrated probabilities\n",
    "\n",
    "Now updated to:\n",
    "  • Read AEZ per-point from Model Training/US_GTPS_per_point.(parquet|csv) when available\n",
    "  • Prefer per-state AEZ tiles: By Country/<State>/<State>_AEZ.tif (fallback: global AEZ)\n",
    "\n",
    "Outputs:\n",
    "  CountryModelPredicted/<Country>_RF_probability_percent.tif\n",
    "  CountryModelPredicted/<Country>_RF_binary_0_1_cropland.tif\n",
    "  CountryModelPredicted/<Country>_RF_predictors_count.tif\n",
    "\n",
    "  Model Training/Regional Models/<Region>/\n",
    "    - metrics.json\n",
    "    - confusion.png\n",
    "    - feature_importance.csv\n",
    "    - per_country_test_metrics.csv\n",
    "    - model.joblib\n",
    "\n",
    "  Model Training/Country Models/<Country>/\n",
    "    - threshold.json\n",
    "    - test_metrics.json\n",
    "    - test_confusion.png\n",
    "\"\"\"\n",
    "import os, re, json, math, tempfile, joblib, time, random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, average_precision_score, roc_auc_score,\n",
    "    precision_recall_fscore_support, confusion_matrix, roc_curve,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import pyproj\n",
    "from sklearn.base import clone\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling as RioResampling\n",
    "from rasterio.warp import reproject, transform as rio_transform, transform_bounds as rio_transform_bounds\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.windows import Window\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from googleapiclient.errors import HttpError\n",
    "except Exception:\n",
    "    HttpError = Exception\n",
    "try:\n",
    "    from pydrive2.files import ApiRequestError\n",
    "except Exception:\n",
    "    ApiRequestError = Exception\n",
    "\n",
    "os.environ.setdefault(\"SHAPE_RESTORE_SHX\", \"YES\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "# ------------------------ CONFIG ------------------------\n",
    "ROOT_FOLDER_ID = os.environ.get(\"ROOT_FOLDER_ID\", \"1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\")\n",
    "BY_COUNTRY_NAME = \"By Country\"\n",
    "CLE_FOLDER_ID = \"1pOpf-Zy5la4SKTIcrmLX6tGlxVBejKYX\"\n",
    "MODEL_FOLDER = \"Model Training\"\n",
    "REGIONAL_MODELS_FOLDER = \"Regional Models\"  # used per country now: region_name == country\n",
    "COUNTRY_MODELS_FOLDER = \"Country Models\"\n",
    "OUTPUT_FOLDER = \"CountryModelPredicted_Cropland\"\n",
    "\n",
    "# River Network (continental fallback for distance-to-river)\n",
    "RIVER_NET_FOLDER_ID = \"1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\"\n",
    "RIVER_DIST_PREFIX = \"Dist_\"\n",
    "RIVER_DIST_SUFFIX = \"_river.tif\"\n",
    "\n",
    "# Country column auto-detection; we copy into this name\n",
    "COUNTRY_COL = os.environ.get(\"COUNTRY_COL\", \"country_hint\")\n",
    "\n",
    "# DBSCAN farm grouping (meters)\n",
    "DBSCAN_EPS_M = float(os.environ.get(\"DBSCAN_EPS_M\", 45))\n",
    "DBSCAN_MIN_SAMPLES = int(os.environ.get(\"DBSCAN_MIN_SAMPLES\", 1))\n",
    "\n",
    "# AEZ (global raster)\n",
    "AEZ_FILE_ID   = os.environ.get(\"AEZ_FILE_ID\", \"1te3nKn8vyt2AECmk8NM_xBgpEFXp84RX\")\n",
    "AEZ_FILE_NAME = os.environ.get(\"AEZ_FILE_NAME\", \"AEZ_2020s.tif\")\n",
    "AEZ_COL       = \"AEZ\"\n",
    "\n",
    "# Local fallbacks\n",
    "LOCAL_BASE_DIR = os.path.join(\"./\", \"Africa_VIs_Env\")\n",
    "LOCAL_BY_COUNTRY_DIR = os.path.join(LOCAL_BASE_DIR, \"By Country\")\n",
    "LOCAL_CLE_DIR = os.path.join(LOCAL_BASE_DIR, \"CLE\")\n",
    "LOCAL_MODEL_DIR = os.path.join(LOCAL_BASE_DIR, MODEL_FOLDER)\n",
    "LOCAL_OUTPUT_DIR = os.path.join(LOCAL_BASE_DIR, OUTPUT_FOLDER)\n",
    "\n",
    "# GTPS/AEZ cache made earlier\n",
    "GTPS_CACHE_PARQUET = \"Nigeria_GTPS_per_point.parquet\"\n",
    "GTPS_CACHE_CSV     = \"Nigeria_GTPS_per_point.csv\"\n",
    "\n",
    "# Drive retry knobs\n",
    "DRIVE_MAX_RETRIES = int(os.environ.get(\"DRIVE_MAX_RETRIES\", 6))\n",
    "DRIVE_RETRY_BASE = float(os.environ.get(\"DRIVE_RETRY_BASE\", 0.8))\n",
    "\n",
    "POINTS_BASENAME = \"Africa\"  # pooled training points\n",
    "\n",
    "ALL_PREDICTORS = [\n",
    "    \"NDVI_mean\",\"NDWI_mean\",\"GI_mean\",\n",
    "    \"NDVI_max\",\"NDWI_max\",\"GI_max\",\n",
    "    \"NDVI_min\",\"NDWI_min\",\"GI_min\",\n",
    "    \"elevation\",\"slope\",\n",
    "    # \"ET\",\"PET\",\n",
    "    \"dist_to_river\",\n",
    "    AEZ_COL,\n",
    "]\n",
    "TARGET = \"irrigated\"\n",
    "LONCOL = \"longitude\"\n",
    "LATCOL = \"latitude\"\n",
    "\n",
    "\n",
    "# RF + data\n",
    "RF_TREES = int(os.environ.get(\"RF_TREES\", 400))\n",
    "RANDOM_SEED = 42\n",
    "TEST_FRACTION = 0.30\n",
    "\n",
    "BORROW_MAX = 2000\n",
    "\n",
    "# ---- Calibration & thresholds ----\n",
    "CALIBRATION_METHOD = os.environ.get(\"CALIBRATION_METHOD\", \"isotonic\").lower()  # \"isotonic\" | \"platt\"\n",
    "THRESHOLD_STRATEGY = os.environ.get(\"THRESHOLD_STRATEGY\", \"auto\").lower()      # \"auto\"|\"j\"|\"f1\"|\"fixed\"|\"precision_at\"|\"recall_at\"\n",
    "FIXED_THRESHOLD = float(os.environ.get(\"FIXED_THRESHOLD\", 0.50))\n",
    "PRECISION_TARGET = os.environ.get(\"PRECISION_TARGET\")\n",
    "RECALL_TARGET = os.environ.get(\"RECALL_TARGET\")\n",
    "PRECISION_TARGET = None if PRECISION_TARGET in (None, \"\", \"None\") else float(PRECISION_TARGET)\n",
    "RECALL_TARGET    = None if RECALL_TARGET    in (None, \"\", \"None\") else float(RECALL_TARGET)\n",
    "\n",
    "# Per-country threshold mode: \"global\" | \"country\" | \"hybrid\"\n",
    "THRESHOLD_MODE = os.environ.get(\"THRESHOLD_MODE\", \"hybrid\").lower()\n",
    "MIN_POS_NEG_FOR_COUNTRY = int(os.environ.get(\"MIN_POS_NEG_FOR_COUNTRY\", 30))\n",
    "MIN_TOTAL_FOR_COUNTRY   = int(os.environ.get(\"MIN_TOTAL_FOR_COUNTRY\", 100))\n",
    "\n",
    "# Prediction\n",
    "TILE_SIZE = 1024\n",
    "RESAMPLING = RioResampling.bilinear\n",
    "PER_BAND_RESAMPLING = {AEZ_COL: RioResampling.nearest}\n",
    "STRICT_ALL_BANDS = False\n",
    "MIN_PREDICTORS = 9\n",
    "\n",
    "# ------------------ UTILS / HELPERS ---------------------\n",
    "def _meters_per_degree(lat_deg: float):\n",
    "    m_per_deg_lat = 111_132.0\n",
    "    m_per_deg_lon = 111_320.0 * math.cos(math.radians(lat_deg))\n",
    "    return m_per_deg_lat, m_per_deg_lon\n",
    "\n",
    "def _target_wgs84_grid_from_bounds(west, south, east, north, pixel_m=30.0):\n",
    "    mid_lat = 0.5 * (south + north)\n",
    "    m_lat, m_lon = _meters_per_degree(mid_lat)\n",
    "    dy = pixel_m / m_lat; dx = pixel_m / m_lon\n",
    "    width = int(math.ceil((east - west) / dx)); height = int(math.ceil((north - south) / dy))\n",
    "    transform = Affine(dx, 0, west, 0, -dy, north)\n",
    "    return height, width, transform, CRS.from_epsg(4326)\n",
    "\n",
    "def _iter_tiles(H, W, tile):\n",
    "    for r0 in range(0, H, tile):\n",
    "        for c0 in range(0, W, tile):\n",
    "            r1 = min(r0 + tile, H); c1 = min(c0 + tile, W)\n",
    "            yield Window.from_slices((r0, r1), (c0, c1))\n",
    "\n",
    "def _tile_count(H, W, tile):\n",
    "    return ((H + tile - 1) // tile) * ((W + tile - 1) // tile)\n",
    "\n",
    "def _compute_basic_metrics(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    try: roc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception: roc = float(\"nan\")\n",
    "    try: prauc = average_precision_score(y_true, y_prob)\n",
    "    except Exception: prauc = float(\"nan\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(pr),\n",
    "        \"recall\": float(rc),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"pr_auc\": float(prauc),\n",
    "        \"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]),\n",
    "        \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1]),\n",
    "    }, cm\n",
    "\n",
    "def _save_confusion_png(cm, title, path):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xlabel(\"Pred\"); ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title)\n",
    "    for (r,c),v in np.ndenumerate(cm):\n",
    "        ax.text(c, r, f\"{v}\", ha=\"center\", va=\"center\")\n",
    "    fig.savefig(path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def assign_farm_ids(df: pd.DataFrame, lon_col=LONCOL, lat_col=LATCOL) -> pd.Series:\n",
    "    if df.empty:\n",
    "        return pd.Series([], dtype=\"int64\")\n",
    "    proj = pyproj.Transformer.from_crs(4326, 3857, always_xy=True)\n",
    "    x, y = proj.transform(df[lon_col].values, df[lat_col].values)\n",
    "    coords = np.column_stack([x, y])\n",
    "    db = DBSCAN(eps=DBSCAN_EPS_M, min_samples=DBSCAN_MIN_SAMPLES, metric=\"euclidean\", algorithm=\"ball_tree\")\n",
    "    labels = db.fit_predict(coords)\n",
    "    if (labels == -1).any():\n",
    "        max_lab = labels[labels >= 0].max() if np.any(labels >= 0) else -1\n",
    "        noise_idx = np.where(labels == -1)[0]\n",
    "        labels[noise_idx] = np.arange(max_lab + 1, max_lab + 1 + len(noise_idx))\n",
    "    return pd.Series(labels.astype(\"int64\"), index=df.index, name=\"farm_id\")\n",
    "\n",
    "def _pick_country_candidate(cols):\n",
    "    cand = [\"country_hint\",\"country\",\"Country\",\"COUNTRY\",\"admin\",\"ADMIN\",\"state\",\"STATE\",\"STATE_NAME\",\"State\",\"province\",\"Province\"]\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for c in cand:\n",
    "        if c.lower() in low:\n",
    "            return low[c.lower()]\n",
    "    return None\n",
    "\n",
    "# ---------------------- DRIVE HELPERS -------------------\n",
    "def _drive_retry(callable_fn, *args, **kwargs):\n",
    "    last_err = None\n",
    "    for i in range(DRIVE_MAX_RETRIES):\n",
    "        try:\n",
    "            return callable_fn(*args, **kwargs)\n",
    "        except (HttpError, ApiRequestError) as e:\n",
    "            last_err = e\n",
    "            code = getattr(getattr(e, \"resp\", None), \"status\", None)\n",
    "            if code is None:\n",
    "                msg = str(e).lower()\n",
    "                transient = any(k in msg for k in [\"internal error\",\"backenderror\",\"rate limit\",\"timeout\"])\n",
    "            else:\n",
    "                transient = 500 <= int(code) < 600 or int(code) in (403, 429)\n",
    "            if not transient or i == DRIVE_MAX_RETRIES - 1:\n",
    "                break\n",
    "            sleep_s = DRIVE_RETRY_BASE * (2 ** i) + random.random() * 0.2\n",
    "            print(f\"[Drive Retry] attempt {i+1}/{DRIVE_MAX_RETRIES} after error {e}; sleeping {sleep_s:.2f}s\")\n",
    "            time.sleep(sleep_s)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            break\n",
    "    raise last_err\n",
    "\n",
    "def _drive_query(drive, q):\n",
    "    def _run():\n",
    "        return drive.ListFile({\"q\": q, \"supportsAllDrives\": True, \"includeItemsFromAllDrives\": True, \"maxResults\": 1000}).GetList()\n",
    "    return _drive_retry(_run)\n",
    "\n",
    "def get_subfolder(drive, parent_id, name):\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BASE_DIR, name)\n",
    "        return p if os.path.isdir(p) else None\n",
    "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    r = _drive_query(drive, q)\n",
    "    return r[0][\"id\"] if r else None\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    if drive is None:\n",
    "        base = LOCAL_BASE_DIR if not os.path.isabs(parent_id) else parent_id\n",
    "        p = os.path.join(base, name) if os.path.isdir(base) else os.path.join(LOCAL_BASE_DIR, name)\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "        return p\n",
    "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    res = _drive_query(drive, q)\n",
    "    if res:\n",
    "        return res[0][\"id\"]\n",
    "    def _create():\n",
    "        f = drive.CreateFile({\"title\": name, \"parents\": [{\"id\": parent_id}], \"mimeType\": \"application/vnd.google-apps.folder\"})\n",
    "        f.Upload(); return f[\"id\"]\n",
    "    return _drive_retry(_create)\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    if drive is None:\n",
    "        return sorted([os.path.join(parent_id, p) for p in os.listdir(parent_id)]) if os.path.isdir(parent_id) else []\n",
    "    q = f\"'{parent_id}' in parents and trashed=false\"\n",
    "    try: return _drive_query(drive, q)\n",
    "    except Exception as e:\n",
    "        print(f\"[Drive Warning] list_files failed for parent {parent_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_to_temp(drive_file, dst_path):\n",
    "    def _dl():\n",
    "        drive_file.GetContentFile(dst_path); return dst_path\n",
    "    return _drive_retry(_dl)\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    if drive is None:\n",
    "        os.makedirs(parent_id, exist_ok=True)\n",
    "        import shutil\n",
    "        dst = os.path.join(parent_id, title or os.path.basename(local_path))\n",
    "        shutil.copy2(local_path, dst); return dst\n",
    "    def _up():\n",
    "        f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\": [{\"id\": parent_id}]})\n",
    "        f.SetContentFile(local_path); f.Upload(); return f[\"id\"]\n",
    "    return _drive_retry(_up)\n",
    "\n",
    "def _drive_walk(drive, start_id, max_depth=4):\n",
    "    q = deque([(start_id, 0)])\n",
    "    while q:\n",
    "        fid, d = q.popleft()\n",
    "        items = list_files(drive, fid)\n",
    "        yield fid, items\n",
    "        if d >= max_depth: continue\n",
    "        for it in items:\n",
    "            if isinstance(it, dict) and it.get(\"mimeType\") == \"application/vnd.google-apps.folder\":\n",
    "                q.append((it[\"id\"], d + 1))\n",
    "\n",
    "# ---------------------- AEZ SOURCES ----------------------\n",
    "def open_aez_path(drive):\n",
    "    \"\"\"Global AEZ.tif (fallback if per-country tile missing).\"\"\"\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
    "        return p if os.path.exists(p) else None\n",
    "\n",
    "    def _resolve_shortcut(file_obj):\n",
    "        if file_obj.get(\"mimeType\") == \"application/vnd.google-apps.shortcut\":\n",
    "            tgt = file_obj.get(\"shortcutDetails\", {}).get(\"targetId\")\n",
    "            if tgt:\n",
    "                g = drive.CreateFile({\"id\": tgt})\n",
    "                g.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); return g\n",
    "        return file_obj\n",
    "\n",
    "    folder_id = None\n",
    "    if AEZ_FILE_ID:\n",
    "        f = drive.CreateFile({\"id\": AEZ_FILE_ID})\n",
    "        f.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); f = _resolve_shortcut(f)\n",
    "        mime = f.get(\"mimeType\")\n",
    "        if mime == \"application/vnd.google-apps.folder\":\n",
    "            folder_id = f[\"id\"]\n",
    "        elif mime and mime.startswith(\"application/vnd.google-apps.\"):\n",
    "            raise RuntimeError(f\"AEZ_FILE_ID points to a Google Doc ({mime}), not a TIFF.\")\n",
    "        else:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            f.GetContentFile(tmp); return tmp\n",
    "\n",
    "    folder_id = folder_id or os.environ.get(\"AEZ_FOLDER_ID\", None)\n",
    "    if folder_id:\n",
    "        q = f\"'{folder_id}' in parents and trashed=false and title='{AEZ_FILE_NAME}'\"\n",
    "        cand = _drive_query(drive, q)\n",
    "        if not cand:\n",
    "            q_any = f\"'{folder_id}' in parents and trashed=false and title contains '.tif'\"\n",
    "            cand = _drive_query(drive, q_any)\n",
    "            cand = [c for c in cand if c.get(\"title\",\"\") == AEZ_FILE_NAME] or cand\n",
    "        if not cand:\n",
    "            raise RuntimeError(f\"Could not find {AEZ_FILE_NAME} in folder id {folder_id}.\")\n",
    "        f = drive.CreateFile({\"id\": cand[0][\"id\"]})\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "        f.GetContentFile(tmp); return tmp\n",
    "\n",
    "    p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "# ---------------------- POINTS / CACHE -------------------\n",
    "def _find_points_vector_drive(drive):\n",
    "    try: mt = get_subfolder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "    except Exception: mt = None\n",
    "    if mt:\n",
    "        items = list_files(drive, mt); parts=[]; has_shp=False\n",
    "        for it in items:\n",
    "            if not isinstance(it, dict): continue\n",
    "            title = it.get(\"title\",\"\"); low = title.lower()\n",
    "            if not low.startswith(\"US.\"): continue\n",
    "            if low.endswith(\".shp\"): has_shp=True\n",
    "            if re.search(r\"\\.(shp|dbf|shx|prj|cpg|qpj)$\", low): parts.append(it)\n",
    "        if has_shp and parts:\n",
    "            tdir = tempfile.mkdtemp()\n",
    "            for p in parts: download_to_temp(p, os.path.join(tdir, p.get(\"title\",\"\")))\n",
    "            shp_path = os.path.join(tdir, \"US.shp\")\n",
    "            if os.path.exists(shp_path):\n",
    "                print(\"Using vector points (shp): Model Training/US.shp\"); return shp_path\n",
    "    roots = [mt or ROOT_FOLDER_ID, ROOT_FOLDER_ID]\n",
    "    for root in roots:\n",
    "        for fid, items in _drive_walk(drive, root, max_depth=4):\n",
    "            stems={}\n",
    "            for it in items:\n",
    "                if not isinstance(it, dict): continue\n",
    "                title = it.get(\"title\",\"\"); m = re.match(rf\"(.+)\\.(shp|dbf|shx|prj|cpg|qpj)$\", title, re.IGNORECASE)\n",
    "                if not m: continue\n",
    "                stem = m.group(1)\n",
    "                if stem.lower() == POINTS_BASENAME.lower():\n",
    "                    stems.setdefault(stem,[]).append(it)\n",
    "            for stem, parts in stems.items():\n",
    "                if any(p.get(\"title\",\"\").lower().endswith(\".shp\") for p in parts):\n",
    "                    tdir = tempfile.mkdtemp()\n",
    "                    for p in parts: download_to_temp(p, os.path.join(tdir, p.get(\"title\",\"\")))\n",
    "                    shp_path = os.path.join(tdir, f\"{os.path.basename(stem)}.shp\")\n",
    "                    if os.path.exists(shp_path):\n",
    "                        print(f\"Using vector points (shp): {os.path.basename(stem)}.shp\"); return shp_path\n",
    "            for it in items:\n",
    "                if not isinstance(it, dict): continue\n",
    "                title = it.get(\"title\",\"\"); low = title.lower()\n",
    "                if it.get(\"mimeType\") == \"application/vnd.google-apps.folder\": continue\n",
    "                if low.endswith(\".gpkg\") and low == f\"{POINTS_BASENAME.lower()}.gpkg\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".gpkg\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (gpkg): {title}\"); return tmp\n",
    "                if low.endswith(\".zip\") and low == f\"{POINTS_BASENAME.lower()}.zip\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (zip): {title}\"); return tmp\n",
    "                if low.endswith(\".csv\") and low == f\"{POINTS_BASENAME.lower()}.csv\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (csv): {title}\"); return tmp\n",
    "    return None\n",
    "\n",
    "def _find_points_vector_local():\n",
    "    search_roots = [os.path.join(LOCAL_BASE_DIR, MODEL_FOLDER), LOCAL_BASE_DIR]\n",
    "    for root in search_roots:\n",
    "        if not os.path.isdir(root): continue\n",
    "        for nm in os.listdir(root):\n",
    "            low = nm.lower()\n",
    "            if low == f\"{POINTS_BASENAME.lower()}.csv\":\n",
    "                print(f\"Using vector points (csv): {nm}\"); return os.path.join(root, nm)\n",
    "            if POINTS_BASENAME.lower() in low and low.endswith(\".gpkg\"):\n",
    "                print(f\"Using vector points (gpkg): {nm}\"); return os.path.join(root, nm)\n",
    "            if POINTS_BASENAME.lower() in low and low.endswith(\".zip\"):\n",
    "                print(f\"Using vector points (zip): {nm}\"); return os.path.join(root, nm)\n",
    "    for root in search_roots:\n",
    "        if not os.path.isdir(root): continue\n",
    "        for nm in os.listdir(root):\n",
    "            if nm.lower().endswith(\".shp\") and POINTS_BASENAME.lower() in nm.lower():\n",
    "                print(f\"Using vector points (shp): {nm}\"); return os.path.join(root, nm)\n",
    "    return None\n",
    "\n",
    "def _ci_lookup(cols, name):\n",
    "    name_l = name.lower()\n",
    "    for c in cols:\n",
    "        if c.lower() == name_l: return c\n",
    "    return None\n",
    "\n",
    "def _round6(x):\n",
    "    return np.round(pd.to_numeric(x, errors=\"coerce\"), 6)\n",
    "\n",
    "def _load_gtps_cache_df(drive):\n",
    "    \"\"\"Load US_GTPS_per_point.(parquet|csv) from Model Training if present.\"\"\"\n",
    "    def _from_drive():\n",
    "        mt = get_or_create_folder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "        # prefer parquet\n",
    "        res = _drive_query(drive, f\"'{mt}' in parents and trashed=false and title='{GTPS_CACHE_PARQUET}'\")\n",
    "        if res:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".parquet\").name\n",
    "            download_to_temp(res[0], tmp)\n",
    "            try:\n",
    "                return pd.read_parquet(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "        res = _drive_query(drive, f\"'{mt}' in parents and trashed=false and title='{GTPS_CACHE_CSV}'\")\n",
    "        if res:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\").name\n",
    "            download_to_temp(res[0], tmp)\n",
    "            return pd.read_csv(tmp)\n",
    "        return None\n",
    "\n",
    "    def _from_local():\n",
    "        p1 = os.path.join(LOCAL_MODEL_DIR, GTPS_CACHE_PARQUET)\n",
    "        p2 = os.path.join(LOCAL_MODEL_DIR, GTPS_CACHE_CSV)\n",
    "        if os.path.exists(p1):\n",
    "            try:\n",
    "                return pd.read_parquet(p1)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if os.path.exists(p2):\n",
    "            return pd.read_csv(p2)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return _from_drive() if drive is not None else _from_local()\n",
    "    except Exception as e:\n",
    "        print(f\"   ! AEZ cache read error: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_points_df(drive):\n",
    "    import geopandas as gpd, fiona\n",
    "    path = _find_points_vector_drive(drive) if drive is not None else _find_points_vector_local()\n",
    "    if path is None:\n",
    "        raise RuntimeError(\"Could not locate US.(shp/zip/gpkg/csv) in 'Model Training' tree.\")\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        with fiona.Env(SHAPE_RESTORE_SHX=\"YES\"):\n",
    "            gdf = gpd.read_file(path)\n",
    "        if gdf.crs is not None and gdf.crs.to_epsg() != 4326:\n",
    "            gdf = gdf.to_crs(4326)\n",
    "        df = pd.DataFrame(gdf.drop(columns=\"geometry\", errors=\"ignore\"))\n",
    "\n",
    "    lon_col = _ci_lookup(df.columns, LONCOL); lat_col = _ci_lookup(df.columns, LATCOL)\n",
    "    if lon_col is None or lat_col is None:\n",
    "        raise RuntimeError(\"Points must contain longitude and latitude columns.\")\n",
    "    if lon_col != LONCOL: df[LONCOL] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "    if lat_col != LATCOL: df[LATCOL] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "\n",
    "    tgt_col = _ci_lookup(df.columns, TARGET)\n",
    "    if tgt_col is None: raise RuntimeError(f\"Points file missing column: {TARGET}\")\n",
    "    if tgt_col != TARGET: df[TARGET] = pd.to_numeric(df[tgt_col], errors=\"coerce\")\n",
    "\n",
    "    # pre-create rounders for cache join\n",
    "    df[\"_lonr\"] = _round6(df[LONCOL]); df[\"_latr\"] = _round6(df[LATCOL])\n",
    "\n",
    "    # copy a country-like column\n",
    "    if COUNTRY_COL not in df.columns:\n",
    "        cand = _pick_country_candidate(df.columns)\n",
    "        df[COUNTRY_COL] = df[cand] if cand else \"Unknown\"\n",
    "\n",
    "    for c in [TARGET, LONCOL, LATCOL]:\n",
    "        if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[TARGET, LONCOL, LATCOL]).reset_index(drop=True)\n",
    "    df[TARGET] = df[TARGET].astype(int)\n",
    "    df[COUNTRY_COL] = df[COUNTRY_COL].astype(str)\n",
    "\n",
    "    # Bring AEZ from cache if available\n",
    "    if AEZ_COL not in df.columns or df[AEZ_COL].isna().all():\n",
    "        cache = _load_gtps_cache_df(drive)\n",
    "        if cache is not None:\n",
    "            # normalize cache cols\n",
    "            cc = {c.lower(): c for c in cache.columns}\n",
    "            lonc = cc.get(LONCOL.lower(), None) or cc.get(\"x\") or cc.get(\"lon\") or list(cache.columns)[1]\n",
    "            latc = cc.get(LATCOL.lower(), None) or cc.get(\"y\") or cc.get(\"lat\") or list(cache.columns)[2]\n",
    "            aezc = cc.get(AEZ_COL.lower(), None) or cc.get(\"gtps\") or cc.get(\"aez\")\n",
    "            if aezc is not None:\n",
    "                cache[\"_lonr\"] = _round6(cache[lonc]); cache[\"_latr\"] = _round6(cache[latc])\n",
    "                df = df.merge(cache[[\"_lonr\",\"_latr\",aezc]].rename(columns={aezc:AEZ_COL}),\n",
    "                              on=[\"_lonr\",\"_latr\"], how=\"left\")\n",
    "                if AEZ_COL in df.columns:\n",
    "                    df[AEZ_COL] = pd.to_numeric(df[AEZ_COL], errors=\"coerce\")\n",
    "                    df[AEZ_COL] = np.rint(df[AEZ_COL]).astype(\"float64\")\n",
    "                    print(f\"   • AEZ filled from cache for {np.isfinite(df[AEZ_COL]).sum()}/{len(df)} points\")\n",
    "    return df\n",
    "\n",
    "# ---------------------- RASTERS -------------------------\n",
    "def open_country_var_path(drive, country, variable):\n",
    "    \"\"\"Any By Country/<Country>/<Country>_<variable>.tif (case-sensitive on names passed in).\"\"\"\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BY_COUNTRY_DIR, country, f\"{country}_{variable}.tif\")\n",
    "        return p if os.path.exists(p) else None\n",
    "    byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
    "    if not byc: return None\n",
    "    q = f\"'{byc}' in parents and trashed=false and title='{country}' and mimeType='application/vnd.google-apps.folder'\"\n",
    "    res = _drive_query(drive, q)\n",
    "    if not res: return None\n",
    "    cid = res[0][\"id\"]\n",
    "    q2 = f\"'{cid}' in parents and trashed=false and title='{country}_{variable}.tif'\"\n",
    "    files = _drive_query(drive, q2)\n",
    "    if files:\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "        return download_to_temp(files[0], tmp)\n",
    "    for it in list_files(drive, cid):\n",
    "        if isinstance(it, dict) and re.match(rf\"^{re.escape(country)}_{re.escape(variable)}.*\\.tif$\", it.get(\"title\",\"\"), re.IGNORECASE):\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            return download_to_temp(it, tmp)\n",
    "    return None\n",
    "\n",
    "def open_CroplandNE(drive):\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_CLE_DIR, \"Cropland_Nigeria.tif\")\n",
    "        return p if os.path.exists(p) else None\n",
    "    items = list_files(drive, CLE_FOLDER_ID)\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict): continue\n",
    "        if it.get(\"title\",\"\").lower().startswith(\"Cropland_Nigeria\") and it.get(\"title\",\"\").lower().endswith(\".tif\"):\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            return download_to_temp(it, tmp)\n",
    "    return None\n",
    "\n",
    "def _clip_bounds_from_reference(raster_path):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        b = rio_transform_bounds(src.crs, CRS.from_epsg(4326),\n",
    "                                 src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top, densify_pts=8)\n",
    "    return b\n",
    "\n",
    "def _pick_reference_raster_for_bounds(drive, country):\n",
    "    # Prefer any available per-country raster (including AEZ tile) to set the grid.\n",
    "    # IMPORTANT: do NOT fall back to global AEZ here, to avoid predicting outside the country.\n",
    "    for v in [\"CLE\"] + ALL_PREDICTORS + [AEZ_COL]:\n",
    "        if v == AEZ_COL:\n",
    "            p = open_country_var_path(drive, country, AEZ_COL)\n",
    "        else:\n",
    "            p = open_country_var_path(drive, country, v)\n",
    "        if p is not None:\n",
    "            return p\n",
    "    raise RuntimeError(f\"No per-country reference raster found for bounds in '{country}'.\")\n",
    "\n",
    "def country_grid_and_mask(drive, country):\n",
    "    ref_path = _pick_reference_raster_for_bounds(drive, country)\n",
    "    west, south, east, north = _clip_bounds_from_reference(ref_path)\n",
    "    H, W, transform, crs = _target_wgs84_grid_from_bounds(west, south, east, north, pixel_m=30.0)\n",
    "\n",
    "    # --- MUST have cropland mask for this country; otherwise skip prediction ---\n",
    "    country_cropland = open_country_var_path(drive, country, \"CLE\")\n",
    "    if country_cropland is None:\n",
    "        # This error is caught in run_region_then_countries and will print a debug line\n",
    "        # then continue to the next country.\n",
    "        raise RuntimeError(f\"No cropland CLE raster found for '{country}'; skipping prediction for this country.\")\n",
    "\n",
    "    cropland = clip_cle_to_grid(drive, H, W, transform, crs, source_path=country_cropland)\n",
    "    return (H, W, transform, crs, cropland, (west, south, east, north))\n",
    "\n",
    "def clip_cle_to_grid(drive, out_h, out_w, out_transform, out_crs, source_path=None):\n",
    "    cle = source_path or open_CroplandNE(drive)\n",
    "    if cle is None: return np.ones((out_h, out_w), dtype=bool)\n",
    "    arr = np.full((out_h, out_w), -9999.0, dtype=np.float32)\n",
    "    with rasterio.open(cle) as src:\n",
    "        reproject(source=rasterio.band(src, 1), destination=arr,\n",
    "                  src_transform=src.transform, src_crs=src.crs,\n",
    "                  dst_transform=out_transform, dst_crs=out_crs,\n",
    "                  dst_nodata=-9999.0, resampling=RioResampling.nearest)\n",
    "        nd = src.nodata\n",
    "    if nd is not None: arr = np.where(np.isclose(arr, nd), np.nan, arr)\n",
    "    arr = np.where(np.isclose(arr, -9999.0), np.nan, arr)\n",
    "    return np.isfinite(arr) & (arr > 0.5)\n",
    "\n",
    "# ---------------- THRESHOLDS -----------------------------\n",
    "def _thr_by_J(y_true, p):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    if np.unique(y).size < 2: return FIXED_THRESHOLD\n",
    "    fpr, tpr, th = roc_curve(y, p)\n",
    "    if len(th) < 2: return FIXED_THRESHOLD\n",
    "    j = tpr - fpr\n",
    "    return float(th[int(np.argmax(j))])\n",
    "\n",
    "def _thr_by_F1(y_true, p):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    ths = np.linspace(0.01, 0.99, 99); best_t, best_f1 = 0.5, -1\n",
    "    for t in ths:\n",
    "        yhat = (p >= t).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y, yhat, average=\"binary\", zero_division=0)\n",
    "        if f1 > best_f1: best_f1, best_t = f1, t\n",
    "    return float(best_t)\n",
    "\n",
    "def _thr_precision_at(y_true, p, target):\n",
    "    if target is None: return _thr_by_J(y_true, p)\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    cand = thr[prec[1:] >= target]\n",
    "    if len(cand) > 0:\n",
    "        return float(np.min(cand))  # smallest threshold achieving target precision\n",
    "    return _thr_by_F1(y_true, p)\n",
    "\n",
    "def _thr_recall_at(y_true, p, target):\n",
    "    if target is None: return _thr_by_J(y_true, p)\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    cand = thr[rec[1:] >= target]\n",
    "    if len(cand) > 0:\n",
    "        return float(np.max(cand))  # largest threshold while keeping recall >= target\n",
    "    return _thr_by_F1(y_true, p)\n",
    "\n",
    "def _pick_threshold(y_true, p):\n",
    "    s = THRESHOLD_STRATEGY.lower()\n",
    "    if s == \"fixed\":       return float(FIXED_THRESHOLD)\n",
    "    if s == \"f1\":          return _thr_by_F1(y_true, p)\n",
    "    if s == \"j\":           return _thr_by_J(y_true, p)\n",
    "    if s == \"precision_at\":return _thr_precision_at(y_true, p, PRECISION_TARGET)\n",
    "    if s == \"recall_at\":   return _thr_recall_at(y_true, p, RECALL_TARGET)\n",
    "    if PRECISION_TARGET is not None:\n",
    "        return _thr_precision_at(y_true, p, PRECISION_TARGET)\n",
    "    if RECALL_TARGET is not None:\n",
    "        return _thr_recall_at(y_true, p, RECALL_TARGET)\n",
    "    return _thr_by_J(y_true, p)\n",
    "\n",
    "# ---------------- Calibration helpers -------------------\n",
    "def _fit_calibrator(raw_pos_probs, y, method=\"isotonic\"):\n",
    "    raw_pos_probs = np.asarray(raw_pos_probs, dtype=float).ravel()\n",
    "    y = np.asarray(y, dtype=int).ravel()\n",
    "    if method == \"platt\":\n",
    "        lr = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "        lr.fit(raw_pos_probs.reshape(-1,1), y)\n",
    "        return {\"method\": \"platt\", \"model\": lr}\n",
    "    ir = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds=\"clip\")\n",
    "    ir.fit(raw_pos_probs, y)\n",
    "    return {\"method\": \"isotonic\", \"model\": ir}\n",
    "\n",
    "def _apply_calibrator(calib, raw_pos_probs):\n",
    "    raw_pos_probs = np.asarray(raw_pos_probs, dtype=float).ravel()\n",
    "    if calib is None:\n",
    "        return raw_pos_probs\n",
    "    if calib.get(\"method\") == \"platt\":\n",
    "        return calib[\"model\"].predict_proba(raw_pos_probs.reshape(-1,1))[:,1]\n",
    "    return calib[\"model\"].predict(raw_pos_probs)\n",
    "\n",
    "# ------------- POINT SAMPLING FROM RASTERS --------------\n",
    "def _sample_raster_at_lonlat(src, lon_arr, lat_arr):\n",
    "    xs = np.asarray(lon_arr, dtype=float); ys = np.asarray(lat_arr, dtype=float)\n",
    "    if src.crs and (src.crs.to_epsg() != 4326):\n",
    "        tx, ty = rio_transform(CRS.from_epsg(4326), src.crs, xs.tolist(), ys.tolist())\n",
    "        xs = np.asarray(tx, dtype=float); ys = np.asarray(ty, dtype=float)\n",
    "    out = np.full(xs.shape[0], np.nan, dtype=np.float32)\n",
    "    xmin, ymin, xmax, ymax = src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top\n",
    "    inside = (xs >= xmin) & (xs <= xmax) & (ys >= ymin) & (ys <= ymax)\n",
    "    if not np.any(inside): return out\n",
    "    idx = np.where(inside)[0]\n",
    "    coords = list(zip(xs[idx], ys[idx]))\n",
    "    vals = np.array([v[0] for v in src.sample(coords)], dtype=np.float32)\n",
    "    if src.nodata is not None and np.isfinite(src.nodata):\n",
    "        vals = np.where(np.isclose(vals, np.float32(src.nodata)), np.float32(np.nan), vals)\n",
    "    out[idx] = vals; return out\n",
    "\n",
    "# ---- TRAINING FILL (num predictors + AEZ) ---------------\n",
    "def _fill_training_predictors_from_country_rasters(drive, df_all, countries):\n",
    "    df = df_all.copy()\n",
    "\n",
    "    # AEZ: if still missing after cache merge, try per-country tiles, else global\n",
    "    if AEZ_COL not in df.columns or df[AEZ_COL].isna().any():\n",
    "        miss = df.index if AEZ_COL not in df.columns else df.index[df[AEZ_COL].isna()]\n",
    "        if len(miss) > 0:\n",
    "            aez_filled = 0\n",
    "            # try per-country tiles first\n",
    "            for c in countries:\n",
    "                try:\n",
    "                    ref = open_country_var_path(drive, c, AEZ_COL) or open_aez_path(drive)\n",
    "                    if not ref: continue\n",
    "                    w, s, e, n = _clip_bounds_from_reference(ref)\n",
    "                    idx = df.loc[miss].index[\n",
    "                        df.loc[miss, LONCOL].between(min(w,e), max(w,e)) &\n",
    "                        df.loc[miss, LATCOL].between(min(s,n), max(s,n))\n",
    "                    ]\n",
    "                    if len(idx) == 0: continue\n",
    "                    with rasterio.open(ref) as aez_src:\n",
    "                        vals = _sample_raster_at_lonlat(aez_src, df.loc[idx, LONCOL].values, df.loc[idx, LATCOL].values)\n",
    "                    df.loc[idx, AEZ_COL] = np.rint(pd.to_numeric(vals, errors=\"coerce\")).astype(\"float64\")\n",
    "                    aez_filled += int(np.isfinite(df.loc[idx, AEZ_COL]).sum())\n",
    "                except Exception as ex:\n",
    "                    print(f\"   ! AEZ fill skip for {c}: {ex}\")\n",
    "            if aez_filled > 0:\n",
    "                print(f\"   • AEZ sampled from tiles/global for {aez_filled} points\")\n",
    "\n",
    "    # Per-country numeric predictors from rasters\n",
    "    for c in countries:\n",
    "        try:\n",
    "            ref = open_country_var_path(drive, c, \"CLE\") or open_country_var_path(drive, c, \"NDVI_mean\")\n",
    "            if not ref: continue\n",
    "            w, s, e, n = _clip_bounds_from_reference(ref)\n",
    "        except Exception as ex:\n",
    "            print(f\"   ! Skip raster fill for {c}: {ex}\"); continue\n",
    "        idx = df.index[df[LONCOL].between(min(w,e), max(w,e)) & df[LATCOL].between(min(s,n), max(s,n))]\n",
    "        if len(idx) == 0: continue\n",
    "        raster_cache = {}; filled_any = False\n",
    "        for p in ALL_PREDICTORS:\n",
    "            if p == AEZ_COL: continue\n",
    "            pth = open_country_var_path(drive, c, p)\n",
    "            if pth is None: continue\n",
    "            if p not in raster_cache: raster_cache[p] = rasterio.open(pth)\n",
    "            vals = _sample_raster_at_lonlat(raster_cache[p], df.loc[idx, LONCOL].values, df.loc[idx, LATCOL].values)\n",
    "            df.loc[idx, p] = pd.to_numeric(vals, errors=\"coerce\").astype(\"float64\")\n",
    "            filled_any = True\n",
    "        if filled_any:\n",
    "            print(f\"   • Training fill from rasters for {c}: {len(idx)} points\")\n",
    "    return df\n",
    "\n",
    "def list_countries_auto(drive):\n",
    "    names = set()\n",
    "    if drive is None:\n",
    "        if os.path.isdir(LOCAL_BY_COUNTRY_DIR):\n",
    "            for d in os.listdir(LOCAL_BY_COUNTRY_DIR):\n",
    "                if os.path.isdir(os.path.join(LOCAL_BY_COUNTRY_DIR, d)): names.add(d)\n",
    "    else:\n",
    "        byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
    "        if byc:\n",
    "            subs = _drive_query(drive, f\"'{byc}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "            for it in subs: names.add(it[\"title\"])\n",
    "    return sorted(names)\n",
    "\n",
    "# ----------------- TRAIN REGIONAL MODEL -----------------\n",
    "def _borrow_neighbors(df_all, base_cols, max_points):\n",
    "    if COUNTRY_COL not in df_all.columns:\n",
    "        df_all[COUNTRY_COL] = \"Unknown\"\n",
    "    keep_cols = list(dict.fromkeys(base_cols))\n",
    "    pooled = df_all[keep_cols].dropna(subset=[TARGET, LONCOL, LATCOL]).copy()\n",
    "    if len(pooled) > max_points:\n",
    "        pooled = pooled.sample(max_points, random_state=RANDOM_SEED)\n",
    "    if pooled[TARGET].nunique() < 2:\n",
    "        other = 1 - int(pooled[TARGET].iloc[0])\n",
    "        extra = df_all[df_all[TARGET] == other]\n",
    "        if not extra.empty:\n",
    "            take = min(1000, len(extra))\n",
    "            pooled = pd.concat([pooled, extra.sample(take, random_state=RANDOM_SEED)[keep_cols]], ignore_index=True)\n",
    "    if COUNTRY_COL not in pooled.columns:\n",
    "        pooled[COUNTRY_COL] = \"Unknown\"\n",
    "    pooled[COUNTRY_COL] = pooled[COUNTRY_COL].astype(str)\n",
    "    return pooled.reset_index(drop=True)\n",
    "\n",
    "def train_region_model(drive, region_name, df_all, countries):\n",
    "    print(f\"=== Train regional model: {region_name} ===\")\n",
    "\n",
    "    # Fill predictors (uses AEZ cache + per-country AEZ tiles)\n",
    "    df_all = _fill_training_predictors_from_country_rasters(drive, df_all, countries)\n",
    "\n",
    "    use_preds = [p for p in ALL_PREDICTORS if p in df_all.columns]\n",
    "    base_cols = [TARGET, LONCOL, LATCOL, COUNTRY_COL] + use_preds\n",
    "    df = _borrow_neighbors(df_all, base_cols, BORROW_MAX)\n",
    "\n",
    "    if len(df) < 200 or df[TARGET].nunique() < 2:\n",
    "        raise RuntimeError(f\"Insufficient pooled points for region training (n={len(df)}, classes={df[TARGET].nunique()}).\")\n",
    "\n",
    "    num_features = [f for f in use_preds if f != AEZ_COL]\n",
    "    cat_features = [AEZ_COL] if AEZ_COL in use_preds else []\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", make_pipeline(SimpleImputer(strategy=\"median\")), num_features),\n",
    "            (\"cat\", make_pipeline(SimpleImputer(strategy=\"most_frequent\"),\n",
    "                                  OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)), cat_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=RF_TREES, max_features=\"sqrt\", class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1, random_state=RANDOM_SEED, bootstrap=True,\n",
    "    )\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"rf\", rf)])\n",
    "\n",
    "    # ---------- Group-aware OOF to fit calibrator ----------\n",
    "    df[\"farm_id\"] = assign_farm_ids(df, lon_col=LONCOL, lat_col=LATCOL)\n",
    "    n_splits = 5 if 0.19 <= TEST_FRACTION <= 0.21 else max(3, int(round(1.0 / max(TEST_FRACTION, 1e-3))))\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    X_all = df[use_preds].copy()\n",
    "    y_all = df[TARGET].astype(int).values\n",
    "    g_all = df[\"farm_id\"].values\n",
    "    countries_all = df[COUNTRY_COL].astype(str).values\n",
    "    lon_all = df[LONCOL].values\n",
    "    lat_all = df[LATCOL].values\n",
    "\n",
    "    # enforce dtypes\n",
    "    for col in [f for f in use_preds if f != AEZ_COL]:\n",
    "        X_all[col] = pd.to_numeric(X_all[col], errors=\"coerce\").astype(\"float64\")\n",
    "    if AEZ_COL in use_preds:\n",
    "        X_all[AEZ_COL] = pd.to_numeric(X_all[AEZ_COL], errors=\"coerce\")\n",
    "        X_all[AEZ_COL] = np.rint(X_all[AEZ_COL]).astype(\"float64\")\n",
    "\n",
    "    oof_raw = np.full(len(X_all), np.nan, dtype=float)\n",
    "    classes_ = None\n",
    "    for tr_idx, te_idx in sgkf.split(X_all, y_all, groups=g_all):\n",
    "        tr_X = X_all.iloc[tr_idx]; tr_y = y_all[tr_idx]\n",
    "        te_X = X_all.iloc[te_idx]\n",
    "        pipe_fold = clone(pipe)\n",
    "        pipe_fold.fit(tr_X, tr_y.astype(int))\n",
    "        if classes_ is None:\n",
    "            classes_ = pipe_fold.named_steps[\"rf\"].classes_\n",
    "        pos_idx_fold = int(np.where(classes_ == 1)[0][0])\n",
    "        oof_raw[te_idx] = pipe_fold.predict_proba(te_X)[:, pos_idx_fold]\n",
    "\n",
    "    # fit final model on all data\n",
    "    pipe.fit(X_all, y_all.astype(int))\n",
    "    classes_ = pipe.named_steps[\"rf\"].classes_\n",
    "    pos_idx = int(np.where(classes_ == 1)[0][0])\n",
    "\n",
    "    # calibrate on OOF\n",
    "    calib = _fit_calibrator(oof_raw, y_all, CALIBRATION_METHOD)\n",
    "    oof_cal = _apply_calibrator(calib, oof_raw)\n",
    "\n",
    "    # ---------- GLOBAL threshold from OOF calibrated ----------\n",
    "    global_thr = _pick_threshold(y_all, oof_cal)\n",
    "\n",
    "    # ---------- Regional OOF metrics & confusion ----------\n",
    "    reg_metrics, reg_cm = _compute_basic_metrics(y_all, oof_cal, global_thr)\n",
    "    reg_metrics.update({\n",
    "        \"region\": region_name,\n",
    "        \"n_oof\": int(len(y_all)),\n",
    "        \"threshold_strategy\": THRESHOLD_STRATEGY,\n",
    "        \"global_threshold\": float(global_thr),\n",
    "        \"calibration\": CALIBRATION_METHOD,\n",
    "    })\n",
    "\n",
    "    # feature importances from final model\n",
    "    out_names = list(pipe.named_steps[\"pre\"].get_feature_names_out())\n",
    "    rf_imp = pipe.named_steps[\"rf\"].feature_importances_\n",
    "    fi_df = pd.DataFrame({\"feature\": out_names, \"importance\": rf_imp}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    # ---------- Prep folders ----------\n",
    "    if drive is None:\n",
    "        reg_base = os.path.join(LOCAL_MODEL_DIR, REGIONAL_MODELS_FOLDER, region_name)\n",
    "        os.makedirs(reg_base, exist_ok=True)\n",
    "        c_base_root = os.path.join(LOCAL_MODEL_DIR, COUNTRY_MODELS_FOLDER)\n",
    "        os.makedirs(c_base_root, exist_ok=True)\n",
    "    else:\n",
    "        mt = get_or_create_folder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "        reg = get_or_create_folder(drive, mt, REGIONAL_MODELS_FOLDER)\n",
    "        reg_base = get_or_create_folder(drive, reg, region_name)\n",
    "        c_base_root = get_or_create_folder(drive, mt, COUNTRY_MODELS_FOLDER)\n",
    "\n",
    "    # ---------- Ensure country tags (bbox fallback) ----------\n",
    "    assigned = countries_all.copy()\n",
    "    unknown_mask = (assigned == \"Unknown\") | (assigned == \"\") | pd.isna(assigned)\n",
    "    if unknown_mask.any():\n",
    "        for ctry in countries:\n",
    "            try:\n",
    "                ref = open_country_var_path(drive, ctry, \"CLE\") or open_country_var_path(drive, ctry, AEZ_COL) or open_aez_path(drive)\n",
    "                if not ref: continue\n",
    "                w, s, e, n = _clip_bounds_from_reference(ref)\n",
    "                inside = unknown_mask & (lon_all >= min(w, e)) & (lon_all <= max(w, e)) \\\n",
    "                                   & (lat_all >= min(s, n)) & (lat_all <= max(s, n))\n",
    "                assigned[inside] = ctry\n",
    "            except Exception:\n",
    "                pass\n",
    "        countries_all = assigned\n",
    "\n",
    "    # ---------- Per-country thresholds & OOF metrics ----------\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"country\": countries_all.astype(str),\n",
    "        \"y\": y_all.astype(int),\n",
    "        \"prob_cal\": oof_cal.astype(float),\n",
    "    })\n",
    "\n",
    "    per_rows = []\n",
    "    per_thr = {}\n",
    "\n",
    "    for ctry in sorted(set(countries)):\n",
    "        sub = oof_df[oof_df[\"country\"] == ctry]\n",
    "        n_total = int(len(sub))\n",
    "        n_pos = int((sub[\"y\"] == 1).sum())\n",
    "        n_neg = n_total - n_pos\n",
    "\n",
    "        if (n_total < MIN_TOTAL_FOR_COUNTRY) or (n_pos < MIN_POS_NEG_FOR_COUNTRY) or (n_neg < MIN_POS_NEG_FOR_COUNTRY):\n",
    "            thr_c = float(global_thr); note = \"fallback_global\"\n",
    "        else:\n",
    "            thr_c = _pick_threshold(sub[\"y\"].values, sub[\"prob_cal\"].values); note = \"oof_calibrated\"\n",
    "\n",
    "        per_thr[ctry] = float(thr_c)\n",
    "        m_c, cm_c = _compute_basic_metrics(sub[\"y\"].values, sub[\"prob_cal\"].values, thr_c)\n",
    "        m_c.update({\n",
    "            \"region\": region_name, \"country\": ctry, \"n_oof\": n_total,\n",
    "            \"n_pos\": n_pos, \"n_neg\": n_neg, \"threshold_used\": float(thr_c),\n",
    "            \"note\": note\n",
    "        })\n",
    "        per_rows.append(m_c)\n",
    "\n",
    "        # save country artifacts (metrics + confusion + threshold.json)\n",
    "        cdir = get_or_create_folder(drive, c_base_root, ctry)\n",
    "        with tempfile.TemporaryDirectory() as tdir:\n",
    "            with open(os.path.join(tdir, \"test_metrics.json\"), \"w\") as f:\n",
    "                json.dump(m_c, f, indent=2)\n",
    "            upload_path(drive, os.path.join(tdir, \"test_metrics.json\"), cdir, \"test_metrics.json\")\n",
    "\n",
    "            png = os.path.join(tdir, \"test_confusion.png\")\n",
    "            _save_confusion_png(cm_c, f\"{ctry} — OOF Confusion (thr={thr_c:.3f})\", png)\n",
    "            upload_path(drive, png, cdir, \"test_confusion.png\")\n",
    "\n",
    "            with open(os.path.join(tdir, \"threshold.json\"), \"w\") as f:\n",
    "                json.dump({\n",
    "                    \"threshold_strategy\": THRESHOLD_STRATEGY,\n",
    "                    \"mode\": THRESHOLD_MODE,\n",
    "                    \"calibration\": CALIBRATION_METHOD,\n",
    "                    \"threshold\": float(thr_c),\n",
    "                    \"n_total\": n_total, \"n_pos\": n_pos, \"n_neg\": n_neg,\n",
    "                    \"note\": note\n",
    "                }, f, indent=2)\n",
    "            upload_path(drive, os.path.join(tdir, \"threshold.json\"), cdir, \"threshold.json\")\n",
    "\n",
    "    # regional CSV summary for per-country\n",
    "    if len(per_rows) > 0:\n",
    "        per_df = pd.DataFrame(per_rows).sort_values(\"country\")\n",
    "    else:\n",
    "        per_df = pd.DataFrame(columns=[\"country\",\"n_oof\",\"precision\",\"recall\",\"f1\",\"roc_auc\",\"pr_auc\",\n",
    "                                       \"tn\",\"fp\",\"fn\",\"tp\",\"threshold_used\",\"note\"])\n",
    "\n",
    "    # ---------- Save regional artifacts ----------\n",
    "    with tempfile.TemporaryDirectory() as tdir:\n",
    "        with open(os.path.join(tdir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(reg_metrics, f, indent=2)\n",
    "        upload_path(drive, os.path.join(tdir, \"metrics.json\"), reg_base, \"metrics.json\")\n",
    "\n",
    "        fi_df.to_csv(os.path.join(tdir, \"feature_importance.csv\"), index=False)\n",
    "        upload_path(drive, os.path.join(tdir, \"feature_importance.csv\"), reg_base, \"feature_importance.csv\")\n",
    "\n",
    "        per_df.to_csv(os.path.join(tdir, \"per_country_test_metrics.csv\"), index=False)\n",
    "        upload_path(drive, os.path.join(tdir, \"per_country_test_metrics.csv\"), reg_base, \"per_country_test_metrics.csv\")\n",
    "\n",
    "        png = os.path.join(tdir, \"confusion.png\")\n",
    "        _save_confusion_png(reg_cm, f\"{region_name} — OOF Confusion ({THRESHOLD_STRATEGY}, thr={global_thr:.3f})\", png)\n",
    "        upload_path(drive, png, reg_base, \"confusion.png\")\n",
    "\n",
    "        joblib.dump(\n",
    "            {\n",
    "                \"pipeline\": pipe,\n",
    "                \"predictors\": use_preds,\n",
    "                \"positive_class_index\": int(pos_idx),\n",
    "                \"calibration\": calib,\n",
    "                \"global_threshold\": float(global_thr),\n",
    "                \"threshold_strategy\": THRESHOLD_STRATEGY,\n",
    "                \"per_country_thresholds\": per_thr,\n",
    "                \"num_features\": [f for f in use_preds if f != AEZ_COL],\n",
    "                \"cat_features\": [AEZ_COL] if AEZ_COL in use_preds else [],\n",
    "            },\n",
    "            os.path.join(tdir, \"model.joblib\"),\n",
    "        )\n",
    "        upload_path(drive, os.path.join(tdir, \"model.joblib\"), reg_base, \"model.joblib\")\n",
    "\n",
    "    model = {\n",
    "        \"pipe\": pipe,\n",
    "        \"use_preds\": use_preds,\n",
    "        \"pos_idx\": int(pos_idx),\n",
    "        \"calib\": calib,\n",
    "        \"global_thr\": float(global_thr),\n",
    "        \"per_thr\": per_thr,\n",
    "        \"reg_base\": reg_base\n",
    "    }\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------- PREDICT ONE COUNTRY ------------------\n",
    "def predict_country(drive, country, model, thr, grid_pack):\n",
    "    out_dir = get_or_create_folder(drive, ROOT_FOLDER_ID, OUTPUT_FOLDER) if drive else LOCAL_OUTPUT_DIR\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    pipe   = model[\"pipe\"]\n",
    "    pos_idx= model[\"pos_idx\"]\n",
    "    calib  = model[\"calib\"]\n",
    "\n",
    "    H, W, transform, crs, cropland = grid_pack\n",
    "    raster_cache = {}\n",
    "\n",
    "    # AEZ: prefer per-country tile, fallback to global AEZ\n",
    "    aez_src_path = open_country_var_path(drive, country, AEZ_COL) or open_aez_path(drive)\n",
    "    if AEZ_COL in model[\"use_preds\"] and aez_src_path is not None:\n",
    "        raster_cache[AEZ_COL] = rasterio.open(aez_src_path)\n",
    "\n",
    "    for v in model[\"use_preds\"]:\n",
    "        if v == AEZ_COL: continue\n",
    "        p = open_country_var_path(drive, country, v)\n",
    "        if p is not None: raster_cache[v] = rasterio.open(p)\n",
    "\n",
    "    base_profile = {\n",
    "        \"driver\": \"GTiff\",\"height\": H,\"width\": W,\"count\": 1,\"crs\": crs,\"transform\": transform,\n",
    "        \"compress\": \"LZW\",\"tiled\": True,\"blockxsize\": 512,\"blockysize\": 512,\n",
    "    }\n",
    "    prob_profile = {**base_profile, \"dtype\": \"float32\", \"nodata\": -9999.0}\n",
    "    bin_profile  = {**base_profile, \"dtype\": \"uint8\", \"nodata\": 255}\n",
    "    cnt_profile  = {**base_profile, \"dtype\": \"uint8\", \"nodata\": 0}\n",
    "\n",
    "    prob_name = f\"{country}_RF_probability_percent.tif\"\n",
    "    bin_name  = f\"{country}_RF_binary_0_1_cropland.tif\"\n",
    "    cnt_name  = f\"{country}_RF_predictors_count.tif\"\n",
    "\n",
    "    eligible_total = 0; positive_total = 0\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tdir:\n",
    "        prob_tmp = os.path.join(tdir, prob_name)\n",
    "        bin_tmp  = os.path.join(tdir, bin_name)\n",
    "        cnt_tmp  = os.path.join(tdir, cnt_name)\n",
    "        with rasterio.open(prob_tmp, \"w\", **prob_profile) as dst_prob, \\\n",
    "             rasterio.open(bin_tmp,  \"w\", **bin_profile)  as dst_bin,  \\\n",
    "             rasterio.open(cnt_tmp,  \"w\", **cnt_profile) as dst_cnt:\n",
    "\n",
    "            for win in tqdm(_iter_tiles(H, W, TILE_SIZE), total=_tile_count(H, W, TILE_SIZE), desc=f\"Predict {country}\"):\n",
    "                r0, r1 = int(win.row_off), int(win.row_off + win.height)\n",
    "                c0, c1 = int(win.col_off), int(win.col_off + win.width)\n",
    "                tile_h, tile_w = (r1 - r0), (c1 - c0)\n",
    "                tile_transform = transform * Affine.translation(c0, r0)\n",
    "\n",
    "                crop_tile = cropland[r0:r1, c0:c1]\n",
    "                if not crop_tile.any():\n",
    "                    dst_prob.write(np.full((1, tile_h, tile_w), prob_profile[\"nodata\"], np.float32), window=win)\n",
    "                    dst_bin.write (np.full((1, tile_h, tile_w), bin_profile[\"nodata\"], np.uint8), window=win)\n",
    "                    dst_cnt.write (np.zeros((1, tile_h, tile_w), np.uint8), window=win)\n",
    "                    continue\n",
    "\n",
    "                stack = np.full((len(model[\"use_preds\"]), tile_h, tile_w), np.nan, dtype=np.float32)\n",
    "                valid_count = np.zeros((tile_h, tile_w), dtype=np.uint8)\n",
    "\n",
    "                for j, v in enumerate(model[\"use_preds\"]):\n",
    "                    src = raster_cache.get(v)\n",
    "                    if src is None:\n",
    "                        p = open_country_var_path(drive, country, v) if v != AEZ_COL else (open_country_var_path(drive, country, AEZ_COL) or open_aez_path(drive))\n",
    "                        if p: src = raster_cache[v] = rasterio.open(p)\n",
    "                        else: continue\n",
    "                    dst_arr = np.full((tile_h, tile_w), np.nan, dtype=np.float32)\n",
    "                    dst_nd  = -9999.0\n",
    "                    resamp  = PER_BAND_RESAMPLING.get(v, RESAMPLING)\n",
    "                    try:\n",
    "                        reproject(source=rasterio.band(src, 1), destination=dst_arr,\n",
    "                                  src_transform=src.transform, src_crs=src.crs,\n",
    "                                  dst_transform=tile_transform, dst_crs=crs,\n",
    "                                  dst_nodata=dst_nd, resampling=resamp)\n",
    "                    except Exception as e:\n",
    "                        print(f\"      ! reprojection failed for {v}: {e}\"); continue\n",
    "                    if src.nodata is not None:\n",
    "                        dst_arr = np.where(np.isclose(dst_arr, src.nodata), np.nan, dst_arr)\n",
    "                    dst_arr = np.where(np.isclose(dst_arr, dst_nd), np.nan, dst_arr)\n",
    "                    stack[j] = dst_arr; valid_count += np.isfinite(dst_arr)\n",
    "\n",
    "                req = (len(model[\"use_preds\"]) if STRICT_ALL_BANDS else min(MIN_PREDICTORS, len(model[\"use_preds\"])))\n",
    "                good = crop_tile & (valid_count >= req)\n",
    "                prob_tile = np.full((tile_h, tile_w), np.nan, dtype=np.float32)\n",
    "\n",
    "                if good.any():\n",
    "                    X = stack.reshape(len(model[\"use_preds\"]), -1).T\n",
    "                    good_flat = good.reshape(-1)\n",
    "                    X_good = X[good_flat].astype(\"float64\")\n",
    "                    X_good_df = pd.DataFrame(X_good, columns=model[\"use_preds\"])\n",
    "\n",
    "                    # enforce dtypes\n",
    "                    num_features = [f for f in model[\"use_preds\"] if f != AEZ_COL]\n",
    "                    for col in num_features:\n",
    "                        X_good_df[col] = pd.to_numeric(X_good_df[col], errors=\"coerce\").astype(\"float64\")\n",
    "                    if AEZ_COL in X_good_df.columns:\n",
    "                        X_good_df[AEZ_COL] = pd.to_numeric(X_good_df[AEZ_COL], errors=\"coerce\")\n",
    "                        X_good_df[AEZ_COL] = np.rint(X_good_df[AEZ_COL]).astype(\"float64\")\n",
    "\n",
    "                    prob_raw = pipe.predict_proba(X_good_df)[:, pos_idx].astype(np.float32)\n",
    "                    prob_vals = _apply_calibrator(calib, prob_raw).astype(np.float32)\n",
    "\n",
    "                    bin_vals  = (prob_vals >= thr).astype(np.uint8)\n",
    "                    prob_tile.reshape(-1)[good_flat] = prob_vals\n",
    "\n",
    "                    mask_arr = np.isfinite(prob_tile)\n",
    "                    prob_out = np.full((1, tile_h, tile_w), prob_profile[\"nodata\"], np.float32)\n",
    "                    prob_out[0][mask_arr] = (prob_tile[mask_arr] * 100.0).astype(np.float32)\n",
    "                    dst_prob.write(prob_out, window=win)\n",
    "\n",
    "                    bin_out = np.full((1, tile_h, tile_w), bin_profile[\"nodata\"], np.uint8)\n",
    "                    bin_out[0].reshape(-1)[good_flat] = bin_vals\n",
    "                    dst_bin.write(bin_out, window=win)\n",
    "\n",
    "                    eligible_total += int(mask_arr.sum()); positive_total += int(bin_vals.sum())\n",
    "                else:\n",
    "                    dst_prob.write(np.full((1, tile_h, tile_w), prob_profile[\"nodata\"], np.float32), window=win)\n",
    "                    dst_bin.write (np.full((1, tile_h, tile_w), bin_profile[\"nodata\"], np.uint8), window=win)\n",
    "\n",
    "                dst_cnt.write(valid_count[np.newaxis], window=win)\n",
    "\n",
    "        upload_path(drive, prob_tmp, out_dir, prob_name)\n",
    "        upload_path(drive, bin_tmp,  out_dir, bin_name)\n",
    "        upload_path(drive, cnt_tmp,  out_dir, cnt_name)\n",
    "\n",
    "    frac = (positive_total / max(1, eligible_total)) * 100.0\n",
    "    print(f\"   • predicted irrigated fraction inside cropland: {frac:.3f}% ({positive_total:,}/{eligible_total:,})\")\n",
    "\n",
    "# ----------------------------- RUNNER -------------------\n",
    "def run_region_then_countries(use_drive=True, countries=None, region_name=\"US\"):\n",
    "    if use_drive:\n",
    "        try:\n",
    "            drive  # noqa: F821\n",
    "            _drive = drive\n",
    "            try: _drive.auth.service.http.timeout = 120\n",
    "            except Exception: pass\n",
    "        except NameError:\n",
    "            raise RuntimeError(\"PyDrive 'drive' not found. Authenticate and expose 'drive', or set use_drive=False.\")\n",
    "    else:\n",
    "        _drive = None\n",
    "        os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    df_all = load_points_df(_drive)\n",
    "    for c in [TARGET, LONCOL, LATCOL] + [p for p in ALL_PREDICTORS if p in df_all.columns and p != AEZ_COL]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    df_all = df_all.dropna(subset=[TARGET, LONCOL, LATCOL]).reset_index(drop=True)\n",
    "    df_all[TARGET] = df_all[TARGET].astype(int)\n",
    "\n",
    "    if not countries:\n",
    "        countries = [\n",
    "            \"Nigeria\",\n",
    "            # \"Sudan\",\"Chad\", \"Niger\"\n",
    "            # \"Florida\",\"Georgia\",\"Arkansas\",\"Kentucky\",\"Louisiana\",\n",
    "            #          \"Mississippi\",\"South Carolina\",\"North Carolina\"\n",
    " \n",
    "        ]\n",
    "\n",
    "    model = train_region_model(_drive, region_name, df_all, countries)\n",
    "\n",
    "    # Predict\n",
    "    print(f\"=== PREDICTION per-country (threshold mode: {THRESHOLD_MODE}) ===\")\n",
    "    for c in countries:\n",
    "        try:\n",
    "            H, W, transform, crs, cropland, _ = country_grid_and_mask(_drive, c)\n",
    "            thr_c = model[\"per_thr\"].get(c, model[\"global_thr\"]) if THRESHOLD_MODE in (\"country\",\"hybrid\") else model[\"global_thr\"]\n",
    "            predict_country(_drive, c, model, thr_c, (H, W, transform, crs, cropland))\n",
    "            print(f\"✓ {c}: maps → {OUTPUT_FOLDER}, model → {MODEL_FOLDER}/{REGIONAL_MODELS_FOLDER}/{region_name}, thr={thr_c:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {c}: {e}\")\n",
    "\n",
    "    print(\"✅ Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_region_then_countries(use_drive=True, countries=None, region_name=\"Africa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301e4d6-1dd7-434f-9be3-897c8c6eacb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34847c05-6174-4360-ab85-085b68a31fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab577e3-5d86-450b-8015-879bf6cb48d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e374d6fe-279e-40d2-94af-32f6b8fe5d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AEI] Shapefile bundle at /tmp/aei_admin_v75mbg_n → found ['.cpg', '.dbf', '.prj', '.shp', '.shx']\n",
      "\n",
      "=== Nigeria (per-admin from shapefile; AEI in hectares) ===\n",
      "   ✓ Wrote Binary/Nigeria_AEI_binary_0_1.tif\n",
      "\n",
      "✅ Done (per-admin from shapefile; AEI in hectares; NaNs preserved).\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Per-admin AEI-constrained binarization using ONLY the admin shapefile.\n",
    "\n",
    "For each country probability TIFF:\n",
    "  - Use unit_code polygons from AEI_2020_2_withAEI.* (AEI in HECTARES).\n",
    "  - For each unit_code, pick pixels INSIDE (centroid-in) from highest probability\n",
    "    downward until its AEI quota is met (quota = floor(AEI_ha*10_000 / 900) pixels).\n",
    "  - Preserve NaNs from the probability input.\n",
    "  - Write ONE binary 0/1 TIFF per country to:\n",
    "      Drive → CountryModelPredicted/Probability/Binary/\n",
    "  - Also write a CSV summary per country with thresholds and counts.\n",
    "\n",
    "No AEI base raster is used.\n",
    "\"\"\"\n",
    "\n",
    "import os, re, math, csv, tempfile, warnings, unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import shape, box\n",
    "from shapely.ops import transform as shp_transform\n",
    "from shapely.strtree import STRtree\n",
    "from pyproj import Transformer\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "ROOT_FOLDER_ID        = os.environ.get(\"ROOT_FOLDER_ID\", \"1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\")\n",
    "PARENT_FOLDER_NAME    = \"CountryModelPredicted\"\n",
    "PROB_SUBFOLDER_NAME   = \"Probability\"      # fuzzy-matched (also matches \"Porbability\")\n",
    "NATIONAL_AEI_FOLDER   = \"National AEI\"     # shapefile lives here\n",
    "\n",
    "# Probability & binning\n",
    "PIXEL_AREA_M2         = 30.0 * 30.0        # 900 m² per 30 m pixel\n",
    "TILE                  = 1024               # reduce if memory tight\n",
    "SCALE                 = 1000               # probability bins: 0..1 → 0..1000\n",
    "\n",
    "# Shapefile (AEI in HECTARES)\n",
    "ADMIN_SHP_BASE        = \"Copy of AEI_2020_2_with_AEI\"  # base name (no extension)\n",
    "ADMIN_CODE_COL        = \"unit_code\"\n",
    "ADMIN_AEI_COLS        = [\"AEI_2020\", \"AEI2020\", \"AEI\"]  # values in HECTARES\n",
    "ADMIN_CNTRY_COLS      = [\"name_cntr\", \"name_cntr1\", \"name_admin\", \"ST_NM\"]\n",
    "\n",
    "# Deterministic tie-breaking\n",
    "RNG_SEED              = int(os.environ.get(\"AEI_RNG_SEED\", \"0\"))\n",
    "_rng = np.random.default_rng(RNG_SEED)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid: return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        tkey = re.sub(r\"\\s+\", \"\", f.get(\"title\",\"\").lower())\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def _resolve_field(props_sample: dict, candidates, required=False):\n",
    "    \"\"\"Fuzzy, case-insensitive resolver for attribute fields.\"\"\"\n",
    "    def canon(s): return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
    "    keys = list(props_sample.keys())\n",
    "    norm_map = {canon(k): k for k in keys}\n",
    "    for want in candidates:\n",
    "        w = canon(want)\n",
    "        if w in norm_map:\n",
    "            return norm_map[w]\n",
    "    want_roots = {canon(want) for want in candidates}\n",
    "    for k in keys:\n",
    "        ck = canon(k)\n",
    "        if any(root in ck for root in want_roots):\n",
    "            return k\n",
    "    if required:\n",
    "        raise RuntimeError(f\"Required attribute not found. Looked for: {candidates}. Available: {keys}\")\n",
    "    return None\n",
    "\n",
    "def download_to_temp(drive_file, suffix):\n",
    "    p = tempfile.NamedTemporaryFile(delete=False, suffix=suffix).name\n",
    "    drive_file.GetContentFile(p)\n",
    "    return p\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    if res: return res[0][\"id\"]\n",
    "    nf = drive.CreateFile({\"title\": name, \"parents\":[{\"id\": parent_id}], \"mimeType\":\"application/vnd.google-apps.folder\"})\n",
    "    nf.Upload()\n",
    "    return nf[\"id\"]\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\":[{\"id\": parent_id}]})\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "# ---------------------- Name helpers ----------------------\n",
    "def _norm(s): return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
    "def _canon(s):\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s)).encode(\"ascii\",\"ignore\").decode()\n",
    "    return re.sub(r\"\\s+\",\"\", s.lower())\n",
    "\n",
    "def _extract_country_from_fname(fname):\n",
    "    # Albania_RF_probability_percent.tif → Albania\n",
    "    fn = re.sub(r\"\\s*\\(.*\\)\\.tif(f)?$\", \".tif\", fname, flags=re.IGNORECASE)\n",
    "    m = re.match(r\"(.+?)_RF_probability_percent\", fn, flags=re.IGNORECASE)\n",
    "    if m: return m.group(1)\n",
    "    return re.sub(r\"\\.tif(f)?$\", \"\", fn, flags=re.IGNORECASE)\n",
    "\n",
    "# ---------------------- Shapefile helpers -----------------\n",
    "def _download_shapefile_bundle(drive, folder_id, base):\n",
    "    \"\"\"\n",
    "    Download AEI_2020_2_withAEI.* into a single temp directory, ensuring\n",
    "    all sidecars share the SAME basename so GDAL/Fiona can see attributes.\n",
    "    Returns a dict of local paths keyed by extension ('.shp', '.dbf', etc).\n",
    "    \"\"\"\n",
    "    exts = [\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\"]\n",
    "    items = {}\n",
    "    for it in list_files(drive, folder_id):\n",
    "        t = it.get(\"title\", \"\")\n",
    "        for e in exts:\n",
    "            if t.lower() == (base.lower() + e):\n",
    "                items[e] = it\n",
    "\n",
    "    if \".shp\" not in items or \".dbf\" not in items:\n",
    "        raise FileNotFoundError(f\"Missing pieces of {base} shapefile (need at least .shp and .dbf). Found: {sorted(items.keys())}\")\n",
    "\n",
    "    tmpdir = tempfile.mkdtemp(prefix=\"aei_admin_\")\n",
    "    out = {}\n",
    "    for e, it in items.items():\n",
    "        local_path = os.path.join(tmpdir, base + e)  # SAME BASENAME!\n",
    "        it.GetContentFile(local_path)\n",
    "        out[e] = local_path\n",
    "\n",
    "    print(f\"[AEI] Shapefile bundle at {tmpdir} → found {sorted(out.keys())}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _read_admins_for_raster(shp_path, raster_crs, raster_bounds):\n",
    "    \"\"\"\n",
    "    Load admin polygons, reproject to raster CRS (no AEI filtering).\n",
    "    - Reads field names from schema (not the first feature).\n",
    "    - Honors .cpg encoding when present.\n",
    "    - Skips null/malformed geometries defensively.\n",
    "    \"\"\"\n",
    "    feats, attrs = [], []\n",
    "    shp_dir = os.path.dirname(shp_path)\n",
    "    base = os.path.splitext(os.path.basename(shp_path))[0]\n",
    "    cpg_path = os.path.join(shp_dir, base + \".cpg\")\n",
    "\n",
    "    # Determine DBF encoding\n",
    "    encoding = None\n",
    "    if os.path.exists(cpg_path):\n",
    "        try:\n",
    "            with open(cpg_path, \"r\", encoding=\"ascii\", errors=\"ignore\") as f:\n",
    "                enc_line = f.read().strip()\n",
    "                if enc_line:\n",
    "                    encoding = enc_line\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _open_fiona(enc):\n",
    "        return fiona.open(shp_path, encoding=enc) if enc else fiona.open(shp_path)\n",
    "\n",
    "    shp_crs_final = None\n",
    "    with fiona.Env(SHAPE_RESTORE_SHX='YES'):\n",
    "        # Try cpg encoding → utf-8 → latin1\n",
    "        tried = [encoding, \"utf-8\", \"latin1\"]\n",
    "        last_err = None\n",
    "        for enc in tried:\n",
    "            try:\n",
    "                with _open_fiona(enc) as src:\n",
    "                    shp_crs_local = src.crs_wkt or src.crs\n",
    "                    props_schema = (src.schema or {}).get(\"properties\", {})\n",
    "                    field_names = list(props_schema.keys())\n",
    "                    if not field_names:\n",
    "                        raise RuntimeError(\"No attribute fields in schema (DBF not visible).\")\n",
    "\n",
    "                    # Resolve keys against SCHEMA (not a sample feature)\n",
    "                    dummy_props = {k: None for k in field_names}\n",
    "                    code_key  = _resolve_field(dummy_props, [ADMIN_CODE_COL], required=True)\n",
    "                    aei_key   = _resolve_field(dummy_props, ADMIN_AEI_COLS, required=True)  # AEI in hectares\n",
    "                    cntry_key = _resolve_field(dummy_props, ADMIN_CNTRY_COLS, required=False)\n",
    "\n",
    "                    # Transform raster bounds to shapefile CRS for coarse prefilter\n",
    "                    if shp_crs_local:\n",
    "                        rb_to_shp = Transformer.from_crs(raster_crs, shp_crs_local, always_xy=True)\n",
    "                        rb_shp = shp_transform(lambda x, y: rb_to_shp.transform(x, y), box(*raster_bounds))\n",
    "                    else:\n",
    "                        rb_shp = box(*raster_bounds)\n",
    "\n",
    "                    # Iterate features\n",
    "                    for rec in src:\n",
    "                        gj = rec.get(\"geometry\")\n",
    "                        if gj is None:\n",
    "                            continue  # null geometry → skip (defensive)\n",
    "                        try:\n",
    "                            g = shape(gj)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        if g.is_empty:\n",
    "                            continue\n",
    "                        if not g.intersects(rb_shp):\n",
    "                            continue\n",
    "\n",
    "                        props = rec.get(\"properties\") or {}\n",
    "                        try:\n",
    "                            uc = int(props[code_key])\n",
    "                            aei_ha = float(props[aei_key])  # hectares\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "                        feats.append(g)\n",
    "                        attrs.append({\n",
    "                            \"unit_code\": uc,\n",
    "                            \"aei_ha\": aei_ha,\n",
    "                            \"country\": str(props.get(cntry_key, \"\")).strip() if cntry_key else \"\"\n",
    "                        })\n",
    "                # success → keep the CRS we used\n",
    "                shp_crs_final = shp_crs_local\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "        if last_err and not feats:\n",
    "            raise RuntimeError(f\"Failed to read attributes from shapefile. Tried encodings {tried}. Last error: {last_err}\")\n",
    "\n",
    "    if not feats:\n",
    "        return [], [], None, {}\n",
    "\n",
    "    # Reproject to raster CRS for rasterize\n",
    "    shp_crs = shp_crs_final\n",
    "    if not shp_crs:\n",
    "        print(\"⚠️  Shapefile has no CRS (.prj missing). Assuming raster CRS.\")\n",
    "        shp_crs = raster_crs\n",
    "\n",
    "    transformer = Transformer.from_crs(shp_crs, raster_crs, always_xy=True)\n",
    "    geoms_ras = [shp_transform(lambda x, y: transformer.transform(x, y), g) for g in feats]\n",
    "\n",
    "    tree_ras = STRtree(geoms_ras)\n",
    "    # IMPORTANT: map by WKB (value identity), not id(...)\n",
    "    g2i_wkb = {g.wkb: i for i, g in enumerate(geoms_ras)}\n",
    "\n",
    "    return geoms_ras, attrs, tree_ras, g2i_wkb\n",
    "\n",
    "\n",
    "# ---- STRtree helper: get candidate indices robustly (Shapely 2 or fallback) ----\n",
    "def _tree_candidate_indices(tree_ras, tile_poly, geoms_ras, g2i_wkb):\n",
    "    \"\"\"\n",
    "    Return list of indices of geoms that intersect tile_poly.\n",
    "    Prefer Shapely 2's predicate indices; otherwise map WKBs.\n",
    "    \"\"\"\n",
    "    # Fast path: Shapely 2 can return integer indices with predicate\n",
    "    try:\n",
    "        idx = tree_ras.query(tile_poly, predicate=\"intersects\")\n",
    "        if isinstance(idx, np.ndarray) and np.issubdtype(idx.dtype, np.integer):\n",
    "            return idx.tolist()\n",
    "    except TypeError:\n",
    "        # Older shapely: predicate argument not supported\n",
    "        pass\n",
    "\n",
    "    # Fallback: geometry array → map to indices by WKB, then precise intersects\n",
    "    cand = tree_ras.query(tile_poly)\n",
    "    if isinstance(cand, np.ndarray):\n",
    "        cand = cand.tolist()\n",
    "    out = []\n",
    "    for g in cand:\n",
    "        i = g2i_wkb.get(g.wkb, None)\n",
    "        if i is None:\n",
    "            # last resort: linear search (rare)\n",
    "            try:\n",
    "                i = next(j for j, gg in enumerate(geoms_ras) if gg.equals(g))\n",
    "            except StopIteration:\n",
    "                continue\n",
    "        if geoms_ras[i].intersects(tile_poly):\n",
    "            out.append(i)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------- Raster helpers -------------------\n",
    "def _iter_tiles(H, W, tile=TILE):\n",
    "    for r0 in range(0, H, tile):\n",
    "        for c0 in range(0, W, tile):\n",
    "            h = min(tile, H - r0)\n",
    "            w = min(tile, W - c0)\n",
    "            yield Window(c0, r0, w, h)\n",
    "\n",
    "def _read_prob_tile(src, W):\n",
    "    arr = src.read(1, window=W, out_dtype=\"float32\", masked=True).filled(np.nan)\n",
    "    finite = np.isfinite(arr)\n",
    "    if finite.any() and float(np.nanmax(arr[finite])) > 1.5:\n",
    "        arr[finite] /= 100.0\n",
    "    np.clip(arr, 0.0, 1.0, out=arr, where=finite)\n",
    "    return arr\n",
    "\n",
    "def _tile_bounds(window, transform):\n",
    "    left, top = transform * (window.col_off, window.row_off)\n",
    "    right, bottom = transform * (window.col_off + window.width, window.row_off + window.height)\n",
    "    x0, x1 = sorted([left, right])\n",
    "    y0, y1 = sorted([bottom, top])\n",
    "    return (x0, y0, x1, y1)\n",
    "\n",
    "# ---------------------- Core algorithm -------------------\n",
    "def aei_binarize_per_admin_from_shapefile(drive):\n",
    "    \"\"\"\n",
    "    Main entry: uses ONLY the admin shapefile with AEI in hectares\n",
    "    to allocate per-admin pixel quotas and write one binary per country.\n",
    "    \"\"\"\n",
    "    # Locate folders\n",
    "    cmp_id  = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id: raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "    prob_id = get_subfolder_fuzzy(drive, cmp_id, PROB_SUBFOLDER_NAME)\n",
    "    if not prob_id: raise RuntimeError(\"Probability folder not found (tried fuzzy match).\")\n",
    "    binary_id = get_or_create_folder(drive, prob_id, \"Binary\")\n",
    "\n",
    "    aei_folder_id = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, NATIONAL_AEI_FOLDER)\n",
    "    if not aei_folder_id: raise RuntimeError(\"National AEI folder not found at ROOT.\")\n",
    "\n",
    "    # Download admin shapefile bundle\n",
    "    shp_paths = _download_shapefile_bundle(drive, aei_folder_id, ADMIN_SHP_BASE)\n",
    "    shp_path  = shp_paths[\".shp\"]\n",
    "\n",
    "    # List probability TIFFs\n",
    "    files = [it for it in list_files(drive, prob_id)\n",
    "             if isinstance(it, dict)\n",
    "             and it.get(\"mimeType\") != \"application/vnd.google-apps.folder\"\n",
    "             and it.get(\"title\",\"\").lower().endswith((\".tif\",\".tiff\"))]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No probability TIFFs in Probability folder.\")\n",
    "\n",
    "    for it in files:\n",
    "        title   = it.get(\"title\",\"\")\n",
    "        country = _extract_country_from_fname(title)\n",
    "        print(f\"\\n=== {country} (per-admin from shapefile; AEI in hectares) ===\")\n",
    "\n",
    "        rtmp = download_to_temp(it, \".tif\")\n",
    "        with rasterio.open(rtmp) as src:\n",
    "            H, W = src.height, src.width\n",
    "            ras_crs = src.crs\n",
    "            rb = src.bounds\n",
    "            ras_bounds = (rb.left, rb.bottom, rb.right, rb.top)\n",
    "\n",
    "            # Read & subset admins, reproject to raster CRS\n",
    "            geoms_ras, attrs, tree_ras, g2i_wkb = _read_admins_for_raster(shp_path, ras_crs, ras_bounds)\n",
    "            if not geoms_ras:\n",
    "                print(\"  ⚠️  No admin polygons intersect this raster; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Optional filter by country name if present (keeps all if missing)\n",
    "            want = _canon(country)\n",
    "            keep = [i for i,a in enumerate(attrs) if (not a[\"country\"]) or _canon(a[\"country\"]) == want]\n",
    "            if keep and len(keep) < len(attrs):\n",
    "                geoms_ras = [geoms_ras[i] for i in keep]\n",
    "                attrs     = [attrs[i] for i in keep]\n",
    "                tree_ras  = STRtree(geoms_ras)\n",
    "                g2i_wkb   = {g.wkb: i for i, g in enumerate(geoms_ras)}\n",
    "\n",
    "            # Targets per admin (AEI in HECTARES → m² → pixels); include zeros\n",
    "            K_map = {}\n",
    "            for a in attrs:\n",
    "                aei_m2 = a[\"aei_ha\"] * 10_000.0\n",
    "                K_map[a[\"unit_code\"]] = int(math.floor(aei_m2 / PIXEL_AREA_M2))  # may be 0\n",
    "\n",
    "            # PASS 1: per-admin histograms of probability bins (centroid-in)\n",
    "            hists = defaultdict(Counter)\n",
    "            for w in _iter_tiles(H, W, TILE):\n",
    "                prob = _read_prob_tile(src, w)\n",
    "                valid = np.isfinite(prob)\n",
    "                if not valid.any():\n",
    "                    continue\n",
    "\n",
    "                tile_t = rasterio.windows.transform(w, src.transform)\n",
    "                tb = _tile_bounds(w, src.transform)\n",
    "                tile_poly = box(*tb)\n",
    "\n",
    "                idxs = _tree_candidate_indices(tree_ras, tile_poly, geoms_ras, g2i_wkb)\n",
    "                if len(idxs) == 0:\n",
    "                    continue\n",
    "\n",
    "                shapes = [(geoms_ras[i], attrs[i][\"unit_code\"]) for i in idxs]\n",
    "                labels = rasterize(\n",
    "                    shapes=shapes,\n",
    "                    out_shape=prob.shape,\n",
    "                    transform=tile_t,\n",
    "                    fill=0, dtype=\"int64\",\n",
    "                    all_touched=False  # centroid-in\n",
    "                )\n",
    "                m = valid & (labels != 0)\n",
    "                if not m.any():\n",
    "                    continue\n",
    "\n",
    "                p_int = np.zeros(prob.shape, dtype=np.int32)\n",
    "                p_int_valid = np.rint(prob[m] * SCALE).astype(np.int32)\n",
    "                p_int[m] = p_int_valid\n",
    "\n",
    "                uc = labels[m].ravel()\n",
    "                pi = p_int[m].ravel()\n",
    "                for u in np.unique(uc):\n",
    "                    sel = (uc == u)\n",
    "                    bc = np.bincount(pi[sel], minlength=SCALE+1)\n",
    "                    nz = np.nonzero(bc)[0]\n",
    "                    for b, v in zip(nz, bc[nz]):\n",
    "                        hists[u][int(b)] += int(v)\n",
    "\n",
    "            # thresholds per admin (quota 0 => thr=-1, no selection)\n",
    "            thr_map, need_eq_map = {}, {}\n",
    "            for u, K in K_map.items():\n",
    "                total = sum(hists[u].values())\n",
    "                if K <= 0 or total == 0:\n",
    "                    thr_map[u] = -1\n",
    "                    need_eq_map[u] = 0\n",
    "                    continue\n",
    "                K = min(K, total)\n",
    "                cum = 0; gt = 0\n",
    "                for b in range(SCALE, -1, -1):\n",
    "                    cnt = int(hists[u].get(b, 0))\n",
    "                    if cum + cnt >= K:\n",
    "                        thr_map[u] = b\n",
    "                        need_eq_map[u] = K - gt\n",
    "                        break\n",
    "                    cum += cnt; gt += cnt\n",
    "            need_eq_left = dict(need_eq_map)\n",
    "\n",
    "            # PASS 2: write binary output (NaN preserved; default 0; set 1s per admin)\n",
    "            out_profile = src.profile.copy()\n",
    "            out_profile.update(\n",
    "                driver=\"GTiff\",\n",
    "                height=H, width=W,\n",
    "                transform=src.transform,\n",
    "                count=1, dtype=\"float32\", nodata=np.nan,\n",
    "                compress=\"LZW\", tiled=True, blockxsize=512, blockysize=512,\n",
    "                BIGTIFF=\"IF_NEEDED\"\n",
    "            )\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\") as tmp_out:\n",
    "                out_local = tmp_out.name\n",
    "\n",
    "            with rasterio.open(out_local, \"w\", **out_profile) as dst:\n",
    "                for w in _iter_tiles(H, W, TILE):\n",
    "                    prob = _read_prob_tile(src, w)\n",
    "                    out_tile = np.full(prob.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "                    valid = np.isfinite(prob)\n",
    "                    if not valid.any():\n",
    "                        dst.write(out_tile, 1, window=w)\n",
    "                        continue\n",
    "\n",
    "                    tile_t = rasterio.windows.transform(w, src.transform)\n",
    "                    tb = _tile_bounds(w, src.transform)\n",
    "                    tile_poly = box(*tb)\n",
    "\n",
    "                    idxs = _tree_candidate_indices(tree_ras, tile_poly, geoms_ras, g2i_wkb)\n",
    "                    if len(idxs) == 0:\n",
    "                        dst.write(out_tile, 1, window=w)\n",
    "                        continue\n",
    "\n",
    "                    shapes = [(geoms_ras[i], attrs[i][\"unit_code\"]) for i in idxs]\n",
    "                    labels = rasterize(\n",
    "                        shapes=shapes,\n",
    "                        out_shape=prob.shape,\n",
    "                        transform=tile_t,\n",
    "                        fill=0, dtype=\"int64\",\n",
    "                        all_touched=False\n",
    "                    )\n",
    "\n",
    "                    out_tile[valid] = 0.0  # default: valid-but-not-selected = 0\n",
    "                    m_all = valid & (labels != 0)\n",
    "                    if m_all.any():\n",
    "                        p_int = np.zeros(prob.shape, dtype=np.int32)\n",
    "                        p_int_valid = np.rint(prob[m_all] * SCALE).astype(np.int32)\n",
    "                        p_int[m_all] = p_int_valid\n",
    "\n",
    "                        present = np.unique(labels[m_all])\n",
    "                        present = [u for u in present if u != 0]  # all units allowed\n",
    "                        for u in present:\n",
    "                            u_m = m_all & (labels == u)\n",
    "                            if not u_m.any():\n",
    "                                continue\n",
    "                            t = thr_map.get(u, -1)\n",
    "                            if t < 0:\n",
    "                                continue  # quota 0 or no pixels -> stays 0\n",
    "                            gt_m = u_m & (p_int > t)\n",
    "                            out_tile[gt_m] = 1.0\n",
    "                            need = need_eq_left.get(u, 0)\n",
    "                            if need > 0:\n",
    "                                eq_m = u_m & (p_int == t) & (out_tile != 1.0)\n",
    "                                if eq_m.any():\n",
    "                                    idx = np.flatnonzero(eq_m.ravel())\n",
    "                                    _rng.shuffle(idx)\n",
    "                                    take = min(need, idx.size)\n",
    "                                    sel = idx[:take]\n",
    "                                    rr, cc = np.unravel_index(sel, eq_m.shape)\n",
    "                                    out_tile[rr, cc] = 1.0\n",
    "                                    need_eq_left[u] = need - int(take)\n",
    "\n",
    "                    dst.write(out_tile, 1, window=w)\n",
    "\n",
    "            out_name = f\"{country}_AEI_binary_0_1.tif\"\n",
    "            upload_path(drive, out_local, binary_id, title=out_name)\n",
    "            try: os.remove(out_local)\n",
    "            except: pass\n",
    "\n",
    "            # CSV summary per admin (includes AEI==0 units)\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode=\"w\", newline=\"\") as tmpcsv:\n",
    "                wcsv = csv.writer(tmpcsv)\n",
    "                wcsv.writerow([\"unit_code\",\"aei_ha\",\"target_pixels\",\"thr_bin\",\"scale\",\"selected_pixels\"])\n",
    "                for a in attrs:\n",
    "                    u = a[\"unit_code\"]\n",
    "                    aei_ha = a[\"aei_ha\"]\n",
    "                    K = int(math.floor((aei_ha * 10_000.0) / PIXEL_AREA_M2))\n",
    "                    th = int(thr_map.get(u, -1))\n",
    "                    gt = sum(v for b, v in (hists[u].items() if u in hists else []) if b > th)\n",
    "                    ties_taken = (need_eq_map.get(u, 0) - need_eq_left.get(u, 0))\n",
    "                    sel = int(gt + max(0, ties_taken))\n",
    "                    wcsv.writerow([u, aei_ha, int(K), th, SCALE, sel])\n",
    "                csv_path = tmpcsv.name\n",
    "            upload_path(drive, csv_path, binary_id, title=f\"{country}_AEI_admin_summary.csv\")\n",
    "            try: os.remove(csv_path)\n",
    "            except: pass\n",
    "\n",
    "            print(f\"   ✓ Wrote Binary/{out_name}\")\n",
    "\n",
    "        try: os.remove(rtmp)\n",
    "        except: pass\n",
    "\n",
    "    print(\"\\n✅ Done (per-admin from shapefile; AEI in hectares; NaNs preserved).\")\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\")\n",
    "    aei_binarize_per_admin_from_shapefile(drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d33058-d624-42d1-a688-39ae50d70a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76772a-ce71-428f-a27b-2d2ada7ad0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b965b1-daed-4f2b-8a4e-7f60e83c5512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 binary rasters in Binary/\n",
      "\n",
      "=== Smoothing Nigeria_AEI_binary_0_1.tif → Nigeria_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ_West_Africa/Nigeria_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "✅ Done smoothing all AEI binary maps (majority filter).\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Post-processing for AEI per-admin binary masks.\n",
    "\n",
    "Step 2 after running `aei_binarize_per_admin_from_shapefile`:\n",
    "\n",
    "  - Take each *_AEI_binary_0_1.tif in\n",
    "        ROOT / CountryModelPredicted_Cropland / Probability / Binary\n",
    "  - Apply a window-based majority filter (on the 0/1 binary mask)\n",
    "    in a streaming / tile-based fashion.\n",
    "  - Preserve NaNs from the input.\n",
    "  - Write smoothed 0/1 binary TIFFs to:\n",
    "        ROOT / CountryModelPredicted_Cropland / Probability / Binary_MAJ\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- This does NOT change the original AEI thresholding step; it only smooths\n",
    "  the resulting binary maps, so exact per-admin AEI may change slightly.\n",
    "- Requires SciPy for `scipy.ndimage.uniform_filter`.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.ndimage import uniform_filter   # pip install scipy\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "ROOT_FOLDER_ID      = os.environ.get(\"ROOT_FOLDER_ID\", \"1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\")\n",
    "PARENT_FOLDER_NAME  = \"CountryModelPredicted\"\n",
    "PROB_SUBFOLDER_NAME = \"Probability\"   # fuzzy-matched\n",
    "BINARY_FOLDER_NAME  = \"Binary\"        # input binaries from step 1\n",
    "SMOOTH_FOLDER_NAME  = \"Binary_MAJ_West_Africa\"    # output smoothed binaries\n",
    "\n",
    "TILE                = 1024            # I/O tile size\n",
    "KERNEL_PX           = 9               # majority window (odd, in pixels, e.g. 9 ~ 270 m at 30 m res)\n",
    "MAJ_THR             = 0.50            # majority threshold (>= 50% neighbors == 1)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid:\n",
    "        return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        tkey = re.sub(r\"\\s+\", \"\", f.get(\"title\", \"\").lower())\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    if res:\n",
    "        return res[0][\"id\"]\n",
    "    nf = drive.CreateFile({\"title\": name, \"parents\":[{\"id\": parent_id}], \"mimeType\":\"application/vnd.google-apps.folder\"})\n",
    "    nf.Upload()\n",
    "    return nf[\"id\"]\n",
    "\n",
    "def download_to_temp(drive_file, suffix):\n",
    "    p = tempfile.NamedTemporaryFile(delete=False, suffix=suffix).name\n",
    "    drive_file.GetContentFile(p)\n",
    "    return p\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\":[{\"id\": parent_id}]})\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "# ---------------------- Raster helpers -------------------\n",
    "def _iter_tiles(H, W, tile=TILE):\n",
    "    for r0 in range(0, H, tile):\n",
    "        for c0 in range(0, W, tile):\n",
    "            h = min(tile, H - r0)\n",
    "            w = min(tile, W - c0)\n",
    "            yield Window(c0, r0, w, h)\n",
    "\n",
    "# ---------------------- Majority smoothing ---------------\n",
    "\n",
    "def _smooth_binary_stream(src_path, dst_path, kernel_px=KERNEL_PX, maj_thr=MAJ_THR):\n",
    "    \"\"\"\n",
    "    Read a float32 0/1/NaN binary raster in tiles and write a smoothed\n",
    "    0/1/NaN raster using a window-based majority filter.\n",
    "\n",
    "    - Input: float32, nodata = NaN, values 0 or 1 elsewhere.\n",
    "    - Output: float32, nodata = NaN, values 0 or 1 elsewhere.\n",
    "    \"\"\"\n",
    "    r = kernel_px // 2\n",
    "    win_area = float(kernel_px * kernel_px)\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        profile = src.profile.copy()\n",
    "        # keep same geo/tiling; ensure float32 + NaN nodata\n",
    "        profile.update(\n",
    "            dtype=\"float32\",\n",
    "            count=1,\n",
    "            nodata=np.nan,\n",
    "            compress=\"LZW\",\n",
    "            tiled=True,\n",
    "            blockxsize=512,\n",
    "            blockysize=512,\n",
    "            BIGTIFF=\"IF_NEEDED\"\n",
    "        )\n",
    "\n",
    "        with rasterio.open(dst_path, \"w\", **profile) as dst:\n",
    "            for _, w in src.block_windows(1):\n",
    "                # expand window by halo for neighborhood support\n",
    "                r0 = max(0, w.row_off - r)\n",
    "                c0 = max(0, w.col_off - r)\n",
    "                r1 = min(src.height, w.row_off + w.height + r)\n",
    "                c1 = min(src.width,  w.col_off + w.width  + r)\n",
    "                if (c1 - c0) <= 0 or (r1 - r0) <= 0:\n",
    "                    # nothing there\n",
    "                    dst.write(np.full((w.height, w.width), np.nan, dtype=np.float32), 1, window=w)\n",
    "                    continue\n",
    "                win_pad = Window(c0, r0, c1 - c0, r1 - r0)\n",
    "\n",
    "                a = src.read(1, window=win_pad, masked=True)\n",
    "                data = a.data\n",
    "                # valid where not masked AND finite\n",
    "                valid = (~a.mask) & np.isfinite(data)\n",
    "                if not valid.any():\n",
    "                    dst.write(np.full((w.height, w.width), np.nan, dtype=np.float32), 1, window=w)\n",
    "                    continue\n",
    "\n",
    "                # current binary: anything > 0.5 treated as 1\n",
    "                base = valid & (data > 0.5)\n",
    "                base_f = base.astype(np.float32)\n",
    "                valf   = valid.astype(np.float32)\n",
    "\n",
    "                # neighborhood counts via uniform_filter\n",
    "                sum_ones  = uniform_filter(base_f, size=kernel_px, mode=\"constant\", cval=0.0) * win_area\n",
    "                cnt_valid = uniform_filter(valf,   size=kernel_px, mode=\"constant\", cval=0.0) * win_area\n",
    "                frac = np.divide(sum_ones, cnt_valid, out=np.zeros_like(sum_ones), where=(cnt_valid > 0))\n",
    "\n",
    "                # majority decision (only where we have valid neighbors)\n",
    "                smoothed = np.zeros_like(base, dtype=bool)\n",
    "                has_nb = (cnt_valid > 0)\n",
    "                smoothed[has_nb] = frac[has_nb] >= maj_thr\n",
    "\n",
    "                # crop back to original tile window\n",
    "                rs = w.row_off - r0\n",
    "                cs = w.col_off - c0\n",
    "                re = rs + w.height\n",
    "                ce = cs + w.width\n",
    "\n",
    "                valid_core    = valid[rs:re, cs:ce]\n",
    "                smooth_core   = smoothed[rs:re, cs:ce]\n",
    "\n",
    "                out_block = np.full((w.height, w.width), np.nan, dtype=np.float32)\n",
    "                out_block[valid_core & smooth_core]  = 1.0\n",
    "                out_block[valid_core & ~smooth_core] = 0.0\n",
    "\n",
    "                dst.write(out_block, 1, window=w)\n",
    "\n",
    "# ---------------------- Main driver ----------------------\n",
    "\n",
    "def smooth_aei_binary_maps(drive):\n",
    "    \"\"\"\n",
    "    Entry point: locate Binary folder, smooth each AEI binary map,\n",
    "    and write to Binary_MAJ sibling folder.\n",
    "    \"\"\"\n",
    "    # Locate folders\n",
    "    cmp_id = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id:\n",
    "        raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "\n",
    "    prob_id = get_subfolder_fuzzy(drive, cmp_id, PROB_SUBFOLDER_NAME)\n",
    "    if not prob_id:\n",
    "        raise RuntimeError(\"Probability folder not found (tried fuzzy match).\")\n",
    "\n",
    "    binary_id = get_subfolder_fuzzy(drive, prob_id, BINARY_FOLDER_NAME)\n",
    "    if not binary_id:\n",
    "        raise RuntimeError(\"Binary folder (with AEI binaries) not found under Probability.\")\n",
    "\n",
    "    smooth_id = get_or_create_folder(drive, prob_id, SMOOTH_FOLDER_NAME)\n",
    "\n",
    "    # List binary TIFFs\n",
    "    files = [\n",
    "        it for it in list_files(drive, binary_id)\n",
    "        if isinstance(it, dict)\n",
    "        and it.get(\"mimeType\") != \"application/vnd.google-apps.folder\"\n",
    "        and it.get(\"title\", \"\").lower().endswith((\".tif\", \".tiff\"))\n",
    "    ]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No binary TIFFs in Binary folder.\")\n",
    "\n",
    "    print(f\"Found {len(files)} binary rasters in Binary/\")\n",
    "\n",
    "    for it in files:\n",
    "        title = it.get(\"title\", \"\")\n",
    "        base  = re.sub(r\"\\.tif(f)?$\", \"\", title, flags=re.IGNORECASE)\n",
    "        out_name = f\"{base}_MAJ_k{KERNEL_PX}_p{int(MAJ_THR*100)}.tif\"\n",
    "\n",
    "        print(f\"\\n=== Smoothing {title} → {out_name} ===\")\n",
    "        tmp_in  = download_to_temp(it, \".tif\")\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\") as tmp_out:\n",
    "            out_local = tmp_out.name\n",
    "\n",
    "        _smooth_binary_stream(tmp_in, out_local, kernel_px=KERNEL_PX, maj_thr=MAJ_THR)\n",
    "        upload_path(drive, out_local, smooth_id, title=out_name)\n",
    "\n",
    "        # cleanup\n",
    "        try:\n",
    "            os.remove(tmp_in)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(out_local)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(f\"   ✓ Wrote {SMOOTH_FOLDER_NAME}/{out_name}\")\n",
    "\n",
    "    print(\"\\n✅ Done smoothing all AEI binary maps (majority filter).\")\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\")\n",
    "    smooth_aei_binary_maps(drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c2262eb-82ad-46be-8200-c9a5d40b159c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1380171659.py, line 20)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m------------------------- CONFIG -------------------------\u001b[39m\n                                                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Merge all per-country AEI binary rasters in Drive → CountryModelPredicted/Probability/Binary/\n",
    "into a single global binary mosaic (union = max), preserving NaNs.\n",
    "\n",
    "Output: Merged_AEI_binary_0_1.tif in the same Binary folder.\n",
    "\"\"\"\n",
    "\n",
    "import os, re, tempfile, warnings\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.merge import merge as rio_merge\n",
    "from rasterio.enums import Resampling\n",
    "ROOT_FOLDER_ID      = os.environ.get(\"ROOT_FOLDER_ID\", \"11ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\")\n",
    "# PARENT_FOLDER_NAME  = \"CountryModelPredicted\"\n",
    "# PROB_SUBFOLDER_NAME = \"Probability\"   # fuzzy-matched\n",
    "# BINARY_FOLDER_NAME  = \"Binary\"        # input binaries from step 1\n",
    "# SMOOTH_FOLDER_NAME  = \"Binary_MAJ_West_Africa\n",
    "------------------------- CONFIG -------------------------\n",
    "ROOT_FOLDER_ID      = os.environ.get(\"ROOT_FOLDER_ID\", \"161LpFY8dIGXWrKO6dVNDVQaHDbugYvJA\")\n",
    "PARENT_FOLDER_NAME  = \"CountryModelPredicted\"\n",
    "PROB_SUBFOLDER_NAME = \"Probability\"   # fuzzy-matched (also matches misspellings)\n",
    "BINARY_SUBFOLDER    = \"Binary_MAJ_West_Africa\"\n",
    "OUT_NAME            = \"West_Africa_binary.tif\"\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid: return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        tkey = re.sub(r\"\\s+\", \"\", f.get(\"title\",\"\").lower())\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    if res: return res[0][\"id\"]\n",
    "    nf = drive.CreateFile({\"title\": name, \"parents\":[{\"id\": parent_id}], \"mimeType\":\"application/vnd.google-apps.folder\"})\n",
    "    nf.Upload()\n",
    "    return nf[\"id\"]\n",
    "\n",
    "def download_to_temp(drive_file, suffix):\n",
    "    p = tempfile.NamedTemporaryFile(delete=False, suffix=suffix).name\n",
    "    drive_file.GetContentFile(p)\n",
    "    return p\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\":[{\"id\": parent_id}]})\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "# ---------------------- Merge helpers ---------------------\n",
    "def _try_merge_with_max(srcs):\n",
    "    \"\"\"Preferred path (newer rasterio): method='max', nodata=np.nan.\"\"\"\n",
    "    return rio_merge(\n",
    "        srcs,\n",
    "        nodata=np.nan,\n",
    "        dtype=\"float32\",\n",
    "        precision=7,\n",
    "        resampling=Resampling.nearest,\n",
    "        method=\"max\",\n",
    "    )\n",
    "\n",
    "def _try_merge_basic(srcs):\n",
    "    \"\"\"Older rasterio: no 'method'. We'll nanmax ourselves after merging.\"\"\"\n",
    "    mosaic, out_transform = rio_merge(\n",
    "        srcs,\n",
    "        nodata=np.nan,\n",
    "        dtype=\"float32\",\n",
    "        precision=7,\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "    # emulate union across sources (elementwise maximum, ignoring NaNs)\n",
    "    mosaic = np.nanmax(mosaic, axis=0, keepdims=True).astype(\"float32\")\n",
    "    return mosaic, out_transform\n",
    "\n",
    "def _try_merge_basic_sentinel(srcs, sentinel=-9999.0):\n",
    "    \"\"\"Very old rasterio: nodata cannot be NaN. Use sentinel then convert and nanmax.\"\"\"\n",
    "    mosaic, out_transform = rio_merge(\n",
    "        srcs,\n",
    "        nodata=sentinel,\n",
    "        dtype=\"float32\",\n",
    "        precision=7,\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "    # convert sentinel to NaN\n",
    "    mosaic = mosaic.astype(\"float32\", copy=False)\n",
    "    mosaic[mosaic == sentinel] = np.nan\n",
    "    mosaic = np.nanmax(mosaic, axis=0, keepdims=True).astype(\"float32\")\n",
    "    return mosaic, out_transform\n",
    "\n",
    "# ---------------------- Main merge ------------------------\n",
    "def merge_all_binary_rasters(drive):\n",
    "    # Locate folders\n",
    "    cmp_id     = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id: raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "    prob_id    = get_subfolder_fuzzy(drive, cmp_id, PROB_SUBFOLDER_NAME)\n",
    "    if not prob_id: raise RuntimeError(\"Probability folder not found (tried fuzzy match).\")\n",
    "    binary_id  = get_subfolder_fuzzy(drive, prob_id, BINARY_SUBFOLDER)\n",
    "    if not binary_id:\n",
    "        binary_id = get_or_create_folder(drive, prob_id, BINARY_SUBFOLDER)\n",
    "\n",
    "    # Find all binary GeoTIFFs (skip the mosaic itself if re-running)\n",
    "    tifs = [it for it in list_files(drive, binary_id)\n",
    "            if isinstance(it, dict)\n",
    "            and it.get(\"mimeType\") != \"application/vnd.google-apps.folder\"\n",
    "            and it.get(\"title\",\"\").lower().endswith((\".tif\",\".tiff\"))\n",
    "            and OUT_NAME.lower() not in it.get(\"title\",\"\").lower()]\n",
    "    if not tifs:\n",
    "        raise FileNotFoundError(\"No binary .tif files found in Binary/.\")\n",
    "\n",
    "    # Download & open datasets\n",
    "    local_paths, srcs = [], []\n",
    "    try:\n",
    "        for it in tifs:\n",
    "            p = download_to_temp(it, \".tif\")\n",
    "            local_paths.append(p)\n",
    "            srcs.append(rasterio.open(p))\n",
    "\n",
    "        # Try modern merge with 'method=max' → else fallback strategies\n",
    "        try:\n",
    "            mosaic, out_transform = _try_merge_with_max(srcs)\n",
    "        except TypeError:\n",
    "            # 'method' not supported\n",
    "            try:\n",
    "                mosaic, out_transform = _try_merge_basic(srcs)\n",
    "            except Exception:\n",
    "                mosaic, out_transform = _try_merge_basic_sentinel(srcs)\n",
    "        except Exception:\n",
    "            # Any other unexpected error → robust fallback\n",
    "            try:\n",
    "                mosaic, out_transform = _try_merge_basic(srcs)\n",
    "            except Exception:\n",
    "                mosaic, out_transform = _try_merge_basic_sentinel(srcs)\n",
    "\n",
    "        # Build output profile from first raster\n",
    "        ref = srcs[0]\n",
    "        out_profile = ref.profile.copy()\n",
    "        out_profile.update(\n",
    "            driver=\"GTiff\",\n",
    "            height=mosaic.shape[1],\n",
    "            width=mosaic.shape[2],\n",
    "            transform=out_transform,\n",
    "            count=1,\n",
    "            dtype=\"float32\",\n",
    "            nodata=np.nan,\n",
    "            compress=\"LZW\",\n",
    "            tiled=True,\n",
    "            blockxsize=512,\n",
    "            blockysize=512,\n",
    "            BIGTIFF=\"IF_NEEDED\"\n",
    "        )\n",
    "\n",
    "        # Write to temp, then upload\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\") as tmp_out:\n",
    "            out_local = tmp_out.name\n",
    "\n",
    "        with rasterio.open(out_local, \"w\", **out_profile) as dst:\n",
    "            dst.write(mosaic[0], 1)\n",
    "\n",
    "        upload_path(drive, out_local, binary_id, title=OUT_NAME)\n",
    "        try: os.remove(out_local)\n",
    "        except: pass\n",
    "\n",
    "        print(f\"✅ Merged {len(srcs)} rasters → Binary/{OUT_NAME}\")\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        for s in srcs:\n",
    "            try: s.close()\n",
    "            except: pass\n",
    "        for p in local_paths:\n",
    "            try: os.remove(p)\n",
    "            except: pass\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\")\n",
    "    merge_all_binary_rasters(drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5a97a-8f2e-4af1-ab65-1963af2b13a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462af3f8-cfce-4de0-b2b3-26b011f224e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Plot US_binary.tif with:\n",
    "  - Top panel: country-wide map (ADM0 boundary + irrigated in dark blue).\n",
    "  - Bottom panel: zoomed inset, clipping US_binary to US_small shapefile.\n",
    "\n",
    "Files (Google Drive layout):\n",
    "  CountryModelPredicted_Cropland /\n",
    "    Probability|Probablity|Prob /\n",
    "      Merged /\n",
    "        US_binary.tif\n",
    "        US_small.(shp,shx,dbf,prj,cpg)\n",
    "        Comparisons /\n",
    "          US.(shp,shx,dbf,prj,cpg)\n",
    "          --> output PNG will be saved here as:\n",
    "              US_binary_with_zoom_map.png\n",
    "\n",
    "Requirements:\n",
    "  - PyDrive2-authenticated global `drive`\n",
    "  - rasterio, numpy, matplotlib, fiona, shapely, pyproj\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.mask import mask\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "from shapely.ops import transform as shp_transform\n",
    "from shapely.ops import unary_union\n",
    "from pyproj import Transformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "ROOT_FOLDER_ID      = os.environ.get(\"ROOT_FOLDER_ID\", \"18pQKnMMnLramhHRZSNwUJrLqG5DXNMmS\")\n",
    "PARENT_FOLDER_NAME  = \"CountryModelPredicted\"\n",
    "PROB_NAMES          = (\"Probability\", \"Probablity\", \"Prob\")\n",
    "BINARY_SUBFOLDER    = \"Merged\"\n",
    "COMPARE_SUBFOLDER   = \"Comparisons\"\n",
    "\n",
    "CHINA_BINARY_NAME   = \"India_binary.tif\"\n",
    "SMALL_BASE          = \"India_small\"\n",
    "CHN_ADM0_BASE       = \"gadm41_IND_0\"\n",
    "\n",
    "\n",
    "PLOT_DPI            = 160\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(\n",
    "        drive,\n",
    "        f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\"\n",
    "    )\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(\n",
    "        drive,\n",
    "        f\"'{parent_id}' in parents and trashed=false \"\n",
    "        f\"and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    )\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    \"\"\"Simple fuzzy finder for folders (handles 'Probability' typo).\"\"\"\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid:\n",
    "        return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        t = (f.get(\"title\") or f.get(\"name\") or \"\").lower()\n",
    "        tkey = re.sub(r\"\\s+\", \"\", t)\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def download_to_temp(drive_file, local_path):\n",
    "    drive_file.GetContentFile(local_path)\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile(\n",
    "        {\"title\": title or os.path.basename(local_path),\n",
    "         \"parents\":[{\"id\": parent_id}]}\n",
    "    )\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "def download_shapefile_bundle(drive, folder_id, base):\n",
    "    \"\"\"\n",
    "    Download base.(shp,shx,dbf,prj,cpg) from a folder into a temp dir.\n",
    "    Returns path to the local .shp.\n",
    "    \"\"\"\n",
    "    exts = [\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\"]\n",
    "    items = {}\n",
    "    for it in list_files(drive, folder_id):\n",
    "        title = (it.get(\"title\") or \"\").lower()\n",
    "        for e in exts:\n",
    "            if title == (base.lower() + e):\n",
    "                items[e] = it\n",
    "\n",
    "    if \".shp\" not in items:\n",
    "        raise FileNotFoundError(f\"Could not find {base}.shp in folder.\")\n",
    "\n",
    "    tmpdir = tempfile.mkdtemp(prefix=base + \"_\")\n",
    "    for e, it in items.items():\n",
    "        lp = os.path.join(tmpdir, base + e)\n",
    "        download_to_temp(it, lp)\n",
    "    return os.path.join(tmpdir, base + \".shp\")\n",
    "\n",
    "\n",
    "# --------------------- Main plotting logic --------------------\n",
    "def plot_china_binary_with_zoom(drive):\n",
    "    # 1) Navigate to folders\n",
    "    root_id = ROOT_FOLDER_ID\n",
    "    cmp_id = get_subfolder_fuzzy(drive, root_id, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id:\n",
    "        raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "\n",
    "    prob_id = None\n",
    "    for nm in PROB_NAMES:\n",
    "        prob_id = get_subfolder_fuzzy(drive, cmp_id, nm)\n",
    "        if prob_id:\n",
    "            break\n",
    "    if not prob_id:\n",
    "        raise RuntimeError(\"Probability/Probablity/Prob folder not found.\")\n",
    "\n",
    "    binary_id = get_subfolder_fuzzy(drive, prob_id, BINARY_SUBFOLDER)\n",
    "    if not binary_id:\n",
    "        raise RuntimeError(f\"Binary folder '{BINARY_SUBFOLDER}' not found.\")\n",
    "\n",
    "    comp_id = get_subfolder_fuzzy(drive, binary_id, COMPARE_SUBFOLDER)\n",
    "    if not comp_id:\n",
    "        raise RuntimeError(f\"Comparisons folder '{COMPARE_SUBFOLDER}' not found inside {BINARY_SUBFOLDER}/.\")\n",
    "\n",
    "    # 2) Find US_binary.tif and small-area shapefile in Merged\n",
    "    china_file = None\n",
    "    small_shp_present = False\n",
    "    for it in list_files(drive, binary_id):\n",
    "        title = (it.get(\"title\") or \"\").strip().lower()\n",
    "        if title == CHINA_BINARY_NAME.lower():\n",
    "            china_file = it\n",
    "        if title.startswith(SMALL_BASE.lower()) and title.endswith(\".shp\"):\n",
    "            small_shp_present = True\n",
    "    if not china_file:\n",
    "        raise FileNotFoundError(f\"{CHINA_BINARY_NAME} not found in Merged/.\")\n",
    "    if not small_shp_present:\n",
    "        raise FileNotFoundError(f\"{SMALL_BASE}.shp not found in Merged/.\")\n",
    "\n",
    "    # 3) Download US_binary.tif and shapefiles\n",
    "    china_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "    download_to_temp(china_file, china_tmp)\n",
    "\n",
    "    shp_small_path = download_shapefile_bundle(drive, binary_id, SMALL_BASE)\n",
    "    shp_chn_path   = download_shapefile_bundle(drive, comp_id, CHN_ADM0_BASE)\n",
    "\n",
    "    try:\n",
    "        # === Read US_binary with downsampling for the main map ===\n",
    "        with rasterio.open(china_tmp) as src:\n",
    "            H, W = src.height, src.width\n",
    "            bounds = src.bounds\n",
    "            raster_crs = src.crs\n",
    "            nodata = src.nodata\n",
    "\n",
    "            # target ~10M pixels max for the main map\n",
    "            max_pixels = 10_000_000\n",
    "            scale_factor = max(1, int(np.sqrt((H * W) / max_pixels)))\n",
    "            out_h = max(1, H // scale_factor)\n",
    "            out_w = max(1, W // scale_factor)\n",
    "\n",
    "            china_arr = src.read(\n",
    "                1,\n",
    "                out_shape=(out_h, out_w),\n",
    "                resampling=Resampling.nearest,\n",
    "                masked=True\n",
    "            )\n",
    "\n",
    "            # Extent for imshow\n",
    "            extent_main = (bounds.left, bounds.right, bounds.bottom, bounds.top)\n",
    "\n",
    "        # === Read ADM0 boundary and reproject to raster CRS ===\n",
    "        chn_geoms = []\n",
    "        with fiona.open(shp_chn_path) as src:\n",
    "            shp_crs = src.crs\n",
    "            if shp_crs and raster_crs and shp_crs != raster_crs:\n",
    "                transformer = Transformer.from_crs(shp_crs, raster_crs, always_xy=True)\n",
    "                project = lambda x, y, z=None: transformer.transform(x, y)\n",
    "            else:\n",
    "                project = None\n",
    "\n",
    "            for feat in src:\n",
    "                g = shape(feat[\"geometry\"])\n",
    "                if project is not None:\n",
    "                    g = shp_transform(project, g)\n",
    "                chn_geoms.append(g)\n",
    "\n",
    "        # === Read small-area shapefile and reproject to raster CRS ===\n",
    "        small_geoms = []\n",
    "        with fiona.open(shp_small_path) as src:\n",
    "            shp_crs = src.crs\n",
    "            if shp_crs and raster_crs and shp_crs != raster_crs:\n",
    "                transformer = Transformer.from_crs(shp_crs, raster_crs, always_xy=True)\n",
    "                project = lambda x, y, z=None: transformer.transform(x, y)\n",
    "            else:\n",
    "                project = None\n",
    "\n",
    "            for feat in src:\n",
    "                g = shape(feat[\"geometry\"])\n",
    "                if project is not None:\n",
    "                    g = shp_transform(project, g)\n",
    "                small_geoms.append(g)\n",
    "\n",
    "        small_union = unary_union(small_geoms)\n",
    "\n",
    "        # === Crop US_binary to the small area for inset ===\n",
    "        with rasterio.open(china_tmp) as src:\n",
    "            inset_arr, inset_transform = mask(\n",
    "                src,\n",
    "                [small_union],\n",
    "                crop=True,\n",
    "                nodata=src.nodata\n",
    "            )\n",
    "            inset_arr = inset_arr[0]   # single band\n",
    "            inset_bounds = rasterio.transform.array_bounds(\n",
    "                inset_arr.shape[0], inset_arr.shape[1], inset_transform\n",
    "            )\n",
    "            # array_bounds returns (ymin, ymax, xmin, xmax)\n",
    "            inset_extent = (inset_bounds[2], inset_bounds[3],\n",
    "                            inset_bounds[0], inset_bounds[1])\n",
    "\n",
    "        # === Set up colormap: 0=grey (non-irrigated), 1=dark blue (irrigated), nodata transparent ===\n",
    "        cmap = ListedColormap([\n",
    "            (0.9, 0.9, 0.9, 1.0),  # 0 non-irrigated\n",
    "            (0.0, 0.0, 0.5, 1.0),  # 1 irrigated (dark blue)\n",
    "        ])\n",
    "        # for masked values (nodata)\n",
    "        cmap.set_bad((0, 0, 0, 0))\n",
    "\n",
    "        # === Build the figure ===\n",
    "        fig, (ax_main, ax_inset) = plt.subplots(\n",
    "            nrows=2, ncols=1,\n",
    "            figsize=(7.5, 9.0),\n",
    "            dpi=PLOT_DPI,\n",
    "            gridspec_kw={\"height_ratios\": [3, 2]}\n",
    "        )\n",
    "\n",
    "        # ----- TOP PANEL: country-wide map -----\n",
    "        ax_main.imshow(\n",
    "            china_arr,\n",
    "            origin=\"upper\",\n",
    "            cmap=cmap,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            extent=extent_main\n",
    "        )\n",
    "\n",
    "        # ONLY outer shapefile boundary in black (ADM0)\n",
    "        for g in chn_geoms:\n",
    "            if g.geom_type == \"MultiPolygon\":\n",
    "                polys = list(g.geoms)\n",
    "            else:\n",
    "                polys = [g]\n",
    "            for poly in polys:\n",
    "                x, y = poly.exterior.xy\n",
    "                ax_main.plot(x, y, color=\"black\", linewidth=0.7)\n",
    "\n",
    "        ax_main.set_xticks([])\n",
    "        ax_main.set_yticks([])\n",
    "        ax_main.set_xlabel(\"\")\n",
    "        ax_main.set_ylabel(\"\")\n",
    "        ax_main.set_title(f\"Irrigated area (dark blue) from {CHINA_BINARY_NAME}\", fontsize=13)\n",
    "\n",
    "        # NOTE: no bounding box rectangle\n",
    "        # NOTE: no legend\n",
    "\n",
    "        # ----- BOTTOM PANEL: zoomed inset -----\n",
    "        # Mask nodata for inset\n",
    "        inset_masked = np.ma.masked_equal(inset_arr, nodata) if nodata is not None else np.ma.masked_invalid(inset_arr)\n",
    "\n",
    "        ax_inset.imshow(\n",
    "            inset_masked,\n",
    "            origin=\"upper\",\n",
    "            cmap=cmap,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            extent=inset_extent\n",
    "        )\n",
    "\n",
    "        # ONLY outer shapefile boundary in black (US_small)\n",
    "        for g in small_geoms:\n",
    "            if g.geom_type == \"MultiPolygon\":\n",
    "                polys = list(g.geoms)\n",
    "            else:\n",
    "                polys = [g]\n",
    "            for poly in polys:\n",
    "                x, y = poly.exterior.xy\n",
    "                ax_inset.plot(x, y, color=\"black\", linewidth=0.7)\n",
    "\n",
    "        ax_inset.set_xticks([])\n",
    "        ax_inset.set_yticks([])\n",
    "        ax_inset.set_xlabel(\"\")\n",
    "        ax_inset.set_ylabel(\"\")\n",
    "        ax_inset.set_title(f\"Zoomed irrigated area ({SMALL_BASE}.shp, dark blue)\", fontsize=12)\n",
    "        ax_inset.set_aspect(\"equal\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ---- Save & upload PNG ----\n",
    "        out_png = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\").name\n",
    "        fig.savefig(out_png, bbox_inches=\"tight\", dpi=PLOT_DPI)\n",
    "        plt.close(fig)\n",
    "\n",
    "        out_title = \"US_binary_with_zoom_map.png\"\n",
    "        upload_path(drive, out_png, comp_id, title=out_title)\n",
    "\n",
    "        try:\n",
    "            os.remove(out_png)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(f\"✅ Saved country-wide + zoom map PNG as '{out_title}' in Merged/Comparisons/\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(china_tmp)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # shapefile tempdirs will be cleaned by OS eventually\n",
    "\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\n",
    "            \"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\"\n",
    "        )\n",
    "    plot_china_binary_with_zoom(drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfc5e26-dd97-4dcf-abb0-2dbeb12be279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: Continental Irr Maps  |  ID: 1atiKrbLV7Ifez9AKTuSqCFMsTxkOqOvc\n",
      "Folder: Moldova_VIs_Env_GrowingSeason  |  ID: 183w10O5hg6pS6ou2gT7LE1N4SI1pSn5B\n",
      "Folder: Cuba_VIs_Env_IrrigationSeason  |  ID: 1OrL9zVhyRh57oLfvg84ySJhB0GZSCM6m\n",
      "Folder: Caribbean_VIs_Env_IrrigationSeason  |  ID: 1vsKprJVxyyjHR6zPcfYV1Rc1wkjIw2fn\n",
      "Folder: GMIE  |  ID: 1lVkH_OShkP5n_9CI4bdJgwFuHwmbx57H\n",
      "Folder: CLE_National  |  ID: 1pOpf-Zy5la4SKTIcrmLX6tGlxVBejKYX\n",
      "Folder: AfricaEnv_WA_noPrecip_NoSoil_NoCropMask  |  ID: 1KGWrOMmhnCVvUFr7uh9y2wyjPdLkRFDd\n",
      "Folder: Europe_VIs_Env2  |  ID: 1Rz21QLD1UnspShqUD7jB6Bfsgjk03Z9e\n",
      "Folder: China  |  ID: 19Ee1kUar4oEabBObq7GcA-jDR7F29x2_\n",
      "Folder: Canada  |  ID: 1qXhq3-3-J2YhKjXmMcCOT0exvb12LxhD\n",
      "Folder: Global_Irr_Maps  |  ID: 1XKYX2EPkPmu1el4Q0FpfZMEkgsWvRuYD\n",
      "Folder: Validation Dataset  |  ID: 1yHuAWZZdz5KsWxvEXPw4nxiRcKgYPtp8\n",
      "Folder: SouthAmerica_VIs_Env  |  ID: 1hitOuuj27P1_Lleo4Dhl630Q-TCcS_b6\n",
      "Folder: Canada_Irrigation_RF  |  ID: 1JUJ8xSgSiuccxHd80eiVYfLgwjZcPS5b\n",
      "Folder: US_VIs_Env  |  ID: 12LaYV-UY6G9lMeKMKk7y-RXFsAoClXIh\n",
      "Folder: China_VIs_Env_bySeason-2024  |  ID: 1Ed9_qQ-qYFN-weL1Tb_2B8mFnCXdtJsG\n",
      "Folder: Chile  |  ID: 1jExB0R7pXMH86NdZG_83qAylZqRpi931\n",
      "Folder: China_VIs_Env_bySeason_new  |  ID: 17a8xF2qvnHjw_Neu8IPtP_-VjOtbvIkd\n",
      "Folder: Oceania_VIs_Env_new_2  |  ID: 1vKJ5o-i8V2K3TUswyTtrBvnMGkhURxiW\n",
      "Folder: Global Water Scarcity  |  ID: 1VQiSgaEykgtdP1GUBQ7rwRvaTa_OICxx\n",
      "Folder: John MIRCA-OS 2020  |  ID: 1b13mul-bIexA3uA8ln6QpgWHVxp76FB2\n",
      "Folder: Big Idea Challenge  |  ID: 1iq2UYDu5YZR8BsU9scxEAM9V-lx4wAdv\n",
      "Folder: Presentation Figures (Kyle)  |  ID: 1t3qWNZyWmgXl4fgvaJIeKQU76DawkAHg\n",
      "Folder: Alaska_VIs_Env  |  ID: 1sAJCK-sp5cf12iNVinjup0Mg0pznoFo4\n",
      "Folder: GMIE_GTP_Exports  |  ID: 1NGHXKFhhzGegRMJyL5V9bs7bWIlaCZjX\n",
      "Folder: Europe_VIs_Env  |  ID: 161LpFY8dIGXWrKO6dVNDVQaHDbugYvJA\n",
      "Folder: LANID_GTPS  |  ID: 1gB8pFxtUKkPguTabZ-Tu4s0WPXn75uLU\n",
      "Folder: LANID_GTPS  |  ID: 11h1oxJyob8Bk1idYODFDSY63Z5D-I65t\n",
      "Folder: NorthAmerica_GTPs  |  ID: 1yF93-d9qWa_gsJbHKiW3PYXLGBjQ2R1d\n",
      "Folder: SouthAmerica_GTPs  |  ID: 1W0HNTYGgvwS15Macx3o3Fck8-Zolcp8H\n",
      "Folder: SouthAmerica_GTPs  |  ID: 1rWEjBsC6IliCKtyg3ZTAi8sF_Mga1-O3\n",
      "Folder: Asia_GTP_Exports  |  ID: 1Mb4lCc_SPY6k1E6RUQkQK00aZI-W7ZcU\n",
      "Folder: Asia_GTP_Exports  |  ID: 1lGpJAaJKf4O83Go3wybVc7qxP9OUFjNX\n",
      "Folder: East_Africa_VIs  |  ID: 1nHooOVyzdpwKK4S7VkYqGYJ7HemV8fR5\n",
      "Folder: Continental_LST  |  ID: 1PXneA1vLweiWq3P27JgWtAu_Bicsn63n\n",
      "Folder: Model Training  |  ID: 1riXbA8PYImJ_IAPLMt-1Mp5OxwvhYq5t\n",
      "Folder: Australia_GTP_Exports  |  ID: 1-pZ78HTRKA0xXHNG543w93gy9RmoU1rK\n",
      "Folder: Oceania_GTP_Exports  |  ID: 1r8e3cf34uLFnttX1wLgLZLGAxGZNA08k\n",
      "Folder: River Network  |  ID: 1ZqVgSQzDUvCG8KHrLRjw_Y9eOSVFoKb6\n",
      "Folder: GEE  |  ID: 1pSu1knrs7PCP0y7GRYjpeY7Q9IBIw90s\n",
      "Folder: Asia_VIs_Env_fix  |  ID: 1RidDG3V91GYJzddA01UXoiyQBO7V3tBw\n",
      "Folder: AEZ  |  ID: 1te3nKn8vyt2AECmk8NM_xBgpEFXp84RX\n",
      "Folder: US_VIs_Env_Final  |  ID: 1hqMIyDYEFKnpS8KLxC4bqHmF_9dHXImG\n",
      "Folder: Russia  |  ID: 1t3wGHApm7zc1IzDLDNUpHXjbBo7zAyx7\n",
      "Folder: NorthAmerica_VIs_Env  |  ID: 15EdJ3CkBPm-tsKBVVF3kdgajV2Skp0I3\n",
      "Folder: MIRCA2020  |  ID: 1eP7mxDR9D6hRyN133Y5Db-KOWrbAQF0k\n",
      "Folder: Asia_VIs_Env  |  ID: 18pQKnMMnLramhHRZSNwUJrLqG5DXNMmS\n",
      "Folder: Africa_VIs_Env  |  ID: 1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\n",
      "Folder: Oceania_VIs_Env  |  ID: 1gtVp-x-dIgcuNPcoaxHfmZs0ctoG71PS\n",
      "Folder: CLE  |  ID: 1BQqWjXTEfTjWcf_SICM4h3ICeX9jAI65\n",
      "Folder: Other files  |  ID: 1UF1SbMqszNi0vQSJaD6ExB5_dKq-A-bN\n",
      "Folder: Asia_GPTS_2023  |  ID: 18QKPr8XZIDNGqIdXncbatM8qL90BLBCa\n",
      "Folder: GEE_exports  |  ID: 1iKEC-xrOomn5eFw4CV5HzUNlq7y5-EDC\n",
      "Folder: Global High Res. Map  |  ID: 1wmu_kb5tcxIt6gIPQ6CLDixM-uSbZ5Hc\n",
      "Folder: MIRCA2015_Latest_Version  |  ID: 1fqtG_HEAvK_wKRTF4NuuGgXl8FNBnDvY\n",
      "Folder: MIRCA-OS_New_version  |  ID: 1mxYpSecaSVYuBgkw7FwbUh0nc9-RHqKD\n",
      "Folder: GPTs  |  ID: 1zSZ57xgJa3LfVMRDgz0oGXOxkFoQqrnA\n",
      "Folder: USA  |  ID: 1nmIDoF9d7eN3QvTLkwwTt-wJ7llTREEt\n",
      "Folder: New AEI CA  |  ID: 1PNFVJ803HVSlPbw2S1D7E5iLj_4H8_Du\n",
      "Folder: Progress Reports  |  ID: 1Fk0549LWrBgEKlFf9TzGPMwEaS5mcPfQ\n",
      "Folder: 2015 Water Scarcity Assessment   |  ID: 1kDqp_iSiDCW842h8Ic_dwG9-nV98bGfQ\n",
      "Folder: USA  |  ID: 1YGpsMpV7d0-PE_pElPBzYUMUA_cAgyYm\n",
      "Folder: Colab Notebooks  |  ID: 1SqDk51R_bKFSdcAqT-4zMo6bZTlpat4C\n",
      "Folder: GGE  |  ID: 1B-dRy_XfHBtZgjlP90v77bjjidWrLxNJ\n"
     ]
    }
   ],
   "source": [
    "# List only folders in Google Drive root\n",
    "folder_list = drive.ListFile({\n",
    "    'q': \"'root' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
    "}).GetList()\n",
    "\n",
    "for folder in folder_list:\n",
    "    print(f\"Folder: {folder['title']}  |  ID: {folder['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73fe1cb2-0d2b-4c12-bd44-2ae6c21e6c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved country-wide + zoom map PNG as 'US_binary_with_zoom_map.png' in Merged/Comparisons/\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Plot US_binary.tif with:\n",
    "  - Top panel: country-wide map (ADM0 boundary + irrigated in dark blue).\n",
    "  - Bottom panel: zoomed inset, clipping US_binary to US_small shapefile.\n",
    "\n",
    "Files (Google Drive layout):\n",
    "  CountryModelPredicted_Cropland /\n",
    "    Probability|Probablity|Prob /\n",
    "      Merged /\n",
    "        US_binary.tif\n",
    "        US_small.(shp,shx,dbf,prj,cpg)\n",
    "        Comparisons /\n",
    "          US.(shp,shx,dbf,prj,cpg)\n",
    "          --> output PNG will be saved here as:\n",
    "              US_binary_with_zoom_map.png\n",
    "\n",
    "Requirements:\n",
    "  - PyDrive2-authenticated global `drive`\n",
    "  - rasterio, numpy, matplotlib, fiona, shapely, pyproj\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.mask import mask\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "from shapely.ops import transform as shp_transform\n",
    "from shapely.ops import unary_union\n",
    "from pyproj import Transformer\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.mask import mask  # kept, though not used now\n",
    "import rasterio.windows as rwindows\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "from shapely.ops import transform as shp_transform\n",
    "from shapely.ops import unary_union\n",
    "from pyproj import Transformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "ROOT_FOLDER_ID      = os.environ.get(\"ROOT_FOLDER_ID\", \"1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\")\n",
    "PARENT_FOLDER_NAME  = \"CountryModelPredicted\"\n",
    "PROB_NAMES          = (\"Probability\", \"Probablity\", \"Prob\")\n",
    "BINARY_SUBFOLDER    = \"Merged\"\n",
    "COMPARE_SUBFOLDER   = \"Comparisons\"\n",
    "\n",
    "CHINA_BINARY_NAME   = \"Egypt_Binary.tif\"\n",
    "SMALL_BASE          = \"Egypt_smalls\"\n",
    "CHN_ADM0_BASE       = \"gadm41_EGY_0\"\n",
    "\n",
    "\n",
    "PLOT_DPI            = 160\n",
    "# ==================================================\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(\n",
    "        drive,\n",
    "        f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\"\n",
    "    )\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(\n",
    "        drive,\n",
    "        f\"'{parent_id}' in parents and trashed=false \"\n",
    "        f\"and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    )\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    \"\"\"Simple fuzzy finder for folders (handles 'Probability' typo).\"\"\"\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid:\n",
    "        return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        t = (f.get(\"title\") or f.get(\"name\") or \"\").lower()\n",
    "        tkey = re.sub(r\"\\s+\", \"\", t)\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def download_to_temp(drive_file, local_path):\n",
    "    drive_file.GetContentFile(local_path)\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile(\n",
    "        {\"title\": title or os.path.basename(local_path),\n",
    "         \"parents\":[{\"id\": parent_id}]}\n",
    "    )\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "def download_shapefile_bundle(drive, folder_id, base):\n",
    "    \"\"\"\n",
    "    Download base.(shp,shx,dbf,prj,cpg) from a folder into a temp dir.\n",
    "    Returns path to the local .shp.\n",
    "    \"\"\"\n",
    "    exts = [\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\"]\n",
    "    items = {}\n",
    "    for it in list_files(drive, folder_id):\n",
    "        title = (it.get(\"title\") or \"\").lower()\n",
    "        for e in exts:\n",
    "            if title == (base.lower() + e):\n",
    "                items[e] = it\n",
    "\n",
    "    if \".shp\" not in items:\n",
    "        raise FileNotFoundError(f\"Could not find {base}.shp in folder.\")\n",
    "\n",
    "    tmpdir = tempfile.mkdtemp(prefix=base + \"_\")\n",
    "    for e, it in items.items():\n",
    "        lp = os.path.join(tmpdir, base + e)\n",
    "        download_to_temp(it, lp)\n",
    "    return os.path.join(tmpdir, base + \".shp\")\n",
    "\n",
    "\n",
    "# --------------------- Main plotting logic --------------------\n",
    "def plot_china_binary_with_zoom(drive):\n",
    "    # 1) Navigate to folders\n",
    "    root_id = ROOT_FOLDER_ID\n",
    "    cmp_id = get_subfolder_fuzzy(drive, root_id, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id:\n",
    "        raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "\n",
    "    prob_id = None\n",
    "    for nm in PROB_NAMES:\n",
    "        prob_id = get_subfolder_fuzzy(drive, cmp_id, nm)\n",
    "        if prob_id:\n",
    "            break\n",
    "    if not prob_id:\n",
    "        raise RuntimeError(\"Probability/Probablity/Prob folder not found.\")\n",
    "\n",
    "    binary_id = get_subfolder_fuzzy(drive, prob_id, BINARY_SUBFOLDER)\n",
    "    if not binary_id:\n",
    "        raise RuntimeError(f\"Binary folder '{BINARY_SUBFOLDER}' not found.\")\n",
    "\n",
    "    comp_id = get_subfolder_fuzzy(drive, binary_id, COMPARE_SUBFOLDER)\n",
    "    if not comp_id:\n",
    "        raise RuntimeError(f\"Comparisons folder '{COMPARE_SUBFOLDER}' not found inside {BINARY_SUBFOLDER}/.\")\n",
    "\n",
    "    # 2) Find US_binary.tif and small-area shapefile in Merged\n",
    "    china_file = None\n",
    "    small_shp_present = False\n",
    "    for it in list_files(drive, binary_id):\n",
    "        title = (it.get(\"title\") or \"\").strip().lower()\n",
    "        if title == CHINA_BINARY_NAME.lower():\n",
    "            china_file = it\n",
    "        if title.startswith(SMALL_BASE.lower()) and title.endswith(\".shp\"):\n",
    "            small_shp_present = True\n",
    "    if not china_file:\n",
    "        raise FileNotFoundError(f\"{CHINA_BINARY_NAME} not found in Merged/.\")\n",
    "    if not small_shp_present:\n",
    "        raise FileNotFoundError(f\"{SMALL_BASE}.shp not found in Merged/.\")\n",
    "\n",
    "    # 3) Download US_binary.tif and shapefiles\n",
    "    china_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "    download_to_temp(china_file, china_tmp)\n",
    "\n",
    "    shp_small_path = download_shapefile_bundle(drive, binary_id, SMALL_BASE)\n",
    "    shp_chn_path   = download_shapefile_bundle(drive, comp_id, CHN_ADM0_BASE)\n",
    "\n",
    "    try:\n",
    "        # === Read US_binary with downsampling for the main map ===\n",
    "        with rasterio.open(china_tmp) as src:\n",
    "            H, W = src.height, src.width\n",
    "            bounds = src.bounds\n",
    "            raster_crs = src.crs\n",
    "            nodata = src.nodata\n",
    "\n",
    "            # target ~10M pixels max for the main map\n",
    "            max_pixels = 10_000_000\n",
    "            scale_factor = max(1, int(np.sqrt((H * W) / max_pixels)))\n",
    "            out_h = max(1, H // scale_factor)\n",
    "            out_w = max(1, W // scale_factor)\n",
    "\n",
    "            china_arr = src.read(\n",
    "                1,\n",
    "                out_shape=(out_h, out_w),\n",
    "                resampling=Resampling.nearest,\n",
    "                masked=True\n",
    "            )\n",
    "\n",
    "            # Extent for imshow\n",
    "            extent_main = (bounds.left, bounds.right, bounds.bottom, bounds.top)\n",
    "\n",
    "        # === Read ADM0 boundary and reproject to raster CRS ===\n",
    "        chn_geoms = []\n",
    "        with fiona.open(shp_chn_path) as src:\n",
    "            shp_crs = src.crs\n",
    "            if shp_crs and raster_crs and shp_crs != raster_crs:\n",
    "                transformer = Transformer.from_crs(shp_crs, raster_crs, always_xy=True)\n",
    "                project = lambda x, y, z=None: transformer.transform(x, y)\n",
    "            else:\n",
    "                project = None\n",
    "\n",
    "            for feat in src:\n",
    "                g = shape(feat[\"geometry\"])\n",
    "                if project is not None:\n",
    "                    g = shp_transform(project, g)\n",
    "                chn_geoms.append(g)\n",
    "\n",
    "        # === Read small-area shapefile and reproject to raster CRS ===\n",
    "        small_geoms = []\n",
    "        with fiona.open(shp_small_path) as src:\n",
    "            shp_crs = src.crs\n",
    "            if shp_crs and raster_crs and shp_crs != raster_crs:\n",
    "                transformer = Transformer.from_crs(shp_crs, raster_crs, always_xy=True)\n",
    "                project = lambda x, y, z=None: transformer.transform(x, y)\n",
    "            else:\n",
    "                project = None\n",
    "\n",
    "            for feat in src:\n",
    "                g = shape(feat[\"geometry\"])\n",
    "                if project is not None:\n",
    "                    g = shp_transform(project, g)\n",
    "                small_geoms.append(g)\n",
    "\n",
    "        small_union = unary_union(small_geoms)\n",
    "\n",
    "        # === Crop US_binary to the RECTANGULAR BBOX of the small area for inset ===\n",
    "        with rasterio.open(china_tmp) as src:\n",
    "            bbox = small_union.bounds  # (minx, miny, maxx, maxy)\n",
    "            win = rwindows.from_bounds(*bbox, transform=src.transform)\n",
    "            inset_arr = src.read(1, window=win, masked=True)\n",
    "            inset_transform = src.window_transform(win)\n",
    "\n",
    "            inset_bounds = rasterio.transform.array_bounds(\n",
    "                inset_arr.shape[0], inset_arr.shape[1], inset_transform\n",
    "            )\n",
    "            # array_bounds returns (ymin, ymax, xmin, xmax)\n",
    "            inset_extent = (inset_bounds[2], inset_bounds[3],\n",
    "                            inset_bounds[0], inset_bounds[1])\n",
    "\n",
    "        # === Set up colormap: 0=grey (non-irrigated), 1=dark blue (irrigated), nodata transparent ===\n",
    "        cmap = ListedColormap([\n",
    "            (0.9, 0.9, 0.9, 1.0),  # 0 non-irrigated\n",
    "            (0.0, 0.0, 0.5, 1.0),  # 1 irrigated (dark blue)\n",
    "        ])\n",
    "        # for masked values (nodata)\n",
    "        cmap.set_bad((0, 0, 0, 0))\n",
    "\n",
    "        # === Build the figure ===\n",
    "        fig, (ax_main, ax_inset) = plt.subplots(\n",
    "            nrows=2, ncols=1,\n",
    "            figsize=(7.5, 9.0),\n",
    "            dpi=PLOT_DPI,\n",
    "            gridspec_kw={\"height_ratios\": [3, 2]}\n",
    "        )\n",
    "\n",
    "        # ----- TOP PANEL: country-wide map -----\n",
    "        ax_main.imshow(\n",
    "            china_arr,\n",
    "            origin=\"upper\",\n",
    "            cmap=cmap,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            extent=extent_main\n",
    "        )\n",
    "\n",
    "        # ONLY outer shapefile boundary in black (ADM0)\n",
    "        for g in chn_geoms:\n",
    "            if g.geom_type == \"MultiPolygon\":\n",
    "                polys = list(g.geoms)\n",
    "            else:\n",
    "                polys = [g]\n",
    "            for poly in polys:\n",
    "                x, y = poly.exterior.xy\n",
    "                ax_main.plot(x, y, color=\"black\", linewidth=0.7)\n",
    "\n",
    "        # Remove axis box but keep titles\n",
    "        for spine in ax_main.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        ax_main.set_xticks([])\n",
    "        ax_main.set_yticks([])\n",
    "        ax_main.set_xlabel(\"\")\n",
    "        ax_main.set_ylabel(\"\")\n",
    "        ax_main.set_title(f\"Irrigated area (dark blue) from {CHINA_BINARY_NAME}\", fontsize=13)\n",
    "\n",
    "        # ----- BOTTOM PANEL: zoomed inset -----\n",
    "        inset_masked = inset_arr  # already masked=True above\n",
    "\n",
    "        ax_inset.imshow(\n",
    "            inset_masked,\n",
    "            origin=\"upper\",\n",
    "            cmap=cmap,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            extent=inset_extent\n",
    "        )\n",
    "\n",
    "        # ONLY outer shapefile boundary in black (US_small)\n",
    "        for g in small_geoms:\n",
    "            if g.geom_type == \"MultiPolygon\":\n",
    "                polys = list(g.geoms)\n",
    "            else:\n",
    "                polys = [g]\n",
    "            for poly in polys:\n",
    "                x, y = poly.exterior.xy\n",
    "                ax_inset.plot(x, y, color=\"black\", linewidth=0.7)\n",
    "\n",
    "        # Remove axis box but keep titles\n",
    "        for spine in ax_inset.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        ax_inset.set_xticks([])\n",
    "        ax_inset.set_yticks([])\n",
    "        ax_inset.set_xlabel(\"\")\n",
    "        ax_inset.set_ylabel(\"\")\n",
    "        ax_inset.set_title(f\"Zoomed irrigated area ({SMALL_BASE}.shp, dark blue)\", fontsize=12)\n",
    "        ax_inset.set_aspect(\"equal\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ---- Save & upload PNG ----\n",
    "        out_png = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\").name\n",
    "        fig.savefig(out_png, bbox_inches=\"tight\", dpi=PLOT_DPI)\n",
    "        plt.close(fig)\n",
    "\n",
    "        out_title = \"US_binary_with_zoom_map.png\"\n",
    "        upload_path(drive, out_png, comp_id, title=out_title)\n",
    "\n",
    "        try:\n",
    "            os.remove(out_png)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(f\"✅ Saved country-wide + zoom map PNG as '{out_title}' in Merged/Comparisons/\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(china_tmp)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # shapefile tempdirs will be cleaned by OS eventually\n",
    "\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\n",
    "            \"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\"\n",
    "        )\n",
    "    plot_china_binary_with_zoom(drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355501db-460c-47a5-bcfe-33703ddb4af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c084b5-dfa7-4c84-b09a-fd0d354d1229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7552c253-9149-4b0b-a69e-b5683a469d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AEI] Shapefile bundle at /tmp/aei_admin_b9psri1j → found ['.cpg', '.dbf', '.prj', '.shp', '.shx']\n",
      "\n",
      "=== New_Zealand (per-admin from shapefile; AEI in hectares) ===\n",
      "   ✓ Wrote Binary/New_Zealand_AEI_binary_0_1.tif\n",
      "\n",
      "✅ Done (per-admin from shapefile; AEI in hectares; NaNs preserved).\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Per-admin AEI-constrained binarization using ONLY the admin shapefile.\n",
    "\n",
    "For each country probability TIFF:\n",
    "  - Use unit_code polygons from AEI_2020_2_withAEI.* (AEI in HECTARES).\n",
    "  - For each unit_code, pick pixels INSIDE (centroid-in) from highest probability\n",
    "    downward until its AEI quota is met (quota = floor(AEI_ha*10_000 / 900) pixels).\n",
    "  - Preserve NaNs from the probability input.\n",
    "  - Write ONE binary 0/1 TIFF per country to:\n",
    "      Drive → CountryModelPredicted/Probability/Binary/\n",
    "  - Also write a CSV summary per country with thresholds and counts.\n",
    "\n",
    "No AEI base raster is used.\n",
    "\"\"\"\n",
    "\n",
    "import os, re, math, csv, tempfile, warnings, unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import shape, box\n",
    "from shapely.ops import transform as shp_transform\n",
    "from shapely.strtree import STRtree\n",
    "from pyproj import Transformer\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "ROOT_FOLDER_ID        = os.environ.get(\"ROOT_FOLDER_ID\", \"1gtVp-x-dIgcuNPcoaxHfmZs0ctoG71PS\")\n",
    "PARENT_FOLDER_NAME    = \"CountryModelPredicted\"\n",
    "PROB_SUBFOLDER_NAME   = \"Probability\"      # fuzzy-matched (also matches \"Porbability\")\n",
    "NATIONAL_AEI_FOLDER   = \"National AEI\"     # shapefile lives here\n",
    "\n",
    "# Probability & binning\n",
    "PIXEL_AREA_M2         = 30.0 * 30.0        # 900 m² per 30 m pixel\n",
    "TILE                  = 1024               # reduce if memory tight\n",
    "SCALE                 = 1000               # probability bins: 0..1 → 0..1000\n",
    "\n",
    "# Shapefile (AEI in HECTARES)\n",
    "ADMIN_SHP_BASE        = \"Copy of AEI_2020_2_with_AEI\"  # base name (no extension)\n",
    "ADMIN_CODE_COL        = \"unit_code\"\n",
    "ADMIN_AEI_COLS        = [\"AEI_2020\", \"AEI2020\", \"AEI\"]  # values in HECTARES\n",
    "ADMIN_CNTRY_COLS      = [\"name_cntr\", \"name_cntr1\", \"name_admin\", \"ST_NM\"]\n",
    "\n",
    "# Deterministic tie-breaking\n",
    "RNG_SEED              = int(os.environ.get(\"AEI_RNG_SEED\", \"0\"))\n",
    "_rng = np.random.default_rng(RNG_SEED)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid: return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        tkey = re.sub(r\"\\s+\", \"\", f.get(\"title\",\"\").lower())\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def _resolve_field(props_sample: dict, candidates, required=False):\n",
    "    \"\"\"Fuzzy, case-insensitive resolver for attribute fields.\"\"\"\n",
    "    def canon(s): return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
    "    keys = list(props_sample.keys())\n",
    "    norm_map = {canon(k): k for k in keys}\n",
    "    for want in candidates:\n",
    "        w = canon(want)\n",
    "        if w in norm_map:\n",
    "            return norm_map[w]\n",
    "    want_roots = {canon(want) for want in candidates}\n",
    "    for k in keys:\n",
    "        ck = canon(k)\n",
    "        if any(root in ck for root in want_roots):\n",
    "            return k\n",
    "    if required:\n",
    "        raise RuntimeError(f\"Required attribute not found. Looked for: {candidates}. Available: {keys}\")\n",
    "    return None\n",
    "\n",
    "def download_to_temp(drive_file, suffix):\n",
    "    p = tempfile.NamedTemporaryFile(delete=False, suffix=suffix).name\n",
    "    drive_file.GetContentFile(p)\n",
    "    return p\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    if res: return res[0][\"id\"]\n",
    "    nf = drive.CreateFile({\"title\": name, \"parents\":[{\"id\": parent_id}], \"mimeType\":\"application/vnd.google-apps.folder\"})\n",
    "    nf.Upload()\n",
    "    return nf[\"id\"]\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\":[{\"id\": parent_id}]})\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "# ---------------------- Name helpers ----------------------\n",
    "def _norm(s): return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
    "def _canon(s):\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s)).encode(\"ascii\",\"ignore\").decode()\n",
    "    return re.sub(r\"\\s+\",\"\", s.lower())\n",
    "\n",
    "def _extract_country_from_fname(fname):\n",
    "    # Albania_RF_probability_percent.tif → Albania\n",
    "    fn = re.sub(r\"\\s*\\(.*\\)\\.tif(f)?$\", \".tif\", fname, flags=re.IGNORECASE)\n",
    "    m = re.match(r\"(.+?)_RF_probability_percent\", fn, flags=re.IGNORECASE)\n",
    "    if m: return m.group(1)\n",
    "    return re.sub(r\"\\.tif(f)?$\", \"\", fn, flags=re.IGNORECASE)\n",
    "\n",
    "# ---------------------- Shapefile helpers -----------------\n",
    "def _download_shapefile_bundle(drive, folder_id, base):\n",
    "    \"\"\"\n",
    "    Download AEI_2020_2_withAEI.* into a single temp directory, ensuring\n",
    "    all sidecars share the SAME basename so GDAL/Fiona can see attributes.\n",
    "    Returns a dict of local paths keyed by extension ('.shp', '.dbf', etc).\n",
    "    \"\"\"\n",
    "    exts = [\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\"]\n",
    "    items = {}\n",
    "    for it in list_files(drive, folder_id):\n",
    "        t = it.get(\"title\", \"\")\n",
    "        for e in exts:\n",
    "            if t.lower() == (base.lower() + e):\n",
    "                items[e] = it\n",
    "\n",
    "    if \".shp\" not in items or \".dbf\" not in items:\n",
    "        raise FileNotFoundError(f\"Missing pieces of {base} shapefile (need at least .shp and .dbf). Found: {sorted(items.keys())}\")\n",
    "\n",
    "    tmpdir = tempfile.mkdtemp(prefix=\"aei_admin_\")\n",
    "    out = {}\n",
    "    for e, it in items.items():\n",
    "        local_path = os.path.join(tmpdir, base + e)  # SAME BASENAME!\n",
    "        it.GetContentFile(local_path)\n",
    "        out[e] = local_path\n",
    "\n",
    "    print(f\"[AEI] Shapefile bundle at {tmpdir} → found {sorted(out.keys())}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _read_admins_for_raster(shp_path, raster_crs, raster_bounds):\n",
    "    \"\"\"\n",
    "    Load admin polygons, reproject to raster CRS (no AEI filtering).\n",
    "    - Reads field names from schema (not the first feature).\n",
    "    - Honors .cpg encoding when present.\n",
    "    - Skips null/malformed geometries defensively.\n",
    "    \"\"\"\n",
    "    feats, attrs = [], []\n",
    "    shp_dir = os.path.dirname(shp_path)\n",
    "    base = os.path.splitext(os.path.basename(shp_path))[0]\n",
    "    cpg_path = os.path.join(shp_dir, base + \".cpg\")\n",
    "\n",
    "    # Determine DBF encoding\n",
    "    encoding = None\n",
    "    if os.path.exists(cpg_path):\n",
    "        try:\n",
    "            with open(cpg_path, \"r\", encoding=\"ascii\", errors=\"ignore\") as f:\n",
    "                enc_line = f.read().strip()\n",
    "                if enc_line:\n",
    "                    encoding = enc_line\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _open_fiona(enc):\n",
    "        return fiona.open(shp_path, encoding=enc) if enc else fiona.open(shp_path)\n",
    "\n",
    "    shp_crs_final = None\n",
    "    with fiona.Env(SHAPE_RESTORE_SHX='YES'):\n",
    "        # Try cpg encoding → utf-8 → latin1\n",
    "        tried = [encoding, \"utf-8\", \"latin1\"]\n",
    "        last_err = None\n",
    "        for enc in tried:\n",
    "            try:\n",
    "                with _open_fiona(enc) as src:\n",
    "                    shp_crs_local = src.crs_wkt or src.crs\n",
    "                    props_schema = (src.schema or {}).get(\"properties\", {})\n",
    "                    field_names = list(props_schema.keys())\n",
    "                    if not field_names:\n",
    "                        raise RuntimeError(\"No attribute fields in schema (DBF not visible).\")\n",
    "\n",
    "                    # Resolve keys against SCHEMA (not a sample feature)\n",
    "                    dummy_props = {k: None for k in field_names}\n",
    "                    code_key  = _resolve_field(dummy_props, [ADMIN_CODE_COL], required=True)\n",
    "                    aei_key   = _resolve_field(dummy_props, ADMIN_AEI_COLS, required=True)  # AEI in hectares\n",
    "                    cntry_key = _resolve_field(dummy_props, ADMIN_CNTRY_COLS, required=False)\n",
    "\n",
    "                    # Transform raster bounds to shapefile CRS for coarse prefilter\n",
    "                    if shp_crs_local:\n",
    "                        rb_to_shp = Transformer.from_crs(raster_crs, shp_crs_local, always_xy=True)\n",
    "                        rb_shp = shp_transform(lambda x, y: rb_to_shp.transform(x, y), box(*raster_bounds))\n",
    "                    else:\n",
    "                        rb_shp = box(*raster_bounds)\n",
    "\n",
    "                    # Iterate features\n",
    "                    for rec in src:\n",
    "                        gj = rec.get(\"geometry\")\n",
    "                        if gj is None:\n",
    "                            continue  # null geometry → skip (defensive)\n",
    "                        try:\n",
    "                            g = shape(gj)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        if g.is_empty:\n",
    "                            continue\n",
    "                        if not g.intersects(rb_shp):\n",
    "                            continue\n",
    "\n",
    "                        props = rec.get(\"properties\") or {}\n",
    "                        try:\n",
    "                            uc = int(props[code_key])\n",
    "                            aei_ha = float(props[aei_key])  # hectares\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "                        feats.append(g)\n",
    "                        attrs.append({\n",
    "                            \"unit_code\": uc,\n",
    "                            \"aei_ha\": aei_ha,\n",
    "                            \"country\": str(props.get(cntry_key, \"\")).strip() if cntry_key else \"\"\n",
    "                        })\n",
    "                # success → keep the CRS we used\n",
    "                shp_crs_final = shp_crs_local\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "        if last_err and not feats:\n",
    "            raise RuntimeError(f\"Failed to read attributes from shapefile. Tried encodings {tried}. Last error: {last_err}\")\n",
    "\n",
    "    if not feats:\n",
    "        return [], [], None, {}\n",
    "\n",
    "    # Reproject to raster CRS for rasterize\n",
    "    shp_crs = shp_crs_final\n",
    "    if not shp_crs:\n",
    "        print(\"⚠️  Shapefile has no CRS (.prj missing). Assuming raster CRS.\")\n",
    "        shp_crs = raster_crs\n",
    "\n",
    "    transformer = Transformer.from_crs(shp_crs, raster_crs, always_xy=True)\n",
    "    geoms_ras = [shp_transform(lambda x, y: transformer.transform(x, y), g) for g in feats]\n",
    "\n",
    "    tree_ras = STRtree(geoms_ras)\n",
    "    # IMPORTANT: map by WKB (value identity), not id(...)\n",
    "    g2i_wkb = {g.wkb: i for i, g in enumerate(geoms_ras)}\n",
    "\n",
    "    return geoms_ras, attrs, tree_ras, g2i_wkb\n",
    "\n",
    "\n",
    "# ---- STRtree helper: get candidate indices robustly (Shapely 2 or fallback) ----\n",
    "def _tree_candidate_indices(tree_ras, tile_poly, geoms_ras, g2i_wkb):\n",
    "    \"\"\"\n",
    "    Return list of indices of geoms that intersect tile_poly.\n",
    "    Prefer Shapely 2's predicate indices; otherwise map WKBs.\n",
    "    \"\"\"\n",
    "    # Fast path: Shapely 2 can return integer indices with predicate\n",
    "    try:\n",
    "        idx = tree_ras.query(tile_poly, predicate=\"intersects\")\n",
    "        if isinstance(idx, np.ndarray) and np.issubdtype(idx.dtype, np.integer):\n",
    "            return idx.tolist()\n",
    "    except TypeError:\n",
    "        # Older shapely: predicate argument not supported\n",
    "        pass\n",
    "\n",
    "    # Fallback: geometry array → map to indices by WKB, then precise intersects\n",
    "    cand = tree_ras.query(tile_poly)\n",
    "    if isinstance(cand, np.ndarray):\n",
    "        cand = cand.tolist()\n",
    "    out = []\n",
    "    for g in cand:\n",
    "        i = g2i_wkb.get(g.wkb, None)\n",
    "        if i is None:\n",
    "            # last resort: linear search (rare)\n",
    "            try:\n",
    "                i = next(j for j, gg in enumerate(geoms_ras) if gg.equals(g))\n",
    "            except StopIteration:\n",
    "                continue\n",
    "        if geoms_ras[i].intersects(tile_poly):\n",
    "            out.append(i)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------- Raster helpers -------------------\n",
    "def _iter_tiles(H, W, tile=TILE):\n",
    "    for r0 in range(0, H, tile):\n",
    "        for c0 in range(0, W, tile):\n",
    "            h = min(tile, H - r0)\n",
    "            w = min(tile, W - c0)\n",
    "            yield Window(c0, r0, w, h)\n",
    "\n",
    "def _read_prob_tile(src, W):\n",
    "    arr = src.read(1, window=W, out_dtype=\"float32\", masked=True).filled(np.nan)\n",
    "    finite = np.isfinite(arr)\n",
    "    if finite.any() and float(np.nanmax(arr[finite])) > 1.5:\n",
    "        arr[finite] /= 100.0\n",
    "    np.clip(arr, 0.0, 1.0, out=arr, where=finite)\n",
    "    return arr\n",
    "\n",
    "def _tile_bounds(window, transform):\n",
    "    left, top = transform * (window.col_off, window.row_off)\n",
    "    right, bottom = transform * (window.col_off + window.width, window.row_off + window.height)\n",
    "    x0, x1 = sorted([left, right])\n",
    "    y0, y1 = sorted([bottom, top])\n",
    "    return (x0, y0, x1, y1)\n",
    "\n",
    "# ---------------------- Core algorithm -------------------\n",
    "def aei_binarize_per_admin_from_shapefile(drive):\n",
    "    \"\"\"\n",
    "    Main entry: uses ONLY the admin shapefile with AEI in hectares\n",
    "    to allocate per-admin pixel quotas and write one binary per country.\n",
    "    \"\"\"\n",
    "    # Locate folders\n",
    "    cmp_id  = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id: raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "    prob_id = get_subfolder_fuzzy(drive, cmp_id, PROB_SUBFOLDER_NAME)\n",
    "    if not prob_id: raise RuntimeError(\"Probability folder not found (tried fuzzy match).\")\n",
    "    binary_id = get_or_create_folder(drive, prob_id, \"Binary\")\n",
    "\n",
    "    aei_folder_id = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, NATIONAL_AEI_FOLDER)\n",
    "    if not aei_folder_id: raise RuntimeError(\"National AEI folder not found at ROOT.\")\n",
    "\n",
    "    # Download admin shapefile bundle\n",
    "    shp_paths = _download_shapefile_bundle(drive, aei_folder_id, ADMIN_SHP_BASE)\n",
    "    shp_path  = shp_paths[\".shp\"]\n",
    "\n",
    "    # List probability TIFFs\n",
    "    files = [it for it in list_files(drive, prob_id)\n",
    "             if isinstance(it, dict)\n",
    "             and it.get(\"mimeType\") != \"application/vnd.google-apps.folder\"\n",
    "             and it.get(\"title\",\"\").lower().endswith((\".tif\",\".tiff\"))]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No probability TIFFs in Probability folder.\")\n",
    "\n",
    "    for it in files:\n",
    "        title   = it.get(\"title\",\"\")\n",
    "        country = _extract_country_from_fname(title)\n",
    "        print(f\"\\n=== {country} (per-admin from shapefile; AEI in hectares) ===\")\n",
    "\n",
    "        rtmp = download_to_temp(it, \".tif\")\n",
    "        with rasterio.open(rtmp) as src:\n",
    "            H, W = src.height, src.width\n",
    "            ras_crs = src.crs\n",
    "            rb = src.bounds\n",
    "            ras_bounds = (rb.left, rb.bottom, rb.right, rb.top)\n",
    "\n",
    "            # Read & subset admins, reproject to raster CRS\n",
    "            geoms_ras, attrs, tree_ras, g2i_wkb = _read_admins_for_raster(shp_path, ras_crs, ras_bounds)\n",
    "            if not geoms_ras:\n",
    "                print(\"  ⚠️  No admin polygons intersect this raster; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Optional filter by country name if present (keeps all if missing)\n",
    "            want = _canon(country)\n",
    "            keep = [i for i,a in enumerate(attrs) if (not a[\"country\"]) or _canon(a[\"country\"]) == want]\n",
    "            if keep and len(keep) < len(attrs):\n",
    "                geoms_ras = [geoms_ras[i] for i in keep]\n",
    "                attrs     = [attrs[i] for i in keep]\n",
    "                tree_ras  = STRtree(geoms_ras)\n",
    "                g2i_wkb   = {g.wkb: i for i, g in enumerate(geoms_ras)}\n",
    "\n",
    "            # Targets per admin (AEI in HECTARES → m² → pixels); include zeros\n",
    "            K_map = {}\n",
    "            for a in attrs:\n",
    "                aei_m2 = a[\"aei_ha\"] * 10_000.0\n",
    "                K_map[a[\"unit_code\"]] = int(math.floor(aei_m2 / PIXEL_AREA_M2))  # may be 0\n",
    "\n",
    "            # PASS 1: per-admin histograms of probability bins (centroid-in)\n",
    "            hists = defaultdict(Counter)\n",
    "            for w in _iter_tiles(H, W, TILE):\n",
    "                prob = _read_prob_tile(src, w)\n",
    "                valid = np.isfinite(prob)\n",
    "                if not valid.any():\n",
    "                    continue\n",
    "\n",
    "                tile_t = rasterio.windows.transform(w, src.transform)\n",
    "                tb = _tile_bounds(w, src.transform)\n",
    "                tile_poly = box(*tb)\n",
    "\n",
    "                idxs = _tree_candidate_indices(tree_ras, tile_poly, geoms_ras, g2i_wkb)\n",
    "                if len(idxs) == 0:\n",
    "                    continue\n",
    "\n",
    "                shapes = [(geoms_ras[i], attrs[i][\"unit_code\"]) for i in idxs]\n",
    "                labels = rasterize(\n",
    "                    shapes=shapes,\n",
    "                    out_shape=prob.shape,\n",
    "                    transform=tile_t,\n",
    "                    fill=0, dtype=\"int64\",\n",
    "                    all_touched=False  # centroid-in\n",
    "                )\n",
    "                m = valid & (labels != 0)\n",
    "                if not m.any():\n",
    "                    continue\n",
    "\n",
    "                p_int = np.zeros(prob.shape, dtype=np.int32)\n",
    "                p_int_valid = np.rint(prob[m] * SCALE).astype(np.int32)\n",
    "                p_int[m] = p_int_valid\n",
    "\n",
    "                uc = labels[m].ravel()\n",
    "                pi = p_int[m].ravel()\n",
    "                for u in np.unique(uc):\n",
    "                    sel = (uc == u)\n",
    "                    bc = np.bincount(pi[sel], minlength=SCALE+1)\n",
    "                    nz = np.nonzero(bc)[0]\n",
    "                    for b, v in zip(nz, bc[nz]):\n",
    "                        hists[u][int(b)] += int(v)\n",
    "\n",
    "            # thresholds per admin (quota 0 => thr=-1, no selection)\n",
    "            thr_map, need_eq_map = {}, {}\n",
    "            for u, K in K_map.items():\n",
    "                total = sum(hists[u].values())\n",
    "                if K <= 0 or total == 0:\n",
    "                    thr_map[u] = -1\n",
    "                    need_eq_map[u] = 0\n",
    "                    continue\n",
    "                K = min(K, total)\n",
    "                cum = 0; gt = 0\n",
    "                for b in range(SCALE, -1, -1):\n",
    "                    cnt = int(hists[u].get(b, 0))\n",
    "                    if cum + cnt >= K:\n",
    "                        thr_map[u] = b\n",
    "                        need_eq_map[u] = K - gt\n",
    "                        break\n",
    "                    cum += cnt; gt += cnt\n",
    "            need_eq_left = dict(need_eq_map)\n",
    "\n",
    "            # PASS 2: write binary output (NaN preserved; default 0; set 1s per admin)\n",
    "            out_profile = src.profile.copy()\n",
    "            out_profile.update(\n",
    "                driver=\"GTiff\",\n",
    "                height=H, width=W,\n",
    "                transform=src.transform,\n",
    "                count=1, dtype=\"float32\", nodata=np.nan,\n",
    "                compress=\"LZW\", tiled=True, blockxsize=512, blockysize=512,\n",
    "                BIGTIFF=\"IF_NEEDED\"\n",
    "            )\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\") as tmp_out:\n",
    "                out_local = tmp_out.name\n",
    "\n",
    "            with rasterio.open(out_local, \"w\", **out_profile) as dst:\n",
    "                for w in _iter_tiles(H, W, TILE):\n",
    "                    prob = _read_prob_tile(src, w)\n",
    "                    out_tile = np.full(prob.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "                    valid = np.isfinite(prob)\n",
    "                    if not valid.any():\n",
    "                        dst.write(out_tile, 1, window=w)\n",
    "                        continue\n",
    "\n",
    "                    tile_t = rasterio.windows.transform(w, src.transform)\n",
    "                    tb = _tile_bounds(w, src.transform)\n",
    "                    tile_poly = box(*tb)\n",
    "\n",
    "                    idxs = _tree_candidate_indices(tree_ras, tile_poly, geoms_ras, g2i_wkb)\n",
    "                    if len(idxs) == 0:\n",
    "                        dst.write(out_tile, 1, window=w)\n",
    "                        continue\n",
    "\n",
    "                    shapes = [(geoms_ras[i], attrs[i][\"unit_code\"]) for i in idxs]\n",
    "                    labels = rasterize(\n",
    "                        shapes=shapes,\n",
    "                        out_shape=prob.shape,\n",
    "                        transform=tile_t,\n",
    "                        fill=0, dtype=\"int64\",\n",
    "                        all_touched=False\n",
    "                    )\n",
    "\n",
    "                    out_tile[valid] = 0.0  # default: valid-but-not-selected = 0\n",
    "                    m_all = valid & (labels != 0)\n",
    "                    if m_all.any():\n",
    "                        p_int = np.zeros(prob.shape, dtype=np.int32)\n",
    "                        p_int_valid = np.rint(prob[m_all] * SCALE).astype(np.int32)\n",
    "                        p_int[m_all] = p_int_valid\n",
    "\n",
    "                        present = np.unique(labels[m_all])\n",
    "                        present = [u for u in present if u != 0]  # all units allowed\n",
    "                        for u in present:\n",
    "                            u_m = m_all & (labels == u)\n",
    "                            if not u_m.any():\n",
    "                                continue\n",
    "                            t = thr_map.get(u, -1)\n",
    "                            if t < 0:\n",
    "                                continue  # quota 0 or no pixels -> stays 0\n",
    "                            gt_m = u_m & (p_int > t)\n",
    "                            out_tile[gt_m] = 1.0\n",
    "                            need = need_eq_left.get(u, 0)\n",
    "                            if need > 0:\n",
    "                                eq_m = u_m & (p_int == t) & (out_tile != 1.0)\n",
    "                                if eq_m.any():\n",
    "                                    idx = np.flatnonzero(eq_m.ravel())\n",
    "                                    _rng.shuffle(idx)\n",
    "                                    take = min(need, idx.size)\n",
    "                                    sel = idx[:take]\n",
    "                                    rr, cc = np.unravel_index(sel, eq_m.shape)\n",
    "                                    out_tile[rr, cc] = 1.0\n",
    "                                    need_eq_left[u] = need - int(take)\n",
    "\n",
    "                    dst.write(out_tile, 1, window=w)\n",
    "\n",
    "            out_name = f\"{country}_AEI_binary_0_1.tif\"\n",
    "            upload_path(drive, out_local, binary_id, title=out_name)\n",
    "            try: os.remove(out_local)\n",
    "            except: pass\n",
    "\n",
    "            # CSV summary per admin (includes AEI==0 units)\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode=\"w\", newline=\"\") as tmpcsv:\n",
    "                wcsv = csv.writer(tmpcsv)\n",
    "                wcsv.writerow([\"unit_code\",\"aei_ha\",\"target_pixels\",\"thr_bin\",\"scale\",\"selected_pixels\"])\n",
    "                for a in attrs:\n",
    "                    u = a[\"unit_code\"]\n",
    "                    aei_ha = a[\"aei_ha\"]\n",
    "                    K = int(math.floor((aei_ha * 10_000.0) / PIXEL_AREA_M2))\n",
    "                    th = int(thr_map.get(u, -1))\n",
    "                    gt = sum(v for b, v in (hists[u].items() if u in hists else []) if b > th)\n",
    "                    ties_taken = (need_eq_map.get(u, 0) - need_eq_left.get(u, 0))\n",
    "                    sel = int(gt + max(0, ties_taken))\n",
    "                    wcsv.writerow([u, aei_ha, int(K), th, SCALE, sel])\n",
    "                csv_path = tmpcsv.name\n",
    "            upload_path(drive, csv_path, binary_id, title=f\"{country}_AEI_admin_summary.csv\")\n",
    "            try: os.remove(csv_path)\n",
    "            except: pass\n",
    "\n",
    "            print(f\"   ✓ Wrote Binary/{out_name}\")\n",
    "\n",
    "        try: os.remove(rtmp)\n",
    "        except: pass\n",
    "\n",
    "    print(\"\\n✅ Done (per-admin from shapefile; AEI in hectares; NaNs preserved).\")\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\")\n",
    "    aei_binarize_per_admin_from_shapefile(drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a95a022-6c2e-446c-bbd1-141b0df75f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 binary rasters in Binary/\n",
      "\n",
      "=== Smoothing New_Zealand_AEI_binary_0_1.tif → New_Zealand_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/New_Zealand_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "✅ Done smoothing all AEI binary maps (majority filter).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b41573ef-5741-4e29-94dc-0e55d3eb5542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/4158/.local/lib/python3.11/site-packages/rasterio/merge.py:217: RasterioDeprecationWarning: The precision parameter is unused, deprecated, and will be removed in 2.0.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged 9 rasters → Binary/Oceania_binary.tif\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Merge all per-country AEI binary rasters in Drive → CountryModelPredicted/Probability/Binary/\n",
    "into a single global binary mosaic (union = max), preserving NaNs.\n",
    "\n",
    "Output: Merged_AEI_binary_0_1.tif in the same Binary folder.\n",
    "\"\"\"\n",
    "\n",
    "import os, re, tempfile, warnings\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.merge import merge as rio_merge\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "ROOT_FOLDER_ID      = os.environ.get(\"ROOT_FOLDER_ID\", \"1gtVp-x-dIgcuNPcoaxHfmZs0ctoG71PS\")\n",
    "PARENT_FOLDER_NAME  = \"CountryModelPredicted\"\n",
    "PROB_SUBFOLDER_NAME = \"Probability\"   # fuzzy-matched (also matches misspellings)\n",
    "BINARY_SUBFOLDER    = \"Binary_MAJ\"\n",
    "OUT_NAME            = \"Oceania_binary.tif\"\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid: return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        tkey = re.sub(r\"\\s+\", \"\", f.get(\"title\",\"\").lower())\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    if res: return res[0][\"id\"]\n",
    "    nf = drive.CreateFile({\"title\": name, \"parents\":[{\"id\": parent_id}], \"mimeType\":\"application/vnd.google-apps.folder\"})\n",
    "    nf.Upload()\n",
    "    return nf[\"id\"]\n",
    "\n",
    "def download_to_temp(drive_file, suffix):\n",
    "    p = tempfile.NamedTemporaryFile(delete=False, suffix=suffix).name\n",
    "    drive_file.GetContentFile(p)\n",
    "    return p\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\":[{\"id\": parent_id}]})\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "# ---------------------- Merge helpers ---------------------\n",
    "def _try_merge_with_max(srcs):\n",
    "    \"\"\"Preferred path (newer rasterio): method='max', nodata=np.nan.\"\"\"\n",
    "    return rio_merge(\n",
    "        srcs,\n",
    "        nodata=np.nan,\n",
    "        dtype=\"float32\",\n",
    "        precision=7,\n",
    "        resampling=Resampling.nearest,\n",
    "        method=\"max\",\n",
    "    )\n",
    "\n",
    "def _try_merge_basic(srcs):\n",
    "    \"\"\"Older rasterio: no 'method'. We'll nanmax ourselves after merging.\"\"\"\n",
    "    mosaic, out_transform = rio_merge(\n",
    "        srcs,\n",
    "        nodata=np.nan,\n",
    "        dtype=\"float32\",\n",
    "        precision=7,\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "    # emulate union across sources (elementwise maximum, ignoring NaNs)\n",
    "    mosaic = np.nanmax(mosaic, axis=0, keepdims=True).astype(\"float32\")\n",
    "    return mosaic, out_transform\n",
    "\n",
    "def _try_merge_basic_sentinel(srcs, sentinel=-9999.0):\n",
    "    \"\"\"Very old rasterio: nodata cannot be NaN. Use sentinel then convert and nanmax.\"\"\"\n",
    "    mosaic, out_transform = rio_merge(\n",
    "        srcs,\n",
    "        nodata=sentinel,\n",
    "        dtype=\"float32\",\n",
    "        precision=7,\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "    # convert sentinel to NaN\n",
    "    mosaic = mosaic.astype(\"float32\", copy=False)\n",
    "    mosaic[mosaic == sentinel] = np.nan\n",
    "    mosaic = np.nanmax(mosaic, axis=0, keepdims=True).astype(\"float32\")\n",
    "    return mosaic, out_transform\n",
    "\n",
    "# ---------------------- Main merge ------------------------\n",
    "def merge_all_binary_rasters(drive):\n",
    "    # Locate folders\n",
    "    cmp_id     = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id: raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "    prob_id    = get_subfolder_fuzzy(drive, cmp_id, PROB_SUBFOLDER_NAME)\n",
    "    if not prob_id: raise RuntimeError(\"Probability folder not found (tried fuzzy match).\")\n",
    "    binary_id  = get_subfolder_fuzzy(drive, prob_id, BINARY_SUBFOLDER)\n",
    "    if not binary_id:\n",
    "        binary_id = get_or_create_folder(drive, prob_id, BINARY_SUBFOLDER)\n",
    "\n",
    "    # Find all binary GeoTIFFs (skip the mosaic itself if re-running)\n",
    "    tifs = [it for it in list_files(drive, binary_id)\n",
    "            if isinstance(it, dict)\n",
    "            and it.get(\"mimeType\") != \"application/vnd.google-apps.folder\"\n",
    "            and it.get(\"title\",\"\").lower().endswith((\".tif\",\".tiff\"))\n",
    "            and OUT_NAME.lower() not in it.get(\"title\",\"\").lower()]\n",
    "    if not tifs:\n",
    "        raise FileNotFoundError(\"No binary .tif files found in Binary/.\")\n",
    "\n",
    "    # Download & open datasets\n",
    "    local_paths, srcs = [], []\n",
    "    try:\n",
    "        for it in tifs:\n",
    "            p = download_to_temp(it, \".tif\")\n",
    "            local_paths.append(p)\n",
    "            srcs.append(rasterio.open(p))\n",
    "\n",
    "        # Try modern merge with 'method=max' → else fallback strategies\n",
    "        try:\n",
    "            mosaic, out_transform = _try_merge_with_max(srcs)\n",
    "        except TypeError:\n",
    "            # 'method' not supported\n",
    "            try:\n",
    "                mosaic, out_transform = _try_merge_basic(srcs)\n",
    "            except Exception:\n",
    "                mosaic, out_transform = _try_merge_basic_sentinel(srcs)\n",
    "        except Exception:\n",
    "            # Any other unexpected error → robust fallback\n",
    "            try:\n",
    "                mosaic, out_transform = _try_merge_basic(srcs)\n",
    "            except Exception:\n",
    "                mosaic, out_transform = _try_merge_basic_sentinel(srcs)\n",
    "\n",
    "        # Build output profile from first raster\n",
    "        ref = srcs[0]\n",
    "        out_profile = ref.profile.copy()\n",
    "        out_profile.update(\n",
    "            driver=\"GTiff\",\n",
    "            height=mosaic.shape[1],\n",
    "            width=mosaic.shape[2],\n",
    "            transform=out_transform,\n",
    "            count=1,\n",
    "            dtype=\"float32\",\n",
    "            nodata=np.nan,\n",
    "            compress=\"LZW\",\n",
    "            tiled=True,\n",
    "            blockxsize=512,\n",
    "            blockysize=512,\n",
    "            BIGTIFF=\"IF_NEEDED\"\n",
    "        )\n",
    "\n",
    "        # Write to temp, then upload\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\") as tmp_out:\n",
    "            out_local = tmp_out.name\n",
    "\n",
    "        with rasterio.open(out_local, \"w\", **out_profile) as dst:\n",
    "            dst.write(mosaic[0], 1)\n",
    "\n",
    "        upload_path(drive, out_local, binary_id, title=OUT_NAME)\n",
    "        try: os.remove(out_local)\n",
    "        except: pass\n",
    "\n",
    "        print(f\"✅ Merged {len(srcs)} rasters → Binary/{OUT_NAME}\")\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        for s in srcs:\n",
    "            try: s.close()\n",
    "            except: pass\n",
    "        for p in local_paths:\n",
    "            try: os.remove(p)\n",
    "            except: pass\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\")\n",
    "    merge_all_binary_rasters(drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a90aa-7b5a-4387-a55f-c3be0e50decf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842d6551-51a0-4ab3-ae0c-d6527059e629",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 555\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m     \u001b[43mdrive\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'drive' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 557\u001b[39m\n\u001b[32m    555\u001b[39m     drive  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPyDrive2 \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdrive\u001b[39m\u001b[33m'\u001b[39m\u001b[33m not found. Authenticate and expose a global `drive` before running.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    558\u001b[39m aei_binarize_per_admin_from_shapefile(drive)\n",
      "\u001b[31mRuntimeError\u001b[39m: PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9dfbd-9fc9-4449-a622-fe45a791d3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6bbc59-eb87-46b1-92ac-d63c2be89131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# match your Slurm allocation (\"64\")\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"64\"        # OpenMP (NumPy/SciPy, some raster ops)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"64\"        # MKL-backed NumPy / scikit-learn\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"64\"    # if you use numexpr\n",
    "os.environ[\"GDAL_NUM_THREADS\"] = \"64\"       # for internal GDAL threaded ops\n",
    "os.environ[\"RASTERIO_NUM_THREADS\"] = \"64\"   # rasterio’s thread pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4d5393-652a-405a-b203-52ea2ce6acb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vector points (shp): China.shp\n",
      "=== Train regional model: Algeria ===\n",
      "   • AEZ sampled from tiles/global for 8465 points\n",
      "   • Training fill from rasters for China_Heilongjiang: 8465 points\n",
      "=== PREDICTION per-country (threshold mode: hybrid) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bf63e3fd3643d895cab5f50f455527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predict China_Heilongjiang:   0%|          | 0/1221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • predicted irrigated fraction inside cropland: 30.787% (61,209,471/198,819,062)\n",
      "✓ China_Heilongjiang: maps → CountryModelPredicted_Cropland, model → Model Training/Regional Models/Algeria, thr=0.619\n",
      "✅ Done.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Regional RF irrigation model with:\n",
    "- Farm-aware split via DBSCAN (no leakage)\n",
    "- AEZ categorical (one-hot)\n",
    "- OUT-OF-FOLD probability calibration (isotonic by default, Platt optional)\n",
    "- Regional OOF metrics + artifacts\n",
    "- Per-country OOF metrics & thresholds saved\n",
    "- Country predictions on ~30m WGS84 using calibrated probabilities\n",
    "\n",
    "Now updated to:\n",
    "  • Read AEZ per-point from Model Training/US_GTPS_per_point.(parquet|csv) when available\n",
    "  • Prefer per-state AEZ tiles: By Country/<State>/<State>_AEZ.tif (fallback: global AEZ)\n",
    "\n",
    "Outputs:\n",
    "  CountryModelPredicted/<Country>_RF_probability_percent.tif\n",
    "  CountryModelPredicted/<Country>_RF_binary_0_1_cropland.tif\n",
    "  CountryModelPredicted/<Country>_RF_predictors_count.tif\n",
    "\n",
    "  Model Training/Regional Models/<Region>/\n",
    "    - metrics.json\n",
    "    - confusion.png\n",
    "    - feature_importance.csv\n",
    "    - per_country_test_metrics.csv\n",
    "    - model.joblib\n",
    "\n",
    "  Model Training/Country Models/<Country>/\n",
    "    - threshold.json\n",
    "    - test_metrics.json\n",
    "    - test_confusion.png\n",
    "\"\"\"\n",
    "import os, re, json, math, tempfile, joblib, time, random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, average_precision_score, roc_auc_score,\n",
    "    precision_recall_fscore_support, confusion_matrix, roc_curve,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import pyproj\n",
    "from sklearn.base import clone\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling as RioResampling\n",
    "from rasterio.warp import reproject, transform as rio_transform, transform_bounds as rio_transform_bounds\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.windows import Window\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from googleapiclient.errors import HttpError\n",
    "except Exception:\n",
    "    HttpError = Exception\n",
    "try:\n",
    "    from pydrive2.files import ApiRequestError\n",
    "except Exception:\n",
    "    ApiRequestError = Exception\n",
    "\n",
    "os.environ.setdefault(\"SHAPE_RESTORE_SHX\", \"YES\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "# ------------------------ CONFIG ------------------------\n",
    "ROOT_FOLDER_ID = os.environ.get(\"ROOT_FOLDER_ID\", \"19Ee1kUar4oEabBObq7GcA-jDR7F29x2_\")\n",
    "BY_COUNTRY_NAME = \"By Country\"\n",
    "CLE_FOLDER_ID = \"1BQqWjXTEfTjWcf_SICM4h3ICeX9jAI65\"\n",
    "MODEL_FOLDER = \"Model Training\"\n",
    "REGIONAL_MODELS_FOLDER = \"Regional Models\"  # used per country now: region_name == country\n",
    "COUNTRY_MODELS_FOLDER = \"Country Models\"\n",
    "OUTPUT_FOLDER = \"CountryModelPredicted_Cropland\"\n",
    "\n",
    "# River Network (continental fallback for distance-to-river)\n",
    "RIVER_NET_FOLDER_ID = \"1ReAmJ809FmU-8jLpyyO6VEdmeqDmbaB7\"\n",
    "RIVER_DIST_PREFIX = \"Dist_\"\n",
    "RIVER_DIST_SUFFIX = \"_river.tif\"\n",
    "\n",
    "# Country column auto-detection; we copy into this name\n",
    "COUNTRY_COL = os.environ.get(\"COUNTRY_COL\", \"country_hint\")\n",
    "\n",
    "# DBSCAN farm grouping (meters)\n",
    "DBSCAN_EPS_M = float(os.environ.get(\"DBSCAN_EPS_M\", 45))\n",
    "DBSCAN_MIN_SAMPLES = int(os.environ.get(\"DBSCAN_MIN_SAMPLES\", 1))\n",
    "\n",
    "# AEZ (global raster)\n",
    "AEZ_FILE_ID   = os.environ.get(\"AEZ_FILE_ID\", \"1te3nKn8vyt2AECmk8NM_xBgpEFXp84RX\")\n",
    "AEZ_FILE_NAME = os.environ.get(\"AEZ_FILE_NAME\", \"AEZ_2020s.tif\")\n",
    "AEZ_COL       = \"AEZ\"\n",
    "\n",
    "# Local fallbacks\n",
    "LOCAL_BASE_DIR = os.path.join(\"./\", \"China\")\n",
    "LOCAL_BY_COUNTRY_DIR = os.path.join(LOCAL_BASE_DIR, \"By Country\")\n",
    "LOCAL_CLE_DIR = os.path.join(LOCAL_BASE_DIR, \"CLE\")\n",
    "LOCAL_MODEL_DIR = os.path.join(LOCAL_BASE_DIR, MODEL_FOLDER)\n",
    "LOCAL_OUTPUT_DIR = os.path.join(LOCAL_BASE_DIR, OUTPUT_FOLDER)\n",
    "\n",
    "# GTPS/AEZ cache made earlier\n",
    "GTPS_CACHE_PARQUET = \"ChinaGTPS_per_point.parquet\"\n",
    "GTPS_CACHE_CSV     = \"ChinaGTPS_per_point.csv\"\n",
    "\n",
    "# Drive retry knobs\n",
    "DRIVE_MAX_RETRIES = int(os.environ.get(\"DRIVE_MAX_RETRIES\", 6))\n",
    "DRIVE_RETRY_BASE = float(os.environ.get(\"DRIVE_RETRY_BASE\", 0.8))\n",
    "\n",
    "POINTS_BASENAME = \"China\"  # pooled training points\n",
    "\n",
    "ALL_PREDICTORS = [\n",
    "    \"NDVI_mean\",\"NDWI_mean\",\"GI_mean\",\n",
    "    \"NDVI_max\",\"NDWI_max\",\"GI_max\",\n",
    "    \"NDVI_min\",\"NDWI_min\",\"GI_min\",\n",
    "    \"elevation\",\"slope\",\"ET\",\"PET\",\"dist_to_river\",\n",
    "    AEZ_COL,\n",
    "]\n",
    "TARGET = \"irrigated\"\n",
    "LONCOL = \"longitude\"\n",
    "LATCOL = \"latitude\"\n",
    "\n",
    "\n",
    "# RF + data\n",
    "RF_TREES = int(os.environ.get(\"RF_TREES\", 400))\n",
    "RANDOM_SEED = 42\n",
    "TEST_FRACTION = 0.30\n",
    "\n",
    "BORROW_MAX = 2000\n",
    "\n",
    "# ---- Calibration & thresholds ----\n",
    "CALIBRATION_METHOD = os.environ.get(\"CALIBRATION_METHOD\", \"isotonic\").lower()  # \"isotonic\" | \"platt\"\n",
    "THRESHOLD_STRATEGY = os.environ.get(\"THRESHOLD_STRATEGY\", \"auto\").lower()      # \"auto\"|\"j\"|\"f1\"|\"fixed\"|\"precision_at\"|\"recall_at\"\n",
    "FIXED_THRESHOLD = float(os.environ.get(\"FIXED_THRESHOLD\", 0.50))\n",
    "PRECISION_TARGET = os.environ.get(\"PRECISION_TARGET\")\n",
    "RECALL_TARGET = os.environ.get(\"RECALL_TARGET\")\n",
    "PRECISION_TARGET = None if PRECISION_TARGET in (None, \"\", \"None\") else float(PRECISION_TARGET)\n",
    "RECALL_TARGET    = None if RECALL_TARGET    in (None, \"\", \"None\") else float(RECALL_TARGET)\n",
    "\n",
    "# Per-country threshold mode: \"global\" | \"country\" | \"hybrid\"\n",
    "THRESHOLD_MODE = os.environ.get(\"THRESHOLD_MODE\", \"hybrid\").lower()\n",
    "MIN_POS_NEG_FOR_COUNTRY = int(os.environ.get(\"MIN_POS_NEG_FOR_COUNTRY\", 30))\n",
    "MIN_TOTAL_FOR_COUNTRY   = int(os.environ.get(\"MIN_TOTAL_FOR_COUNTRY\", 100))\n",
    "\n",
    "# Prediction\n",
    "TILE_SIZE = 1024\n",
    "RESAMPLING = RioResampling.bilinear\n",
    "PER_BAND_RESAMPLING = {AEZ_COL: RioResampling.nearest}\n",
    "STRICT_ALL_BANDS = False\n",
    "MIN_PREDICTORS = 9\n",
    "\n",
    "# ------------------ UTILS / HELPERS ---------------------\n",
    "def _meters_per_degree(lat_deg: float):\n",
    "    m_per_deg_lat = 111_132.0\n",
    "    m_per_deg_lon = 111_320.0 * math.cos(math.radians(lat_deg))\n",
    "    return m_per_deg_lat, m_per_deg_lon\n",
    "\n",
    "def _target_wgs84_grid_from_bounds(west, south, east, north, pixel_m=30.0):\n",
    "    mid_lat = 0.5 * (south + north)\n",
    "    m_lat, m_lon = _meters_per_degree(mid_lat)\n",
    "    dy = pixel_m / m_lat; dx = pixel_m / m_lon\n",
    "    width = int(math.ceil((east - west) / dx)); height = int(math.ceil((north - south) / dy))\n",
    "    transform = Affine(dx, 0, west, 0, -dy, north)\n",
    "    return height, width, transform, CRS.from_epsg(4326)\n",
    "\n",
    "def _iter_tiles(H, W, tile):\n",
    "    for r0 in range(0, H, tile):\n",
    "        for c0 in range(0, W, tile):\n",
    "            r1 = min(r0 + tile, H); c1 = min(c0 + tile, W)\n",
    "            yield Window.from_slices((r0, r1), (c0, c1))\n",
    "\n",
    "def _tile_count(H, W, tile):\n",
    "    return ((H + tile - 1) // tile) * ((W + tile - 1) // tile)\n",
    "\n",
    "def _compute_basic_metrics(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    try: roc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception: roc = float(\"nan\")\n",
    "    try: prauc = average_precision_score(y_true, y_prob)\n",
    "    except Exception: prauc = float(\"nan\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(pr),\n",
    "        \"recall\": float(rc),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"pr_auc\": float(prauc),\n",
    "        \"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]),\n",
    "        \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1]),\n",
    "    }, cm\n",
    "\n",
    "def _save_confusion_png(cm, title, path):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xlabel(\"Pred\"); ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title)\n",
    "    for (r,c),v in np.ndenumerate(cm):\n",
    "        ax.text(c, r, f\"{v}\", ha=\"center\", va=\"center\")\n",
    "    fig.savefig(path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def assign_farm_ids(df: pd.DataFrame, lon_col=LONCOL, lat_col=LATCOL) -> pd.Series:\n",
    "    if df.empty:\n",
    "        return pd.Series([], dtype=\"int64\")\n",
    "    proj = pyproj.Transformer.from_crs(4326, 3857, always_xy=True)\n",
    "    x, y = proj.transform(df[lon_col].values, df[lat_col].values)\n",
    "    coords = np.column_stack([x, y])\n",
    "    db = DBSCAN(eps=DBSCAN_EPS_M, min_samples=DBSCAN_MIN_SAMPLES, metric=\"euclidean\", algorithm=\"ball_tree\")\n",
    "    labels = db.fit_predict(coords)\n",
    "    if (labels == -1).any():\n",
    "        max_lab = labels[labels >= 0].max() if np.any(labels >= 0) else -1\n",
    "        noise_idx = np.where(labels == -1)[0]\n",
    "        labels[noise_idx] = np.arange(max_lab + 1, max_lab + 1 + len(noise_idx))\n",
    "    return pd.Series(labels.astype(\"int64\"), index=df.index, name=\"farm_id\")\n",
    "\n",
    "def _pick_country_candidate(cols):\n",
    "    cand = [\"country_hint\",\"country\",\"Country\",\"COUNTRY\",\"admin\",\"ADMIN\",\"state\",\"STATE\",\"STATE_NAME\",\"State\",\"province\",\"Province\"]\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for c in cand:\n",
    "        if c.lower() in low:\n",
    "            return low[c.lower()]\n",
    "    return None\n",
    "\n",
    "# ---------------------- DRIVE HELPERS -------------------\n",
    "def _drive_retry(callable_fn, *args, **kwargs):\n",
    "    last_err = None\n",
    "    for i in range(DRIVE_MAX_RETRIES):\n",
    "        try:\n",
    "            return callable_fn(*args, **kwargs)\n",
    "        except (HttpError, ApiRequestError) as e:\n",
    "            last_err = e\n",
    "            code = getattr(getattr(e, \"resp\", None), \"status\", None)\n",
    "            if code is None:\n",
    "                msg = str(e).lower()\n",
    "                transient = any(k in msg for k in [\"internal error\",\"backenderror\",\"rate limit\",\"timeout\"])\n",
    "            else:\n",
    "                transient = 500 <= int(code) < 600 or int(code) in (403, 429)\n",
    "            if not transient or i == DRIVE_MAX_RETRIES - 1:\n",
    "                break\n",
    "            sleep_s = DRIVE_RETRY_BASE * (2 ** i) + random.random() * 0.2\n",
    "            print(f\"[Drive Retry] attempt {i+1}/{DRIVE_MAX_RETRIES} after error {e}; sleeping {sleep_s:.2f}s\")\n",
    "            time.sleep(sleep_s)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            break\n",
    "    raise last_err\n",
    "\n",
    "def _drive_query(drive, q):\n",
    "    def _run():\n",
    "        return drive.ListFile({\"q\": q, \"supportsAllDrives\": True, \"includeItemsFromAllDrives\": True, \"maxResults\": 1000}).GetList()\n",
    "    return _drive_retry(_run)\n",
    "\n",
    "def get_subfolder(drive, parent_id, name):\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BASE_DIR, name)\n",
    "        return p if os.path.isdir(p) else None\n",
    "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    r = _drive_query(drive, q)\n",
    "    return r[0][\"id\"] if r else None\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    if drive is None:\n",
    "        base = LOCAL_BASE_DIR if not os.path.isabs(parent_id) else parent_id\n",
    "        p = os.path.join(base, name) if os.path.isdir(base) else os.path.join(LOCAL_BASE_DIR, name)\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "        return p\n",
    "    q = f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\"\n",
    "    res = _drive_query(drive, q)\n",
    "    if res:\n",
    "        return res[0][\"id\"]\n",
    "    def _create():\n",
    "        f = drive.CreateFile({\"title\": name, \"parents\": [{\"id\": parent_id}], \"mimeType\": \"application/vnd.google-apps.folder\"})\n",
    "        f.Upload(); return f[\"id\"]\n",
    "    return _drive_retry(_create)\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    if drive is None:\n",
    "        return sorted([os.path.join(parent_id, p) for p in os.listdir(parent_id)]) if os.path.isdir(parent_id) else []\n",
    "    q = f\"'{parent_id}' in parents and trashed=false\"\n",
    "    try: return _drive_query(drive, q)\n",
    "    except Exception as e:\n",
    "        print(f\"[Drive Warning] list_files failed for parent {parent_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def download_to_temp(drive_file, dst_path):\n",
    "    def _dl():\n",
    "        drive_file.GetContentFile(dst_path); return dst_path\n",
    "    return _drive_retry(_dl)\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    if drive is None:\n",
    "        os.makedirs(parent_id, exist_ok=True)\n",
    "        import shutil\n",
    "        dst = os.path.join(parent_id, title or os.path.basename(local_path))\n",
    "        shutil.copy2(local_path, dst); return dst\n",
    "    def _up():\n",
    "        f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\": [{\"id\": parent_id}]})\n",
    "        f.SetContentFile(local_path); f.Upload(); return f[\"id\"]\n",
    "    return _drive_retry(_up)\n",
    "\n",
    "def _drive_walk(drive, start_id, max_depth=4):\n",
    "    q = deque([(start_id, 0)])\n",
    "    while q:\n",
    "        fid, d = q.popleft()\n",
    "        items = list_files(drive, fid)\n",
    "        yield fid, items\n",
    "        if d >= max_depth: continue\n",
    "        for it in items:\n",
    "            if isinstance(it, dict) and it.get(\"mimeType\") == \"application/vnd.google-apps.folder\":\n",
    "                q.append((it[\"id\"], d + 1))\n",
    "\n",
    "# ---------------------- AEZ SOURCES ----------------------\n",
    "def open_aez_path(drive):\n",
    "    \"\"\"Global AEZ.tif (fallback if per-country tile missing).\"\"\"\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
    "        return p if os.path.exists(p) else None\n",
    "\n",
    "    def _resolve_shortcut(file_obj):\n",
    "        if file_obj.get(\"mimeType\") == \"application/vnd.google-apps.shortcut\":\n",
    "            tgt = file_obj.get(\"shortcutDetails\", {}).get(\"targetId\")\n",
    "            if tgt:\n",
    "                g = drive.CreateFile({\"id\": tgt})\n",
    "                g.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); return g\n",
    "        return file_obj\n",
    "\n",
    "    folder_id = None\n",
    "    if AEZ_FILE_ID:\n",
    "        f = drive.CreateFile({\"id\": AEZ_FILE_ID})\n",
    "        f.FetchMetadata(fields=\"title,mimeType,shortcutDetails\"); f = _resolve_shortcut(f)\n",
    "        mime = f.get(\"mimeType\")\n",
    "        if mime == \"application/vnd.google-apps.folder\":\n",
    "            folder_id = f[\"id\"]\n",
    "        elif mime and mime.startswith(\"application/vnd.google-apps.\"):\n",
    "            raise RuntimeError(f\"AEZ_FILE_ID points to a Google Doc ({mime}), not a TIFF.\")\n",
    "        else:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            f.GetContentFile(tmp); return tmp\n",
    "\n",
    "    folder_id = folder_id or os.environ.get(\"AEZ_FOLDER_ID\", None)\n",
    "    if folder_id:\n",
    "        q = f\"'{folder_id}' in parents and trashed=false and title='{AEZ_FILE_NAME}'\"\n",
    "        cand = _drive_query(drive, q)\n",
    "        if not cand:\n",
    "            q_any = f\"'{folder_id}' in parents and trashed=false and title contains '.tif'\"\n",
    "            cand = _drive_query(drive, q_any)\n",
    "            cand = [c for c in cand if c.get(\"title\",\"\") == AEZ_FILE_NAME] or cand\n",
    "        if not cand:\n",
    "            raise RuntimeError(f\"Could not find {AEZ_FILE_NAME} in folder id {folder_id}.\")\n",
    "        f = drive.CreateFile({\"id\": cand[0][\"id\"]})\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "        f.GetContentFile(tmp); return tmp\n",
    "\n",
    "    p = os.path.join(LOCAL_BASE_DIR, \"AEZ\", AEZ_FILE_NAME)\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "# ---------------------- POINTS / CACHE -------------------\n",
    "def _find_points_vector_drive(drive):\n",
    "    try: mt = get_subfolder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "    except Exception: mt = None\n",
    "    if mt:\n",
    "        items = list_files(drive, mt); parts=[]; has_shp=False\n",
    "        for it in items:\n",
    "            if not isinstance(it, dict): continue\n",
    "            title = it.get(\"title\",\"\"); low = title.lower()\n",
    "            if not low.startswith(\"US.\"): continue\n",
    "            if low.endswith(\".shp\"): has_shp=True\n",
    "            if re.search(r\"\\.(shp|dbf|shx|prj|cpg|qpj)$\", low): parts.append(it)\n",
    "        if has_shp and parts:\n",
    "            tdir = tempfile.mkdtemp()\n",
    "            for p in parts: download_to_temp(p, os.path.join(tdir, p.get(\"title\",\"\")))\n",
    "            shp_path = os.path.join(tdir, \"US.shp\")\n",
    "            if os.path.exists(shp_path):\n",
    "                print(\"Using vector points (shp): Model Training/US.shp\"); return shp_path\n",
    "    roots = [mt or ROOT_FOLDER_ID, ROOT_FOLDER_ID]\n",
    "    for root in roots:\n",
    "        for fid, items in _drive_walk(drive, root, max_depth=4):\n",
    "            stems={}\n",
    "            for it in items:\n",
    "                if not isinstance(it, dict): continue\n",
    "                title = it.get(\"title\",\"\"); m = re.match(rf\"(.+)\\.(shp|dbf|shx|prj|cpg|qpj)$\", title, re.IGNORECASE)\n",
    "                if not m: continue\n",
    "                stem = m.group(1)\n",
    "                if stem.lower() == POINTS_BASENAME.lower():\n",
    "                    stems.setdefault(stem,[]).append(it)\n",
    "            for stem, parts in stems.items():\n",
    "                if any(p.get(\"title\",\"\").lower().endswith(\".shp\") for p in parts):\n",
    "                    tdir = tempfile.mkdtemp()\n",
    "                    for p in parts: download_to_temp(p, os.path.join(tdir, p.get(\"title\",\"\")))\n",
    "                    shp_path = os.path.join(tdir, f\"{os.path.basename(stem)}.shp\")\n",
    "                    if os.path.exists(shp_path):\n",
    "                        print(f\"Using vector points (shp): {os.path.basename(stem)}.shp\"); return shp_path\n",
    "            for it in items:\n",
    "                if not isinstance(it, dict): continue\n",
    "                title = it.get(\"title\",\"\"); low = title.lower()\n",
    "                if it.get(\"mimeType\") == \"application/vnd.google-apps.folder\": continue\n",
    "                if low.endswith(\".gpkg\") and low == f\"{POINTS_BASENAME.lower()}.gpkg\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".gpkg\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (gpkg): {title}\"); return tmp\n",
    "                if low.endswith(\".zip\") and low == f\"{POINTS_BASENAME.lower()}.zip\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (zip): {title}\"); return tmp\n",
    "                if low.endswith(\".csv\") and low == f\"{POINTS_BASENAME.lower()}.csv\":\n",
    "                    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\").name\n",
    "                    download_to_temp(it, tmp); print(f\"Using vector points (csv): {title}\"); return tmp\n",
    "    return None\n",
    "\n",
    "def _find_points_vector_local():\n",
    "    search_roots = [os.path.join(LOCAL_BASE_DIR, MODEL_FOLDER), LOCAL_BASE_DIR]\n",
    "    for root in search_roots:\n",
    "        if not os.path.isdir(root): continue\n",
    "        for nm in os.listdir(root):\n",
    "            low = nm.lower()\n",
    "            if low == f\"{POINTS_BASENAME.lower()}.csv\":\n",
    "                print(f\"Using vector points (csv): {nm}\"); return os.path.join(root, nm)\n",
    "            if POINTS_BASENAME.lower() in low and low.endswith(\".gpkg\"):\n",
    "                print(f\"Using vector points (gpkg): {nm}\"); return os.path.join(root, nm)\n",
    "            if POINTS_BASENAME.lower() in low and low.endswith(\".zip\"):\n",
    "                print(f\"Using vector points (zip): {nm}\"); return os.path.join(root, nm)\n",
    "    for root in search_roots:\n",
    "        if not os.path.isdir(root): continue\n",
    "        for nm in os.listdir(root):\n",
    "            if nm.lower().endswith(\".shp\") and POINTS_BASENAME.lower() in nm.lower():\n",
    "                print(f\"Using vector points (shp): {nm}\"); return os.path.join(root, nm)\n",
    "    return None\n",
    "\n",
    "def _ci_lookup(cols, name):\n",
    "    name_l = name.lower()\n",
    "    for c in cols:\n",
    "        if c.lower() == name_l: return c\n",
    "    return None\n",
    "\n",
    "def _round6(x):\n",
    "    return np.round(pd.to_numeric(x, errors=\"coerce\"), 6)\n",
    "\n",
    "def _load_gtps_cache_df(drive):\n",
    "    \"\"\"Load US_GTPS_per_point.(parquet|csv) from Model Training if present.\"\"\"\n",
    "    def _from_drive():\n",
    "        mt = get_or_create_folder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "        # prefer parquet\n",
    "        res = _drive_query(drive, f\"'{mt}' in parents and trashed=false and title='{GTPS_CACHE_PARQUET}'\")\n",
    "        if res:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".parquet\").name\n",
    "            download_to_temp(res[0], tmp)\n",
    "            try:\n",
    "                return pd.read_parquet(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "        res = _drive_query(drive, f\"'{mt}' in parents and trashed=false and title='{GTPS_CACHE_CSV}'\")\n",
    "        if res:\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\").name\n",
    "            download_to_temp(res[0], tmp)\n",
    "            return pd.read_csv(tmp)\n",
    "        return None\n",
    "\n",
    "    def _from_local():\n",
    "        p1 = os.path.join(LOCAL_MODEL_DIR, GTPS_CACHE_PARQUET)\n",
    "        p2 = os.path.join(LOCAL_MODEL_DIR, GTPS_CACHE_CSV)\n",
    "        if os.path.exists(p1):\n",
    "            try:\n",
    "                return pd.read_parquet(p1)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if os.path.exists(p2):\n",
    "            return pd.read_csv(p2)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return _from_drive() if drive is not None else _from_local()\n",
    "    except Exception as e:\n",
    "        print(f\"   ! AEZ cache read error: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_points_df(drive):\n",
    "    import geopandas as gpd, fiona\n",
    "    path = _find_points_vector_drive(drive) if drive is not None else _find_points_vector_local()\n",
    "    if path is None:\n",
    "        raise RuntimeError(\"Could not locate US.(shp/zip/gpkg/csv) in 'Model Training' tree.\")\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        with fiona.Env(SHAPE_RESTORE_SHX=\"YES\"):\n",
    "            gdf = gpd.read_file(path)\n",
    "        if gdf.crs is not None and gdf.crs.to_epsg() != 4326:\n",
    "            gdf = gdf.to_crs(4326)\n",
    "        df = pd.DataFrame(gdf.drop(columns=\"geometry\", errors=\"ignore\"))\n",
    "\n",
    "    lon_col = _ci_lookup(df.columns, LONCOL); lat_col = _ci_lookup(df.columns, LATCOL)\n",
    "    if lon_col is None or lat_col is None:\n",
    "        raise RuntimeError(\"Points must contain longitude and latitude columns.\")\n",
    "    if lon_col != LONCOL: df[LONCOL] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "    if lat_col != LATCOL: df[LATCOL] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "\n",
    "    tgt_col = _ci_lookup(df.columns, TARGET)\n",
    "    if tgt_col is None: raise RuntimeError(f\"Points file missing column: {TARGET}\")\n",
    "    if tgt_col != TARGET: df[TARGET] = pd.to_numeric(df[tgt_col], errors=\"coerce\")\n",
    "\n",
    "    # pre-create rounders for cache join\n",
    "    df[\"_lonr\"] = _round6(df[LONCOL]); df[\"_latr\"] = _round6(df[LATCOL])\n",
    "\n",
    "    # copy a country-like column\n",
    "    if COUNTRY_COL not in df.columns:\n",
    "        cand = _pick_country_candidate(df.columns)\n",
    "        df[COUNTRY_COL] = df[cand] if cand else \"Unknown\"\n",
    "\n",
    "    for c in [TARGET, LONCOL, LATCOL]:\n",
    "        if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[TARGET, LONCOL, LATCOL]).reset_index(drop=True)\n",
    "    df[TARGET] = df[TARGET].astype(int)\n",
    "    df[COUNTRY_COL] = df[COUNTRY_COL].astype(str)\n",
    "\n",
    "    # Bring AEZ from cache if available\n",
    "    if AEZ_COL not in df.columns or df[AEZ_COL].isna().all():\n",
    "        cache = _load_gtps_cache_df(drive)\n",
    "        if cache is not None:\n",
    "            # normalize cache cols\n",
    "            cc = {c.lower(): c for c in cache.columns}\n",
    "            lonc = cc.get(LONCOL.lower(), None) or cc.get(\"x\") or cc.get(\"lon\") or list(cache.columns)[1]\n",
    "            latc = cc.get(LATCOL.lower(), None) or cc.get(\"y\") or cc.get(\"lat\") or list(cache.columns)[2]\n",
    "            aezc = cc.get(AEZ_COL.lower(), None) or cc.get(\"gtps\") or cc.get(\"aez\")\n",
    "            if aezc is not None:\n",
    "                cache[\"_lonr\"] = _round6(cache[lonc]); cache[\"_latr\"] = _round6(cache[latc])\n",
    "                df = df.merge(cache[[\"_lonr\",\"_latr\",aezc]].rename(columns={aezc:AEZ_COL}),\n",
    "                              on=[\"_lonr\",\"_latr\"], how=\"left\")\n",
    "                if AEZ_COL in df.columns:\n",
    "                    df[AEZ_COL] = pd.to_numeric(df[AEZ_COL], errors=\"coerce\")\n",
    "                    df[AEZ_COL] = np.rint(df[AEZ_COL]).astype(\"float64\")\n",
    "                    print(f\"   • AEZ filled from cache for {np.isfinite(df[AEZ_COL]).sum()}/{len(df)} points\")\n",
    "    return df\n",
    "\n",
    "# ---------------------- RASTERS -------------------------\n",
    "def open_country_var_path(drive, country, variable):\n",
    "    \"\"\"Any By Country/<Country>/<Country>_<variable>.tif (case-sensitive on names passed in).\"\"\"\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_BY_COUNTRY_DIR, country, f\"{country}_{variable}.tif\")\n",
    "        return p if os.path.exists(p) else None\n",
    "    byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
    "    if not byc: return None\n",
    "    q = f\"'{byc}' in parents and trashed=false and title='{country}' and mimeType='application/vnd.google-apps.folder'\"\n",
    "    res = _drive_query(drive, q)\n",
    "    if not res: return None\n",
    "    cid = res[0][\"id\"]\n",
    "    q2 = f\"'{cid}' in parents and trashed=false and title='{country}_{variable}.tif'\"\n",
    "    files = _drive_query(drive, q2)\n",
    "    if files:\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "        return download_to_temp(files[0], tmp)\n",
    "    for it in list_files(drive, cid):\n",
    "        if isinstance(it, dict) and re.match(rf\"^{re.escape(country)}_{re.escape(variable)}.*\\.tif$\", it.get(\"title\",\"\"), re.IGNORECASE):\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            return download_to_temp(it, tmp)\n",
    "    return None\n",
    "\n",
    "def open_CroplandNE(drive):\n",
    "    if drive is None:\n",
    "        p = os.path.join(LOCAL_CLE_DIR, \"CroplandSE.tif\")\n",
    "        return p if os.path.exists(p) else None\n",
    "    items = list_files(drive, CLE_FOLDER_ID)\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict): continue\n",
    "        if it.get(\"title\",\"\").lower().startswith(\"CroplandSE\") and it.get(\"title\",\"\").lower().endswith(\".tif\"):\n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\").name\n",
    "            return download_to_temp(it, tmp)\n",
    "    return None\n",
    "\n",
    "def _clip_bounds_from_reference(raster_path):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        b = rio_transform_bounds(src.crs, CRS.from_epsg(4326),\n",
    "                                 src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top, densify_pts=8)\n",
    "    return b\n",
    "\n",
    "def _pick_reference_raster_for_bounds(drive, country):\n",
    "    # Prefer any available country raster (including AEZ tile) to set the grid\n",
    "    for v in [AEZ_COL, \"CLE\"] + ALL_PREDICTORS:\n",
    "        p = open_country_var_path(drive, country, v) if v != AEZ_COL else open_country_var_path(drive, country, AEZ_COL)\n",
    "        if v == AEZ_COL and p is None:\n",
    "            p = open_aez_path(drive)  # fallback\n",
    "        if p is not None: return p\n",
    "    raise RuntimeError(f\"No reference raster found for bounds in '{country}'.\")\n",
    "\n",
    "def country_grid_and_mask(drive, country):\n",
    "    ref_path = _pick_reference_raster_for_bounds(drive, country)\n",
    "    west, south, east, north = _clip_bounds_from_reference(ref_path)\n",
    "    H, W, transform, crs = _target_wgs84_grid_from_bounds(west, south, east, north, pixel_m=30.0)\n",
    "    country_cropland = open_country_var_path(drive, country, \"CLE\")\n",
    "    if country_cropland is not None:\n",
    "        cropland = clip_cle_to_grid(drive, H, W, transform, crs, source_path=country_cropland)\n",
    "    else:\n",
    "        cropland = np.ones((H, W), dtype=bool)\n",
    "    return (H, W, transform, crs, cropland, (west, south, east, north))\n",
    "\n",
    "def clip_cle_to_grid(drive, out_h, out_w, out_transform, out_crs, source_path=None):\n",
    "    cle = source_path or open_CroplandNE(drive)\n",
    "    if cle is None: return np.ones((out_h, out_w), dtype=bool)\n",
    "    arr = np.full((out_h, out_w), -9999.0, dtype=np.float32)\n",
    "    with rasterio.open(cle) as src:\n",
    "        reproject(source=rasterio.band(src, 1), destination=arr,\n",
    "                  src_transform=src.transform, src_crs=src.crs,\n",
    "                  dst_transform=out_transform, dst_crs=out_crs,\n",
    "                  dst_nodata=-9999.0, resampling=RioResampling.nearest)\n",
    "        nd = src.nodata\n",
    "    if nd is not None: arr = np.where(np.isclose(arr, nd), np.nan, arr)\n",
    "    arr = np.where(np.isclose(arr, -9999.0), np.nan, arr)\n",
    "    return np.isfinite(arr) & (arr > 0.5)\n",
    "\n",
    "# ---------------- THRESHOLDS -----------------------------\n",
    "def _thr_by_J(y_true, p):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    if np.unique(y).size < 2: return FIXED_THRESHOLD\n",
    "    fpr, tpr, th = roc_curve(y, p)\n",
    "    if len(th) < 2: return FIXED_THRESHOLD\n",
    "    j = tpr - fpr\n",
    "    return float(th[int(np.argmax(j))])\n",
    "\n",
    "def _thr_by_F1(y_true, p):\n",
    "    y = np.asarray(y_true).astype(int)\n",
    "    ths = np.linspace(0.01, 0.99, 99); best_t, best_f1 = 0.5, -1\n",
    "    for t in ths:\n",
    "        yhat = (p >= t).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y, yhat, average=\"binary\", zero_division=0)\n",
    "        if f1 > best_f1: best_f1, best_t = f1, t\n",
    "    return float(best_t)\n",
    "\n",
    "def _thr_precision_at(y_true, p, target):\n",
    "    if target is None: return _thr_by_J(y_true, p)\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    cand = thr[prec[1:] >= target]\n",
    "    if len(cand) > 0:\n",
    "        return float(np.min(cand))  # smallest threshold achieving target precision\n",
    "    return _thr_by_F1(y_true, p)\n",
    "\n",
    "def _thr_recall_at(y_true, p, target):\n",
    "    if target is None: return _thr_by_J(y_true, p)\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    cand = thr[rec[1:] >= target]\n",
    "    if len(cand) > 0:\n",
    "        return float(np.max(cand))  # largest threshold while keeping recall >= target\n",
    "    return _thr_by_F1(y_true, p)\n",
    "\n",
    "def _pick_threshold(y_true, p):\n",
    "    s = THRESHOLD_STRATEGY.lower()\n",
    "    if s == \"fixed\":       return float(FIXED_THRESHOLD)\n",
    "    if s == \"f1\":          return _thr_by_F1(y_true, p)\n",
    "    if s == \"j\":           return _thr_by_J(y_true, p)\n",
    "    if s == \"precision_at\":return _thr_precision_at(y_true, p, PRECISION_TARGET)\n",
    "    if s == \"recall_at\":   return _thr_recall_at(y_true, p, RECALL_TARGET)\n",
    "    if PRECISION_TARGET is not None:\n",
    "        return _thr_precision_at(y_true, p, PRECISION_TARGET)\n",
    "    if RECALL_TARGET is not None:\n",
    "        return _thr_recall_at(y_true, p, RECALL_TARGET)\n",
    "    return _thr_by_J(y_true, p)\n",
    "\n",
    "# ---------------- Calibration helpers -------------------\n",
    "def _fit_calibrator(raw_pos_probs, y, method=\"isotonic\"):\n",
    "    raw_pos_probs = np.asarray(raw_pos_probs, dtype=float).ravel()\n",
    "    y = np.asarray(y, dtype=int).ravel()\n",
    "    if method == \"platt\":\n",
    "        lr = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "        lr.fit(raw_pos_probs.reshape(-1,1), y)\n",
    "        return {\"method\": \"platt\", \"model\": lr}\n",
    "    ir = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds=\"clip\")\n",
    "    ir.fit(raw_pos_probs, y)\n",
    "    return {\"method\": \"isotonic\", \"model\": ir}\n",
    "\n",
    "def _apply_calibrator(calib, raw_pos_probs):\n",
    "    raw_pos_probs = np.asarray(raw_pos_probs, dtype=float).ravel()\n",
    "    if calib is None:\n",
    "        return raw_pos_probs\n",
    "    if calib.get(\"method\") == \"platt\":\n",
    "        return calib[\"model\"].predict_proba(raw_pos_probs.reshape(-1,1))[:,1]\n",
    "    return calib[\"model\"].predict(raw_pos_probs)\n",
    "\n",
    "# ------------- POINT SAMPLING FROM RASTERS --------------\n",
    "def _sample_raster_at_lonlat(src, lon_arr, lat_arr):\n",
    "    xs = np.asarray(lon_arr, dtype=float); ys = np.asarray(lat_arr, dtype=float)\n",
    "    if src.crs and (src.crs.to_epsg() != 4326):\n",
    "        tx, ty = rio_transform(CRS.from_epsg(4326), src.crs, xs.tolist(), ys.tolist())\n",
    "        xs = np.asarray(tx, dtype=float); ys = np.asarray(ty, dtype=float)\n",
    "    out = np.full(xs.shape[0], np.nan, dtype=np.float32)\n",
    "    xmin, ymin, xmax, ymax = src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top\n",
    "    inside = (xs >= xmin) & (xs <= xmax) & (ys >= ymin) & (ys <= ymax)\n",
    "    if not np.any(inside): return out\n",
    "    idx = np.where(inside)[0]\n",
    "    coords = list(zip(xs[idx], ys[idx]))\n",
    "    vals = np.array([v[0] for v in src.sample(coords)], dtype=np.float32)\n",
    "    if src.nodata is not None and np.isfinite(src.nodata):\n",
    "        vals = np.where(np.isclose(vals, np.float32(src.nodata)), np.float32(np.nan), vals)\n",
    "    out[idx] = vals; return out\n",
    "\n",
    "# ---- TRAINING FILL (num predictors + AEZ) ---------------\n",
    "def _fill_training_predictors_from_country_rasters(drive, df_all, countries):\n",
    "    df = df_all.copy()\n",
    "\n",
    "    # AEZ: if still missing after cache merge, try per-country tiles, else global\n",
    "    if AEZ_COL not in df.columns or df[AEZ_COL].isna().any():\n",
    "        miss = df.index if AEZ_COL not in df.columns else df.index[df[AEZ_COL].isna()]\n",
    "        if len(miss) > 0:\n",
    "            aez_filled = 0\n",
    "            # try per-country tiles first\n",
    "            for c in countries:\n",
    "                try:\n",
    "                    ref = open_country_var_path(drive, c, AEZ_COL) or open_aez_path(drive)\n",
    "                    if not ref: continue\n",
    "                    w, s, e, n = _clip_bounds_from_reference(ref)\n",
    "                    idx = df.loc[miss].index[\n",
    "                        df.loc[miss, LONCOL].between(min(w,e), max(w,e)) &\n",
    "                        df.loc[miss, LATCOL].between(min(s,n), max(s,n))\n",
    "                    ]\n",
    "                    if len(idx) == 0: continue\n",
    "                    with rasterio.open(ref) as aez_src:\n",
    "                        vals = _sample_raster_at_lonlat(aez_src, df.loc[idx, LONCOL].values, df.loc[idx, LATCOL].values)\n",
    "                    df.loc[idx, AEZ_COL] = np.rint(pd.to_numeric(vals, errors=\"coerce\")).astype(\"float64\")\n",
    "                    aez_filled += int(np.isfinite(df.loc[idx, AEZ_COL]).sum())\n",
    "                except Exception as ex:\n",
    "                    print(f\"   ! AEZ fill skip for {c}: {ex}\")\n",
    "            if aez_filled > 0:\n",
    "                print(f\"   • AEZ sampled from tiles/global for {aez_filled} points\")\n",
    "\n",
    "    # Per-country numeric predictors from rasters\n",
    "    for c in countries:\n",
    "        try:\n",
    "            ref = open_country_var_path(drive, c, \"CLE\") or open_country_var_path(drive, c, \"NDVI_mean\")\n",
    "            if not ref: continue\n",
    "            w, s, e, n = _clip_bounds_from_reference(ref)\n",
    "        except Exception as ex:\n",
    "            print(f\"   ! Skip raster fill for {c}: {ex}\"); continue\n",
    "        idx = df.index[df[LONCOL].between(min(w,e), max(w,e)) & df[LATCOL].between(min(s,n), max(s,n))]\n",
    "        if len(idx) == 0: continue\n",
    "        raster_cache = {}; filled_any = False\n",
    "        for p in ALL_PREDICTORS:\n",
    "            if p == AEZ_COL: continue\n",
    "            pth = open_country_var_path(drive, c, p)\n",
    "            if pth is None: continue\n",
    "            if p not in raster_cache: raster_cache[p] = rasterio.open(pth)\n",
    "            vals = _sample_raster_at_lonlat(raster_cache[p], df.loc[idx, LONCOL].values, df.loc[idx, LATCOL].values)\n",
    "            df.loc[idx, p] = pd.to_numeric(vals, errors=\"coerce\").astype(\"float64\")\n",
    "            filled_any = True\n",
    "        if filled_any:\n",
    "            print(f\"   • Training fill from rasters for {c}: {len(idx)} points\")\n",
    "    return df\n",
    "\n",
    "def list_countries_auto(drive):\n",
    "    names = set()\n",
    "    if drive is None:\n",
    "        if os.path.isdir(LOCAL_BY_COUNTRY_DIR):\n",
    "            for d in os.listdir(LOCAL_BY_COUNTRY_DIR):\n",
    "                if os.path.isdir(os.path.join(LOCAL_BY_COUNTRY_DIR, d)): names.add(d)\n",
    "    else:\n",
    "        byc = get_subfolder(drive, ROOT_FOLDER_ID, BY_COUNTRY_NAME)\n",
    "        if byc:\n",
    "            subs = _drive_query(drive, f\"'{byc}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "            for it in subs: names.add(it[\"title\"])\n",
    "    return sorted(names)\n",
    "\n",
    "# ----------------- TRAIN REGIONAL MODEL -----------------\n",
    "def _borrow_neighbors(df_all, base_cols, max_points):\n",
    "    if COUNTRY_COL not in df_all.columns:\n",
    "        df_all[COUNTRY_COL] = \"Unknown\"\n",
    "    keep_cols = list(dict.fromkeys(base_cols))\n",
    "    pooled = df_all[keep_cols].dropna(subset=[TARGET, LONCOL, LATCOL]).copy()\n",
    "    if len(pooled) > max_points:\n",
    "        pooled = pooled.sample(max_points, random_state=RANDOM_SEED)\n",
    "    if pooled[TARGET].nunique() < 2:\n",
    "        other = 1 - int(pooled[TARGET].iloc[0])\n",
    "        extra = df_all[df_all[TARGET] == other]\n",
    "        if not extra.empty:\n",
    "            take = min(1000, len(extra))\n",
    "            pooled = pd.concat([pooled, extra.sample(take, random_state=RANDOM_SEED)[keep_cols]], ignore_index=True)\n",
    "    if COUNTRY_COL not in pooled.columns:\n",
    "        pooled[COUNTRY_COL] = \"Unknown\"\n",
    "    pooled[COUNTRY_COL] = pooled[COUNTRY_COL].astype(str)\n",
    "    return pooled.reset_index(drop=True)\n",
    "\n",
    "def train_region_model(drive, region_name, df_all, countries):\n",
    "    print(f\"=== Train regional model: {region_name} ===\")\n",
    "\n",
    "    # Fill predictors (uses AEZ cache + per-country AEZ tiles)\n",
    "    df_all = _fill_training_predictors_from_country_rasters(drive, df_all, countries)\n",
    "\n",
    "    use_preds = [p for p in ALL_PREDICTORS if p in df_all.columns]\n",
    "    base_cols = [TARGET, LONCOL, LATCOL, COUNTRY_COL] + use_preds\n",
    "    df = _borrow_neighbors(df_all, base_cols, BORROW_MAX)\n",
    "\n",
    "    if len(df) < 200 or df[TARGET].nunique() < 2:\n",
    "        raise RuntimeError(f\"Insufficient pooled points for region training (n={len(df)}, classes={df[TARGET].nunique()}).\")\n",
    "\n",
    "    num_features = [f for f in use_preds if f != AEZ_COL]\n",
    "    cat_features = [AEZ_COL] if AEZ_COL in use_preds else []\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", make_pipeline(SimpleImputer(strategy=\"median\")), num_features),\n",
    "            (\"cat\", make_pipeline(SimpleImputer(strategy=\"most_frequent\"),\n",
    "                                  OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)), cat_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=RF_TREES, max_features=\"sqrt\", class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1, random_state=RANDOM_SEED, bootstrap=True,\n",
    "    )\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"rf\", rf)])\n",
    "\n",
    "    # ---------- Group-aware OOF to fit calibrator ----------\n",
    "    df[\"farm_id\"] = assign_farm_ids(df, lon_col=LONCOL, lat_col=LATCOL)\n",
    "    n_splits = 5 if 0.19 <= TEST_FRACTION <= 0.21 else max(3, int(round(1.0 / max(TEST_FRACTION, 1e-3))))\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    X_all = df[use_preds].copy()\n",
    "    y_all = df[TARGET].astype(int).values\n",
    "    g_all = df[\"farm_id\"].values\n",
    "    countries_all = df[COUNTRY_COL].astype(str).values\n",
    "    lon_all = df[LONCOL].values\n",
    "    lat_all = df[LATCOL].values\n",
    "\n",
    "    # enforce dtypes\n",
    "    for col in [f for f in use_preds if f != AEZ_COL]:\n",
    "        X_all[col] = pd.to_numeric(X_all[col], errors=\"coerce\").astype(\"float64\")\n",
    "    if AEZ_COL in use_preds:\n",
    "        X_all[AEZ_COL] = pd.to_numeric(X_all[AEZ_COL], errors=\"coerce\")\n",
    "        X_all[AEZ_COL] = np.rint(X_all[AEZ_COL]).astype(\"float64\")\n",
    "\n",
    "    oof_raw = np.full(len(X_all), np.nan, dtype=float)\n",
    "    classes_ = None\n",
    "    for tr_idx, te_idx in sgkf.split(X_all, y_all, groups=g_all):\n",
    "        tr_X = X_all.iloc[tr_idx]; tr_y = y_all[tr_idx]\n",
    "        te_X = X_all.iloc[te_idx]\n",
    "        pipe_fold = clone(pipe)\n",
    "        pipe_fold.fit(tr_X, tr_y.astype(int))\n",
    "        if classes_ is None:\n",
    "            classes_ = pipe_fold.named_steps[\"rf\"].classes_\n",
    "        pos_idx_fold = int(np.where(classes_ == 1)[0][0])\n",
    "        oof_raw[te_idx] = pipe_fold.predict_proba(te_X)[:, pos_idx_fold]\n",
    "\n",
    "    # fit final model on all data\n",
    "    pipe.fit(X_all, y_all.astype(int))\n",
    "    classes_ = pipe.named_steps[\"rf\"].classes_\n",
    "    pos_idx = int(np.where(classes_ == 1)[0][0])\n",
    "\n",
    "    # calibrate on OOF\n",
    "    calib = _fit_calibrator(oof_raw, y_all, CALIBRATION_METHOD)\n",
    "    oof_cal = _apply_calibrator(calib, oof_raw)\n",
    "\n",
    "    # ---------- GLOBAL threshold from OOF calibrated ----------\n",
    "    global_thr = _pick_threshold(y_all, oof_cal)\n",
    "\n",
    "    # ---------- Regional OOF metrics & confusion ----------\n",
    "    reg_metrics, reg_cm = _compute_basic_metrics(y_all, oof_cal, global_thr)\n",
    "    reg_metrics.update({\n",
    "        \"region\": region_name,\n",
    "        \"n_oof\": int(len(y_all)),\n",
    "        \"threshold_strategy\": THRESHOLD_STRATEGY,\n",
    "        \"global_threshold\": float(global_thr),\n",
    "        \"calibration\": CALIBRATION_METHOD,\n",
    "    })\n",
    "\n",
    "    # feature importances from final model\n",
    "    out_names = list(pipe.named_steps[\"pre\"].get_feature_names_out())\n",
    "    rf_imp = pipe.named_steps[\"rf\"].feature_importances_\n",
    "    fi_df = pd.DataFrame({\"feature\": out_names, \"importance\": rf_imp}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    # ---------- Prep folders ----------\n",
    "    if drive is None:\n",
    "        reg_base = os.path.join(LOCAL_MODEL_DIR, REGIONAL_MODELS_FOLDER, region_name)\n",
    "        os.makedirs(reg_base, exist_ok=True)\n",
    "        c_base_root = os.path.join(LOCAL_MODEL_DIR, COUNTRY_MODELS_FOLDER)\n",
    "        os.makedirs(c_base_root, exist_ok=True)\n",
    "    else:\n",
    "        mt = get_or_create_folder(drive, ROOT_FOLDER_ID, MODEL_FOLDER)\n",
    "        reg = get_or_create_folder(drive, mt, REGIONAL_MODELS_FOLDER)\n",
    "        reg_base = get_or_create_folder(drive, reg, region_name)\n",
    "        c_base_root = get_or_create_folder(drive, mt, COUNTRY_MODELS_FOLDER)\n",
    "\n",
    "    # ---------- Ensure country tags (bbox fallback) ----------\n",
    "    assigned = countries_all.copy()\n",
    "    unknown_mask = (assigned == \"Unknown\") | (assigned == \"\") | pd.isna(assigned)\n",
    "    if unknown_mask.any():\n",
    "        for ctry in countries:\n",
    "            try:\n",
    "                ref = open_country_var_path(drive, ctry, \"CLE\") or open_country_var_path(drive, ctry, AEZ_COL) or open_aez_path(drive)\n",
    "                if not ref: continue\n",
    "                w, s, e, n = _clip_bounds_from_reference(ref)\n",
    "                inside = unknown_mask & (lon_all >= min(w, e)) & (lon_all <= max(w, e)) \\\n",
    "                                   & (lat_all >= min(s, n)) & (lat_all <= max(s, n))\n",
    "                assigned[inside] = ctry\n",
    "            except Exception:\n",
    "                pass\n",
    "        countries_all = assigned\n",
    "\n",
    "    # ---------- Per-country thresholds & OOF metrics ----------\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"country\": countries_all.astype(str),\n",
    "        \"y\": y_all.astype(int),\n",
    "        \"prob_cal\": oof_cal.astype(float),\n",
    "    })\n",
    "\n",
    "    per_rows = []\n",
    "    per_thr = {}\n",
    "\n",
    "    for ctry in sorted(set(countries)):\n",
    "        sub = oof_df[oof_df[\"country\"] == ctry]\n",
    "        n_total = int(len(sub))\n",
    "        n_pos = int((sub[\"y\"] == 1).sum())\n",
    "        n_neg = n_total - n_pos\n",
    "\n",
    "        if (n_total < MIN_TOTAL_FOR_COUNTRY) or (n_pos < MIN_POS_NEG_FOR_COUNTRY) or (n_neg < MIN_POS_NEG_FOR_COUNTRY):\n",
    "            thr_c = float(global_thr); note = \"fallback_global\"\n",
    "        else:\n",
    "            thr_c = _pick_threshold(sub[\"y\"].values, sub[\"prob_cal\"].values); note = \"oof_calibrated\"\n",
    "\n",
    "        per_thr[ctry] = float(thr_c)\n",
    "        m_c, cm_c = _compute_basic_metrics(sub[\"y\"].values, sub[\"prob_cal\"].values, thr_c)\n",
    "        m_c.update({\n",
    "            \"region\": region_name, \"country\": ctry, \"n_oof\": n_total,\n",
    "            \"n_pos\": n_pos, \"n_neg\": n_neg, \"threshold_used\": float(thr_c),\n",
    "            \"note\": note\n",
    "        })\n",
    "        per_rows.append(m_c)\n",
    "\n",
    "        # save country artifacts (metrics + confusion + threshold.json)\n",
    "        cdir = get_or_create_folder(drive, c_base_root, ctry)\n",
    "        with tempfile.TemporaryDirectory() as tdir:\n",
    "            with open(os.path.join(tdir, \"test_metrics.json\"), \"w\") as f:\n",
    "                json.dump(m_c, f, indent=2)\n",
    "            upload_path(drive, os.path.join(tdir, \"test_metrics.json\"), cdir, \"test_metrics.json\")\n",
    "\n",
    "            png = os.path.join(tdir, \"test_confusion.png\")\n",
    "            _save_confusion_png(cm_c, f\"{ctry} — OOF Confusion (thr={thr_c:.3f})\", png)\n",
    "            upload_path(drive, png, cdir, \"test_confusion.png\")\n",
    "\n",
    "            with open(os.path.join(tdir, \"threshold.json\"), \"w\") as f:\n",
    "                json.dump({\n",
    "                    \"threshold_strategy\": THRESHOLD_STRATEGY,\n",
    "                    \"mode\": THRESHOLD_MODE,\n",
    "                    \"calibration\": CALIBRATION_METHOD,\n",
    "                    \"threshold\": float(thr_c),\n",
    "                    \"n_total\": n_total, \"n_pos\": n_pos, \"n_neg\": n_neg,\n",
    "                    \"note\": note\n",
    "                }, f, indent=2)\n",
    "            upload_path(drive, os.path.join(tdir, \"threshold.json\"), cdir, \"threshold.json\")\n",
    "\n",
    "    # regional CSV summary for per-country\n",
    "    if len(per_rows) > 0:\n",
    "        per_df = pd.DataFrame(per_rows).sort_values(\"country\")\n",
    "    else:\n",
    "        per_df = pd.DataFrame(columns=[\"country\",\"n_oof\",\"precision\",\"recall\",\"f1\",\"roc_auc\",\"pr_auc\",\n",
    "                                       \"tn\",\"fp\",\"fn\",\"tp\",\"threshold_used\",\"note\"])\n",
    "\n",
    "    # ---------- Save regional artifacts ----------\n",
    "    with tempfile.TemporaryDirectory() as tdir:\n",
    "        with open(os.path.join(tdir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(reg_metrics, f, indent=2)\n",
    "        upload_path(drive, os.path.join(tdir, \"metrics.json\"), reg_base, \"metrics.json\")\n",
    "\n",
    "        fi_df.to_csv(os.path.join(tdir, \"feature_importance.csv\"), index=False)\n",
    "        upload_path(drive, os.path.join(tdir, \"feature_importance.csv\"), reg_base, \"feature_importance.csv\")\n",
    "\n",
    "        per_df.to_csv(os.path.join(tdir, \"per_country_test_metrics.csv\"), index=False)\n",
    "        upload_path(drive, os.path.join(tdir, \"per_country_test_metrics.csv\"), reg_base, \"per_country_test_metrics.csv\")\n",
    "\n",
    "        png = os.path.join(tdir, \"confusion.png\")\n",
    "        _save_confusion_png(reg_cm, f\"{region_name} — OOF Confusion ({THRESHOLD_STRATEGY}, thr={global_thr:.3f})\", png)\n",
    "        upload_path(drive, png, reg_base, \"confusion.png\")\n",
    "\n",
    "        joblib.dump(\n",
    "            {\n",
    "                \"pipeline\": pipe,\n",
    "                \"predictors\": use_preds,\n",
    "                \"positive_class_index\": int(pos_idx),\n",
    "                \"calibration\": calib,\n",
    "                \"global_threshold\": float(global_thr),\n",
    "                \"threshold_strategy\": THRESHOLD_STRATEGY,\n",
    "                \"per_country_thresholds\": per_thr,\n",
    "                \"num_features\": [f for f in use_preds if f != AEZ_COL],\n",
    "                \"cat_features\": [AEZ_COL] if AEZ_COL in use_preds else [],\n",
    "            },\n",
    "            os.path.join(tdir, \"model.joblib\"),\n",
    "        )\n",
    "        upload_path(drive, os.path.join(tdir, \"model.joblib\"), reg_base, \"model.joblib\")\n",
    "\n",
    "    model = {\n",
    "        \"pipe\": pipe,\n",
    "        \"use_preds\": use_preds,\n",
    "        \"pos_idx\": int(pos_idx),\n",
    "        \"calib\": calib,\n",
    "        \"global_thr\": float(global_thr),\n",
    "        \"per_thr\": per_thr,\n",
    "        \"reg_base\": reg_base\n",
    "    }\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------- PREDICT ONE COUNTRY ------------------\n",
    "def predict_country(drive, country, model, thr, grid_pack):\n",
    "    out_dir = get_or_create_folder(drive, ROOT_FOLDER_ID, OUTPUT_FOLDER) if drive else LOCAL_OUTPUT_DIR\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    pipe   = model[\"pipe\"]\n",
    "    pos_idx= model[\"pos_idx\"]\n",
    "    calib  = model[\"calib\"]\n",
    "\n",
    "    H, W, transform, crs, cropland = grid_pack\n",
    "    raster_cache = {}\n",
    "\n",
    "    # AEZ: prefer per-country tile, fallback to global AEZ\n",
    "    aez_src_path = open_country_var_path(drive, country, AEZ_COL) or open_aez_path(drive)\n",
    "    if AEZ_COL in model[\"use_preds\"] and aez_src_path is not None:\n",
    "        raster_cache[AEZ_COL] = rasterio.open(aez_src_path)\n",
    "\n",
    "    for v in model[\"use_preds\"]:\n",
    "        if v == AEZ_COL: continue\n",
    "        p = open_country_var_path(drive, country, v)\n",
    "        if p is not None: raster_cache[v] = rasterio.open(p)\n",
    "\n",
    "    base_profile = {\n",
    "        \"driver\": \"GTiff\",\"height\": H,\"width\": W,\"count\": 1,\"crs\": crs,\"transform\": transform,\n",
    "        \"compress\": \"LZW\",\"tiled\": True,\"blockxsize\": 512,\"blockysize\": 512,\n",
    "    }\n",
    "    prob_profile = {**base_profile, \"dtype\": \"float32\", \"nodata\": -9999.0}\n",
    "    bin_profile  = {**base_profile, \"dtype\": \"uint8\", \"nodata\": 255}\n",
    "    cnt_profile  = {**base_profile, \"dtype\": \"uint8\", \"nodata\": 0}\n",
    "\n",
    "    prob_name = f\"{country}_RF_probability_percent.tif\"\n",
    "    bin_name  = f\"{country}_RF_binary_0_1_cropland.tif\"\n",
    "    cnt_name  = f\"{country}_RF_predictors_count.tif\"\n",
    "\n",
    "    eligible_total = 0; positive_total = 0\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tdir:\n",
    "        prob_tmp = os.path.join(tdir, prob_name)\n",
    "        bin_tmp  = os.path.join(tdir, bin_name)\n",
    "        cnt_tmp  = os.path.join(tdir, cnt_name)\n",
    "        with rasterio.open(prob_tmp, \"w\", **prob_profile) as dst_prob, \\\n",
    "             rasterio.open(bin_tmp,  \"w\", **bin_profile)  as dst_bin,  \\\n",
    "             rasterio.open(cnt_tmp,  \"w\", **cnt_profile) as dst_cnt:\n",
    "\n",
    "            for win in tqdm(_iter_tiles(H, W, TILE_SIZE), total=_tile_count(H, W, TILE_SIZE), desc=f\"Predict {country}\"):\n",
    "                r0, r1 = int(win.row_off), int(win.row_off + win.height)\n",
    "                c0, c1 = int(win.col_off), int(win.col_off + win.width)\n",
    "                tile_h, tile_w = (r1 - r0), (c1 - c0)\n",
    "                tile_transform = transform * Affine.translation(c0, r0)\n",
    "\n",
    "                crop_tile = cropland[r0:r1, c0:c1]\n",
    "                if not crop_tile.any():\n",
    "                    dst_prob.write(np.full((1, tile_h, tile_w), prob_profile[\"nodata\"], np.float32), window=win)\n",
    "                    dst_bin.write (np.full((1, tile_h, tile_w), bin_profile[\"nodata\"], np.uint8), window=win)\n",
    "                    dst_cnt.write (np.zeros((1, tile_h, tile_w), np.uint8), window=win)\n",
    "                    continue\n",
    "\n",
    "                stack = np.full((len(model[\"use_preds\"]), tile_h, tile_w), np.nan, dtype=np.float32)\n",
    "                valid_count = np.zeros((tile_h, tile_w), dtype=np.uint8)\n",
    "\n",
    "                for j, v in enumerate(model[\"use_preds\"]):\n",
    "                    src = raster_cache.get(v)\n",
    "                    if src is None:\n",
    "                        p = open_country_var_path(drive, country, v) if v != AEZ_COL else (open_country_var_path(drive, country, AEZ_COL) or open_aez_path(drive))\n",
    "                        if p: src = raster_cache[v] = rasterio.open(p)\n",
    "                        else: continue\n",
    "                    dst_arr = np.full((tile_h, tile_w), np.nan, dtype=np.float32)\n",
    "                    dst_nd  = -9999.0\n",
    "                    resamp  = PER_BAND_RESAMPLING.get(v, RESAMPLING)\n",
    "                    try:\n",
    "                        reproject(source=rasterio.band(src, 1), destination=dst_arr,\n",
    "                                  src_transform=src.transform, src_crs=src.crs,\n",
    "                                  dst_transform=tile_transform, dst_crs=crs,\n",
    "                                  dst_nodata=dst_nd, resampling=resamp)\n",
    "                    except Exception as e:\n",
    "                        print(f\"      ! reprojection failed for {v}: {e}\"); continue\n",
    "                    if src.nodata is not None:\n",
    "                        dst_arr = np.where(np.isclose(dst_arr, src.nodata), np.nan, dst_arr)\n",
    "                    dst_arr = np.where(np.isclose(dst_arr, dst_nd), np.nan, dst_arr)\n",
    "                    stack[j] = dst_arr; valid_count += np.isfinite(dst_arr)\n",
    "\n",
    "                req = (len(model[\"use_preds\"]) if STRICT_ALL_BANDS else min(MIN_PREDICTORS, len(model[\"use_preds\"])))\n",
    "                good = crop_tile & (valid_count >= req)\n",
    "                prob_tile = np.full((tile_h, tile_w), np.nan, dtype=np.float32)\n",
    "\n",
    "                if good.any():\n",
    "                    X = stack.reshape(len(model[\"use_preds\"]), -1).T\n",
    "                    good_flat = good.reshape(-1)\n",
    "                    X_good = X[good_flat].astype(\"float64\")\n",
    "                    X_good_df = pd.DataFrame(X_good, columns=model[\"use_preds\"])\n",
    "\n",
    "                    # enforce dtypes\n",
    "                    num_features = [f for f in model[\"use_preds\"] if f != AEZ_COL]\n",
    "                    for col in num_features:\n",
    "                        X_good_df[col] = pd.to_numeric(X_good_df[col], errors=\"coerce\").astype(\"float64\")\n",
    "                    if AEZ_COL in X_good_df.columns:\n",
    "                        X_good_df[AEZ_COL] = pd.to_numeric(X_good_df[AEZ_COL], errors=\"coerce\")\n",
    "                        X_good_df[AEZ_COL] = np.rint(X_good_df[AEZ_COL]).astype(\"float64\")\n",
    "\n",
    "                    prob_raw = pipe.predict_proba(X_good_df)[:, pos_idx].astype(np.float32)\n",
    "                    prob_vals = _apply_calibrator(calib, prob_raw).astype(np.float32)\n",
    "\n",
    "                    bin_vals  = (prob_vals >= thr).astype(np.uint8)\n",
    "                    prob_tile.reshape(-1)[good_flat] = prob_vals\n",
    "\n",
    "                    mask_arr = np.isfinite(prob_tile)\n",
    "                    prob_out = np.full((1, tile_h, tile_w), prob_profile[\"nodata\"], np.float32)\n",
    "                    prob_out[0][mask_arr] = (prob_tile[mask_arr] * 100.0).astype(np.float32)\n",
    "                    dst_prob.write(prob_out, window=win)\n",
    "\n",
    "                    bin_out = np.full((1, tile_h, tile_w), bin_profile[\"nodata\"], np.uint8)\n",
    "                    bin_out[0].reshape(-1)[good_flat] = bin_vals\n",
    "                    dst_bin.write(bin_out, window=win)\n",
    "\n",
    "                    eligible_total += int(mask_arr.sum()); positive_total += int(bin_vals.sum())\n",
    "                else:\n",
    "                    dst_prob.write(np.full((1, tile_h, tile_w), prob_profile[\"nodata\"], np.float32), window=win)\n",
    "                    dst_bin.write (np.full((1, tile_h, tile_w), bin_profile[\"nodata\"], np.uint8), window=win)\n",
    "\n",
    "                dst_cnt.write(valid_count[np.newaxis], window=win)\n",
    "\n",
    "        upload_path(drive, prob_tmp, out_dir, prob_name)\n",
    "        upload_path(drive, bin_tmp,  out_dir, bin_name)\n",
    "        upload_path(drive, cnt_tmp,  out_dir, cnt_name)\n",
    "\n",
    "    frac = (positive_total / max(1, eligible_total)) * 100.0\n",
    "    print(f\"   • predicted irrigated fraction inside cropland: {frac:.3f}% ({positive_total:,}/{eligible_total:,})\")\n",
    "\n",
    "# ----------------------------- RUNNER -------------------\n",
    "def run_region_then_countries(use_drive=True, countries=None, region_name=\"US\"):\n",
    "    if use_drive:\n",
    "        try:\n",
    "            drive  # noqa: F821\n",
    "            _drive = drive\n",
    "            try: _drive.auth.service.http.timeout = 120\n",
    "            except Exception: pass\n",
    "        except NameError:\n",
    "            raise RuntimeError(\"PyDrive 'drive' not found. Authenticate and expose 'drive', or set use_drive=False.\")\n",
    "    else:\n",
    "        _drive = None\n",
    "        os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    df_all = load_points_df(_drive)\n",
    "    for c in [TARGET, LONCOL, LATCOL] + [p for p in ALL_PREDICTORS if p in df_all.columns and p != AEZ_COL]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\")\n",
    "    df_all = df_all.dropna(subset=[TARGET, LONCOL, LATCOL]).reset_index(drop=True)\n",
    "    df_all[TARGET] = df_all[TARGET].astype(int)\n",
    "\n",
    "    if not countries:\n",
    "        countries = [\n",
    "            \"China_Heilongjiang\",\n",
    "            # \"Sudan\",\"Chad\", \"Niger\"\n",
    "            # \"Florida\",\"Georgia\",\"Arkansas\",\"Kentucky\",\"Louisiana\",\n",
    "            #          \"Mississippi\",\"South Carolina\",\"North Carolina\"\n",
    " \n",
    "        ]\n",
    "\n",
    "    model = train_region_model(_drive, region_name, df_all, countries)\n",
    "\n",
    "    # Predict\n",
    "    print(f\"=== PREDICTION per-country (threshold mode: {THRESHOLD_MODE}) ===\")\n",
    "    for c in countries:\n",
    "        try:\n",
    "            H, W, transform, crs, cropland, _ = country_grid_and_mask(_drive, c)\n",
    "            thr_c = model[\"per_thr\"].get(c, model[\"global_thr\"]) if THRESHOLD_MODE in (\"country\",\"hybrid\") else model[\"global_thr\"]\n",
    "            predict_country(_drive, c, model, thr_c, (H, W, transform, crs, cropland))\n",
    "            print(f\"✓ {c}: maps → {OUTPUT_FOLDER}, model → {MODEL_FOLDER}/{REGIONAL_MODELS_FOLDER}/{region_name}, thr={thr_c:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {c}: {e}\")\n",
    "\n",
    "    print(\"✅ Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_region_then_countries(use_drive=True, countries=None, region_name=\"Algeria\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d673cb53-eba6-4870-aec6-39c8578ce16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420925f7-715c-4857-be56-09616ba85d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
