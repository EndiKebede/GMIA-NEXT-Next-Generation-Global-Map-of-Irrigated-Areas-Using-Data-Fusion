{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a251c-bd4c-45fc-a973-17029bb7ebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google Drive connected\n"
     ]
    }
   ],
   "source": [
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "# JSON.apth \n",
    "CLIENT_JSON = \"client_secret_Final.json\"   #The google drive API \n",
    "\n",
    "gauth = GoogleAuth()\n",
    "gauth.LoadClientConfigFile(CLIENT_JSON)\n",
    "\n",
    "# Command-line auth -> \n",
    "gauth.CommandLineAuth()\n",
    "\n",
    "# Save token \n",
    "gauth.SaveCredentialsFile('token.json')\n",
    "\n",
    "drive = GoogleDrive(gauth)\n",
    "print(\"✅ Google Drive connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438cb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List  folders in Google Drive \n",
    "folder_list = drive.ListFile({\n",
    "    'q': \"'root' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
    "}).GetList()\n",
    "\n",
    "for folder in folder_list:\n",
    "    print(f\"Folder: {folder['title']}  |  ID: {folder['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f88291-b217-4c35-8c7b-99fcfe35937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AEI] Shapefile bundle at /tmp/aei_admin_ji6czoq5 → found ['.cpg', '.dbf', '.prj', '.shp', '.shx']\n",
      "\n",
      "=== North_Dakota (per-admin from shapefile; AEI in hectares) ===\n",
      "   ✓ Wrote Binary/North_Dakota_AEI_binary_0_1.tif\n",
      "\n",
      "=== Missouri (per-admin from shapefile; AEI in hectares) ===\n",
      "   ✓ Wrote Binary/Missouri_AEI_binary_0_1.tif\n",
      "\n",
      "✅ Done (per-admin from shapefile; AEI in hectares; NaNs preserved).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Per-admin AEI-constrained binarization using ONLY the admin shapefile.\n",
    "\n",
    "For each country probability TIFF:\n",
    "  - Use unit_code polygons from AEI_2020_2_withAEI.* (AEI in HECTARES).\n",
    "  - For each unit_code, pick pixels INSIDE (centroid-in) from highest probability\n",
    "    downward until its AEI quota is met (quota = floor(AEI_ha*10_000 / 900) pixels).\n",
    "  - Preserve NaNs from the probability input.\n",
    "  - Write ONE binary 0/1 TIFF per country to:\n",
    "      Drive → CountryModelPredicted/Probability/Binary/\n",
    "  - Also write a CSV summary per country with thresholds and counts.\n",
    "\"\"\"\n",
    "\n",
    "import os, re, math, csv, tempfile, warnings, unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import shape, box\n",
    "from shapely.ops import transform as shp_transform\n",
    "from shapely.strtree import STRtree\n",
    "from pyproj import Transformer\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "ROOT_FOLDER_ID        = os.environ.get(\"ROOT_FOLDER_ID\", \"1hqMIyDYEFKnpS8KLxC4bqHmF_9dHXImG\")\n",
    "PARENT_FOLDER_NAME    = \"CountryModelPredicted_Cropland\"\n",
    "PROB_SUBFOLDER_NAME   = \"Probability\"      \n",
    "NATIONAL_AEI_FOLDER   = \"National AEI\"     # folder where the shapefile of AEI statistics found\n",
    "\n",
    "# Probability & binning\n",
    "PIXEL_AREA_M2         = 30.0 * 30.0        # 900 m² per 30 m pixel\n",
    "TILE                  = 1024               # reduce if memory tight\n",
    "SCALE                 = 1000               \n",
    "\n",
    "# Shapefile (AEI in HECTARES)\n",
    "ADMIN_SHP_BASE        = \"AEI_2020_2_with_AEI\"  # base name AEI statistics shapefile each with unique unit_code \n",
    "ADMIN_CODE_COL        = \"unit_code\"\n",
    "ADMIN_AEI_COLS        = [\"AEI_2020\", \"AEI2020\", \"AEI\"]  # shapefile feature of AEI statistics  in HECTARES\n",
    "ADMIN_CNTRY_COLS      = [\"name_cntr\", \"name_cntr1\", \"name_admin\", \"ST_NM\"]\n",
    "\n",
    "# Deterministic tie-breaking\n",
    "RNG_SEED              = int(os.environ.get(\"AEI_RNG_SEED\", \"0\"))\n",
    "_rng = np.random.default_rng(RNG_SEED)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid: return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        tkey = re.sub(r\"\\s+\", \"\", f.get(\"title\",\"\").lower())\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def _resolve_field(props_sample: dict, candidates, required=False):\n",
    "    \"\"\"Fuzzy, case-insensitive resolver for attribute fields.\"\"\"\n",
    "    def canon(s): return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
    "    keys = list(props_sample.keys())\n",
    "    norm_map = {canon(k): k for k in keys}\n",
    "    for want in candidates:\n",
    "        w = canon(want)\n",
    "        if w in norm_map:\n",
    "            return norm_map[w]\n",
    "    want_roots = {canon(want) for want in candidates}\n",
    "    for k in keys:\n",
    "        ck = canon(k)\n",
    "        if any(root in ck for root in want_roots):\n",
    "            return k\n",
    "    if required:\n",
    "        raise RuntimeError(f\"Required attribute not found. Looked for: {candidates}. Available: {keys}\")\n",
    "    return None\n",
    "\n",
    "def download_to_temp(drive_file, suffix):\n",
    "    p = tempfile.NamedTemporaryFile(delete=False, suffix=suffix).name\n",
    "    drive_file.GetContentFile(p)\n",
    "    return p\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    if res: return res[0][\"id\"]\n",
    "    nf = drive.CreateFile({\"title\": name, \"parents\":[{\"id\": parent_id}], \"mimeType\":\"application/vnd.google-apps.folder\"})\n",
    "    nf.Upload()\n",
    "    return nf[\"id\"]\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\":[{\"id\": parent_id}]})\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "# ---------------------- Name helpers ----------------------\n",
    "def _norm(s): return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
    "def _canon(s):\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s)).encode(\"ascii\",\"ignore\").decode()\n",
    "    return re.sub(r\"\\s+\",\"\", s.lower())\n",
    "\n",
    "def _extract_country_from_fname(fname):\n",
    "    # Albania_RF_probability_percent.tif → Albania\n",
    "    fn = re.sub(r\"\\s*\\(.*\\)\\.tif(f)?$\", \".tif\", fname, flags=re.IGNORECASE)\n",
    "    m = re.match(r\"(.+?)_RF_probability_percent\", fn, flags=re.IGNORECASE)\n",
    "    if m: return m.group(1)\n",
    "    return re.sub(r\"\\.tif(f)?$\", \"\", fn, flags=re.IGNORECASE)\n",
    "\n",
    "# ---------------------- Shapefile helpers -----------------\n",
    "def _download_shapefile_bundle(drive, folder_id, base):\n",
    "    \"\"\"\n",
    "    Download AEI_2020_2_withAEI.* into a single temp directory, ensuring\n",
    "    all sidecars share the SAME basename so GDAL/Fiona can see attributes.\n",
    "    Returns a dict of local paths keyed by extension ('.shp', '.dbf', etc).\n",
    "    \"\"\"\n",
    "    exts = [\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\"]\n",
    "    items = {}\n",
    "    for it in list_files(drive, folder_id):\n",
    "        t = it.get(\"title\", \"\")\n",
    "        for e in exts:\n",
    "            if t.lower() == (base.lower() + e):\n",
    "                items[e] = it\n",
    "\n",
    "    if \".shp\" not in items or \".dbf\" not in items:\n",
    "        raise FileNotFoundError(f\"Missing pieces of {base} shapefile (need at least .shp and .dbf). Found: {sorted(items.keys())}\")\n",
    "\n",
    "    tmpdir = tempfile.mkdtemp(prefix=\"aei_admin_\")\n",
    "    out = {}\n",
    "    for e, it in items.items():\n",
    "        local_path = os.path.join(tmpdir, base + e)  # SAME BASENAME!\n",
    "        it.GetContentFile(local_path)\n",
    "        out[e] = local_path\n",
    "\n",
    "    print(f\"[AEI] Shapefile bundle at {tmpdir} → found {sorted(out.keys())}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _read_admins_for_raster(shp_path, raster_crs, raster_bounds):\n",
    "    \"\"\"\n",
    "    Load admin polygons, reproject to raster CRS (no AEI filtering).\n",
    "    - Reads field names from schema (not the first feature).\n",
    "    - Honors .cpg encoding when present.\n",
    "    - Skips null/malformed geometries defensively.\n",
    "    \"\"\"\n",
    "    feats, attrs = [], []\n",
    "    shp_dir = os.path.dirname(shp_path)\n",
    "    base = os.path.splitext(os.path.basename(shp_path))[0]\n",
    "    cpg_path = os.path.join(shp_dir, base + \".cpg\")\n",
    "\n",
    "    # Determine DBF encoding\n",
    "    encoding = None\n",
    "    if os.path.exists(cpg_path):\n",
    "        try:\n",
    "            with open(cpg_path, \"r\", encoding=\"ascii\", errors=\"ignore\") as f:\n",
    "                enc_line = f.read().strip()\n",
    "                if enc_line:\n",
    "                    encoding = enc_line\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _open_fiona(enc):\n",
    "        return fiona.open(shp_path, encoding=enc) if enc else fiona.open(shp_path)\n",
    "\n",
    "    shp_crs_final = None\n",
    "    with fiona.Env(SHAPE_RESTORE_SHX='YES'):\n",
    "        # Try cpg encoding → utf-8 → latin1\n",
    "        tried = [encoding, \"utf-8\", \"latin1\"]\n",
    "        last_err = None\n",
    "        for enc in tried:\n",
    "            try:\n",
    "                with _open_fiona(enc) as src:\n",
    "                    shp_crs_local = src.crs_wkt or src.crs\n",
    "                    props_schema = (src.schema or {}).get(\"properties\", {})\n",
    "                    field_names = list(props_schema.keys())\n",
    "                    if not field_names:\n",
    "                        raise RuntimeError(\"No attribute fields in schema (DBF not visible).\")\n",
    "\n",
    "                    # Resolve keys against SCHEMA (not a sample feature)\n",
    "                    dummy_props = {k: None for k in field_names}\n",
    "                    code_key  = _resolve_field(dummy_props, [ADMIN_CODE_COL], required=True)\n",
    "                    aei_key   = _resolve_field(dummy_props, ADMIN_AEI_COLS, required=True)  # AEI in hectares\n",
    "                    cntry_key = _resolve_field(dummy_props, ADMIN_CNTRY_COLS, required=False)\n",
    "\n",
    "                    # Transform raster bounds to shapefile CRS for coarse prefilter\n",
    "                    if shp_crs_local:\n",
    "                        rb_to_shp = Transformer.from_crs(raster_crs, shp_crs_local, always_xy=True)\n",
    "                        rb_shp = shp_transform(lambda x, y: rb_to_shp.transform(x, y), box(*raster_bounds))\n",
    "                    else:\n",
    "                        rb_shp = box(*raster_bounds)\n",
    "\n",
    "                    # Iterate features\n",
    "                    for rec in src:\n",
    "                        gj = rec.get(\"geometry\")\n",
    "                        if gj is None:\n",
    "                            continue  # null geometry → skip (defensive)\n",
    "                        try:\n",
    "                            g = shape(gj)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        if g.is_empty:\n",
    "                            continue\n",
    "                        if not g.intersects(rb_shp):\n",
    "                            continue\n",
    "\n",
    "                        props = rec.get(\"properties\") or {}\n",
    "                        try:\n",
    "                            uc = int(props[code_key])\n",
    "                            aei_ha = float(props[aei_key])  # hectares\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "                        feats.append(g)\n",
    "                        attrs.append({\n",
    "                            \"unit_code\": uc,\n",
    "                            \"aei_ha\": aei_ha,\n",
    "                            \"country\": str(props.get(cntry_key, \"\")).strip() if cntry_key else \"\"\n",
    "                        })\n",
    "                # success → keep the CRS we used\n",
    "                shp_crs_final = shp_crs_local\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "        if last_err and not feats:\n",
    "            raise RuntimeError(f\"Failed to read attributes from shapefile. Tried encodings {tried}. Last error: {last_err}\")\n",
    "\n",
    "    if not feats:\n",
    "        return [], [], None, {}\n",
    "\n",
    "    # Reproject to raster CRS for rasterize\n",
    "    shp_crs = shp_crs_final\n",
    "    if not shp_crs:\n",
    "        print(\"⚠️  Shapefile has no CRS (.prj missing). Assuming raster CRS.\")\n",
    "        shp_crs = raster_crs\n",
    "\n",
    "    transformer = Transformer.from_crs(shp_crs, raster_crs, always_xy=True)\n",
    "    geoms_ras = [shp_transform(lambda x, y: transformer.transform(x, y), g) for g in feats]\n",
    "\n",
    "    tree_ras = STRtree(geoms_ras)\n",
    "    # IMPORTANT: map by WKB (value identity), not id(...)\n",
    "    g2i_wkb = {g.wkb: i for i, g in enumerate(geoms_ras)}\n",
    "\n",
    "    return geoms_ras, attrs, tree_ras, g2i_wkb\n",
    "\n",
    "\n",
    "# ---- STRtree helper: get candidate indices robustly (Shapely 2 or fallback) ----\n",
    "def _tree_candidate_indices(tree_ras, tile_poly, geoms_ras, g2i_wkb):\n",
    "    \"\"\"\n",
    "    Return list of indices of geoms that intersect tile_poly.\n",
    "    Prefer Shapely 2's predicate indices; otherwise map WKBs.\n",
    "    \"\"\"\n",
    "    # Fast path: Shapely 2 can return integer indices with predicate\n",
    "    try:\n",
    "        idx = tree_ras.query(tile_poly, predicate=\"intersects\")\n",
    "        if isinstance(idx, np.ndarray) and np.issubdtype(idx.dtype, np.integer):\n",
    "            return idx.tolist()\n",
    "    except TypeError:\n",
    "        # Older shapely: predicate argument not supported\n",
    "        pass\n",
    "\n",
    "    # Fallback: geometry array → map to indices by WKB, then precise intersects\n",
    "    cand = tree_ras.query(tile_poly)\n",
    "    if isinstance(cand, np.ndarray):\n",
    "        cand = cand.tolist()\n",
    "    out = []\n",
    "    for g in cand:\n",
    "        i = g2i_wkb.get(g.wkb, None)\n",
    "        if i is None:\n",
    "            # last resort: linear search (rare)\n",
    "            try:\n",
    "                i = next(j for j, gg in enumerate(geoms_ras) if gg.equals(g))\n",
    "            except StopIteration:\n",
    "                continue\n",
    "        if geoms_ras[i].intersects(tile_poly):\n",
    "            out.append(i)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------- Raster helpers -------------------\n",
    "def _iter_tiles(H, W, tile=TILE):\n",
    "    for r0 in range(0, H, tile):\n",
    "        for c0 in range(0, W, tile):\n",
    "            h = min(tile, H - r0)\n",
    "            w = min(tile, W - c0)\n",
    "            yield Window(c0, r0, w, h)\n",
    "\n",
    "def _read_prob_tile(src, W):\n",
    "    arr = src.read(1, window=W, out_dtype=\"float32\", masked=True).filled(np.nan)\n",
    "    finite = np.isfinite(arr)\n",
    "    if finite.any() and float(np.nanmax(arr[finite])) > 1.5:\n",
    "        arr[finite] /= 100.0\n",
    "    np.clip(arr, 0.0, 1.0, out=arr, where=finite)\n",
    "    return arr\n",
    "\n",
    "def _tile_bounds(window, transform):\n",
    "    left, top = transform * (window.col_off, window.row_off)\n",
    "    right, bottom = transform * (window.col_off + window.width, window.row_off + window.height)\n",
    "    x0, x1 = sorted([left, right])\n",
    "    y0, y1 = sorted([bottom, top])\n",
    "    return (x0, y0, x1, y1)\n",
    "\n",
    "# ---------------------- Core algorithm -------------------\n",
    "def aei_binarize_per_admin_from_shapefile(drive):\n",
    "    \"\"\"\n",
    "    Main entry: uses ONLY the admin shapefile with AEI in hectares\n",
    "    to allocate per-admin pixel quotas and write one binary per country.\n",
    "    \"\"\"\n",
    "    # Locate folders\n",
    "    cmp_id  = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id: raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "    prob_id = get_subfolder_fuzzy(drive, cmp_id, PROB_SUBFOLDER_NAME)\n",
    "    if not prob_id: raise RuntimeError(\"Probability folder not found (tried fuzzy match).\")\n",
    "    binary_id = get_or_create_folder(drive, prob_id, \"Binary\")\n",
    "\n",
    "    aei_folder_id = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, NATIONAL_AEI_FOLDER)\n",
    "    if not aei_folder_id: raise RuntimeError(\"National AEI folder not found at ROOT.\")\n",
    "\n",
    "    # Download admin shapefile bundle\n",
    "    shp_paths = _download_shapefile_bundle(drive, aei_folder_id, ADMIN_SHP_BASE)\n",
    "    shp_path  = shp_paths[\".shp\"]\n",
    "\n",
    "    # List probability TIFFs\n",
    "    files = [it for it in list_files(drive, prob_id)\n",
    "             if isinstance(it, dict)\n",
    "             and it.get(\"mimeType\") != \"application/vnd.google-apps.folder\"\n",
    "             and it.get(\"title\",\"\").lower().endswith((\".tif\",\".tiff\"))]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No probability TIFFs in Probability folder.\")\n",
    "\n",
    "    for it in files:\n",
    "        title   = it.get(\"title\",\"\")\n",
    "        country = _extract_country_from_fname(title)\n",
    "        print(f\"\\n=== {country} (per-admin from shapefile; AEI in hectares) ===\")\n",
    "\n",
    "        rtmp = download_to_temp(it, \".tif\")\n",
    "        with rasterio.open(rtmp) as src:\n",
    "            H, W = src.height, src.width\n",
    "            ras_crs = src.crs\n",
    "            rb = src.bounds\n",
    "            ras_bounds = (rb.left, rb.bottom, rb.right, rb.top)\n",
    "\n",
    "            # Read & subset admins, reproject to raster CRS\n",
    "            geoms_ras, attrs, tree_ras, g2i_wkb = _read_admins_for_raster(shp_path, ras_crs, ras_bounds)\n",
    "            if not geoms_ras:\n",
    "                print(\"  ⚠️  No admin polygons intersect this raster; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Optional filter by country name if present (keeps all if missing)\n",
    "            want = _canon(country)\n",
    "            keep = [i for i,a in enumerate(attrs) if (not a[\"country\"]) or _canon(a[\"country\"]) == want]\n",
    "            if keep and len(keep) < len(attrs):\n",
    "                geoms_ras = [geoms_ras[i] for i in keep]\n",
    "                attrs     = [attrs[i] for i in keep]\n",
    "                tree_ras  = STRtree(geoms_ras)\n",
    "                g2i_wkb   = {g.wkb: i for i, g in enumerate(geoms_ras)}\n",
    "\n",
    "            # Targets per admin (AEI in HECTARES → m² → pixels); include zeros\n",
    "            K_map = {}\n",
    "            for a in attrs:\n",
    "                aei_m2 = a[\"aei_ha\"] * 10_000.0\n",
    "                K_map[a[\"unit_code\"]] = int(math.floor(aei_m2 / PIXEL_AREA_M2))  # may be 0\n",
    "\n",
    "            # PASS 1: per-admin histograms of probability bins (centroid-in)\n",
    "            hists = defaultdict(Counter)\n",
    "            for w in _iter_tiles(H, W, TILE):\n",
    "                prob = _read_prob_tile(src, w)\n",
    "                valid = np.isfinite(prob)\n",
    "                if not valid.any():\n",
    "                    continue\n",
    "\n",
    "                tile_t = rasterio.windows.transform(w, src.transform)\n",
    "                tb = _tile_bounds(w, src.transform)\n",
    "                tile_poly = box(*tb)\n",
    "\n",
    "                idxs = _tree_candidate_indices(tree_ras, tile_poly, geoms_ras, g2i_wkb)\n",
    "                if len(idxs) == 0:\n",
    "                    continue\n",
    "\n",
    "                shapes = [(geoms_ras[i], attrs[i][\"unit_code\"]) for i in idxs]\n",
    "                labels = rasterize(\n",
    "                    shapes=shapes,\n",
    "                    out_shape=prob.shape,\n",
    "                    transform=tile_t,\n",
    "                    fill=0, dtype=\"int64\",\n",
    "                    all_touched=False  # centroid-in\n",
    "                )\n",
    "                m = valid & (labels != 0)\n",
    "                if not m.any():\n",
    "                    continue\n",
    "\n",
    "                p_int = np.zeros(prob.shape, dtype=np.int32)\n",
    "                p_int_valid = np.rint(prob[m] * SCALE).astype(np.int32)\n",
    "                p_int[m] = p_int_valid\n",
    "\n",
    "                uc = labels[m].ravel()\n",
    "                pi = p_int[m].ravel()\n",
    "                for u in np.unique(uc):\n",
    "                    sel = (uc == u)\n",
    "                    bc = np.bincount(pi[sel], minlength=SCALE+1)\n",
    "                    nz = np.nonzero(bc)[0]\n",
    "                    for b, v in zip(nz, bc[nz]):\n",
    "                        hists[u][int(b)] += int(v)\n",
    "\n",
    "            # thresholds per admin (quota 0 => thr=-1, no selection)\n",
    "            thr_map, need_eq_map = {}, {}\n",
    "            for u, K in K_map.items():\n",
    "                total = sum(hists[u].values())\n",
    "                if K <= 0 or total == 0:\n",
    "                    thr_map[u] = -1\n",
    "                    need_eq_map[u] = 0\n",
    "                    continue\n",
    "                K = min(K, total)\n",
    "                cum = 0; gt = 0\n",
    "                for b in range(SCALE, -1, -1):\n",
    "                    cnt = int(hists[u].get(b, 0))\n",
    "                    if cum + cnt >= K:\n",
    "                        thr_map[u] = b\n",
    "                        need_eq_map[u] = K - gt\n",
    "                        break\n",
    "                    cum += cnt; gt += cnt\n",
    "            need_eq_left = dict(need_eq_map)\n",
    "\n",
    "            # PASS 2: write binary output (NaN preserved; default 0; set 1s per admin)\n",
    "            out_profile = src.profile.copy()\n",
    "            out_profile.update(\n",
    "                driver=\"GTiff\",\n",
    "                height=H, width=W,\n",
    "                transform=src.transform,\n",
    "                count=1, dtype=\"float32\", nodata=np.nan,\n",
    "                compress=\"LZW\", tiled=True, blockxsize=512, blockysize=512,\n",
    "                BIGTIFF=\"IF_NEEDED\"\n",
    "            )\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\") as tmp_out:\n",
    "                out_local = tmp_out.name\n",
    "\n",
    "            with rasterio.open(out_local, \"w\", **out_profile) as dst:\n",
    "                for w in _iter_tiles(H, W, TILE):\n",
    "                    prob = _read_prob_tile(src, w)\n",
    "                    out_tile = np.full(prob.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "                    valid = np.isfinite(prob)\n",
    "                    if not valid.any():\n",
    "                        dst.write(out_tile, 1, window=w)\n",
    "                        continue\n",
    "\n",
    "                    tile_t = rasterio.windows.transform(w, src.transform)\n",
    "                    tb = _tile_bounds(w, src.transform)\n",
    "                    tile_poly = box(*tb)\n",
    "\n",
    "                    idxs = _tree_candidate_indices(tree_ras, tile_poly, geoms_ras, g2i_wkb)\n",
    "                    if len(idxs) == 0:\n",
    "                        dst.write(out_tile, 1, window=w)\n",
    "                        continue\n",
    "\n",
    "                    shapes = [(geoms_ras[i], attrs[i][\"unit_code\"]) for i in idxs]\n",
    "                    labels = rasterize(\n",
    "                        shapes=shapes,\n",
    "                        out_shape=prob.shape,\n",
    "                        transform=tile_t,\n",
    "                        fill=0, dtype=\"int64\",\n",
    "                        all_touched=False\n",
    "                    )\n",
    "\n",
    "                    out_tile[valid] = 0.0  # default: valid-but-not-selected = 0\n",
    "                    m_all = valid & (labels != 0)\n",
    "                    if m_all.any():\n",
    "                        p_int = np.zeros(prob.shape, dtype=np.int32)\n",
    "                        p_int_valid = np.rint(prob[m_all] * SCALE).astype(np.int32)\n",
    "                        p_int[m_all] = p_int_valid\n",
    "\n",
    "                        present = np.unique(labels[m_all])\n",
    "                        present = [u for u in present if u != 0]  # all units allowed\n",
    "                        for u in present:\n",
    "                            u_m = m_all & (labels == u)\n",
    "                            if not u_m.any():\n",
    "                                continue\n",
    "                            t = thr_map.get(u, -1)\n",
    "                            if t < 0:\n",
    "                                continue  # quota 0 or no pixels -> stays 0\n",
    "                            gt_m = u_m & (p_int > t)\n",
    "                            out_tile[gt_m] = 1.0\n",
    "                            need = need_eq_left.get(u, 0)\n",
    "                            if need > 0:\n",
    "                                eq_m = u_m & (p_int == t) & (out_tile != 1.0)\n",
    "                                if eq_m.any():\n",
    "                                    idx = np.flatnonzero(eq_m.ravel())\n",
    "                                    _rng.shuffle(idx)\n",
    "                                    take = min(need, idx.size)\n",
    "                                    sel = idx[:take]\n",
    "                                    rr, cc = np.unravel_index(sel, eq_m.shape)\n",
    "                                    out_tile[rr, cc] = 1.0\n",
    "                                    need_eq_left[u] = need - int(take)\n",
    "\n",
    "                    dst.write(out_tile, 1, window=w)\n",
    "\n",
    "            out_name = f\"{country}_AEI_binary_0_1.tif\"\n",
    "            upload_path(drive, out_local, binary_id, title=out_name)\n",
    "            try: os.remove(out_local)\n",
    "            except: pass\n",
    "\n",
    "            # CSV summary per admin (includes AEI==0 units)\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode=\"w\", newline=\"\") as tmpcsv:\n",
    "                wcsv = csv.writer(tmpcsv)\n",
    "                wcsv.writerow([\"unit_code\",\"aei_ha\",\"target_pixels\",\"thr_bin\",\"scale\",\"selected_pixels\"])\n",
    "                for a in attrs:\n",
    "                    u = a[\"unit_code\"]\n",
    "                    aei_ha = a[\"aei_ha\"]\n",
    "                    K = int(math.floor((aei_ha * 10_000.0) / PIXEL_AREA_M2))\n",
    "                    th = int(thr_map.get(u, -1))\n",
    "                    gt = sum(v for b, v in (hists[u].items() if u in hists else []) if b > th)\n",
    "                    ties_taken = (need_eq_map.get(u, 0) - need_eq_left.get(u, 0))\n",
    "                    sel = int(gt + max(0, ties_taken))\n",
    "                    wcsv.writerow([u, aei_ha, int(K), th, SCALE, sel])\n",
    "                csv_path = tmpcsv.name\n",
    "            upload_path(drive, csv_path, binary_id, title=f\"{country}_AEI_admin_summary.csv\")\n",
    "            try: os.remove(csv_path)\n",
    "            except: pass\n",
    "\n",
    "            print(f\"   ✓ Wrote Binary/{out_name}\")\n",
    "\n",
    "        try: os.remove(rtmp)\n",
    "        except: pass\n",
    "\n",
    "    print(\"\\n✅ Done (per-admin from shapefile; AEI in hectares; NaNs preserved).\")\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\")\n",
    "    aei_binarize_per_admin_from_shapefile(drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6b230-28db-4fd6-b150-3f736ac01451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 binary rasters in Binary/\n",
      "\n",
      "=== Smoothing Missouri_AEI_binary_0_1.tif → Missouri_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Missouri_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing North_Dakota_AEI_binary_0_1.tif → North_Dakota_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/North_Dakota_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Alabama_AEI_binary_0_1.tif → Alabama_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Alabama_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Wisconsin_AEI_binary_0_1.tif → Copy of Wisconsin_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Wisconsin_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Utah_AEI_binary_0_1.tif → Copy of Utah_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Utah_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Tennessee_AEI_binary_0_1.tif → Copy of Tennessee_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Tennessee_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of South_Carolina_AEI_binary_0_1.tif → Copy of South_Carolina_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of South_Carolina_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of West_Virginia_AEI_binary_0_1.tif → Copy of West_Virginia_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of West_Virginia_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Virginia_AEI_binary_0_1.tif → Copy of Virginia_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Virginia_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of South_Dakota_AEI_binary_0_1.tif → Copy of South_Dakota_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of South_Dakota_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Wyoming_AEI_binary_0_1.tif → Copy of Wyoming_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Wyoming_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Texas_AEI_binary_0_1.tif → Copy of Texas_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Texas_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Washington_AEI_binary_0_1.tif → Copy of Washington_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Washington_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Rhode_Island_AEI_binary_0_1.tif → Copy of Rhode_Island_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Rhode_Island_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Oklahoma_AEI_binary_0_1.tif → Copy of Oklahoma_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Oklahoma_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of New_Jersey_AEI_binary_0_1.tif → Copy of New_Jersey_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of New_Jersey_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of New_Mexico_AEI_binary_0_1.tif → Copy of New_Mexico_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of New_Mexico_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of New_York_AEI_binary_0_1.tif → Copy of New_York_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of New_York_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Ohio_AEI_binary_0_1.tif → Copy of Ohio_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Ohio_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Pennsylvania_AEI_binary_0_1.tif → Copy of Pennsylvania_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Pennsylvania_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Oregon_AEI_binary_0_1.tif → Copy of Oregon_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Oregon_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of North_Carolina_AEI_binary_0_1.tif → Copy of North_Carolina_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of North_Carolina_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Montana_AEI_binary_0_1.tif → Copy of Montana_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Montana_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Nebraska_AEI_binary_0_1.tif → Copy of Nebraska_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Nebraska_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Minnesota_AEI_binary_0_1.tif → Copy of Minnesota_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Minnesota_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Michigan_AEI_binary_0_1.tif → Copy of Michigan_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Michigan_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of New_Hampshire_AEI_binary_0_1.tif → Copy of New_Hampshire_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of New_Hampshire_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Nevada_AEI_binary_0_1.tif → Copy of Nevada_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Nevada_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Maryland_AEI_binary_0_1.tif → Copy of Maryland_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Maryland_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Massachusetts_AEI_binary_0_1.tif → Copy of Massachusetts_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Massachusetts_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Louisiana_AEI_binary_0_1.tif → Copy of Louisiana_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Louisiana_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Indiana_AEI_binary_0_1.tif → Copy of Indiana_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Indiana_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Maine_AEI_binary_0_1.tif → Copy of Maine_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Maine_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Georgia_AEI_binary_0_1.tif → Copy of Georgia_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Georgia_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Illinois_AEI_binary_0_1.tif → Copy of Illinois_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Illinois_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Idaho_AEI_binary_0_1.tif → Copy of Idaho_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Idaho_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Kentucky_AEI_binary_0_1.tif → Copy of Kentucky_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Kentucky_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Florida_AEI_binary_0_1.tif → Copy of Florida_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Florida_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Iowa_AEI_binary_0_1.tif → Copy of Iowa_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Iowa_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Delaware_AEI_binary_0_1.tif → Copy of Delaware_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Delaware_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Arkansas_AEI_binary_0_1.tif → Copy of Arkansas_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Arkansas_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of California_AEI_binary_0_1.tif → Copy of California_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of California_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Arizona_AEI_binary_0_1.tif → Copy of Arizona_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Arizona_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Connecticut_AEI_binary_0_1.tif → Copy of Connecticut_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Connecticut_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Colorado_AEI_binary_0_1.tif → Copy of Colorado_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Colorado_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Vermont_AEI_binary_0_1.tif → Copy of Vermont_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Vermont_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Mississippi_AEI_binary_0_1.tif → Copy of Mississippi_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Mississippi_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "=== Smoothing Copy of Kansas_AEI_binary_0_1.tif → Copy of Kansas_AEI_binary_0_1_MAJ_k9_p50.tif ===\n",
      "   ✓ Wrote Binary_MAJ/Copy of Kansas_AEI_binary_0_1_MAJ_k9_p50.tif\n",
      "\n",
      "✅ Done smoothing all AEI binary maps (majority filter).\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Post-processing for AEI per-admin binary masks.\n",
    "\n",
    "Step 2 after running `aei_binarize_per_admin_from_shapefile`:\n",
    "\n",
    "  - Take each *_AEI_binary_0_1.tif in\n",
    "        ROOT / CountryModelPredicted_Cropland / Probability / Binary\n",
    "  - Apply a window-based majority filter (3*3 windows on the 0/1 binary mask)\n",
    "    in a streaming / tile-based fashion.\n",
    "  - Preserve NaNs from the input.\n",
    "  - Write smoothed 0/1 binary TIFFs to:\n",
    "        ROOT / CountryModelPredicted_Cropland / Probability / Binary_MAJ\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- This does NOT change the original AEI thresholding step; it only smooths\n",
    "  the resulting binary maps, so exact per-admin AEI may change slightly.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.ndimage import uniform_filter   # pip install scipy\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "ROOT_FOLDER_ID      = os.environ.get(\"ROOT_FOLDER_ID\", \"1hqMIyDYEFKnpS8KLxC4bqHmF_9dHXImG\")\n",
    "PARENT_FOLDER_NAME  = \"CountryModelPredicted_Cropland\"\n",
    "PROB_SUBFOLDER_NAME = \"Probability\"   #=\n",
    "BINARY_FOLDER_NAME  = \"Binary\"        # input binaries from AEi calibrated postprocessing step\n",
    "SMOOTH_FOLDER_NAME  = \"Binary_MAJ\"    # output smoothed binaries\n",
    "\n",
    "TILE                = 1024            # I/O tile size\n",
    "KERNEL_PX           = 3              # majority window (odd, in pixels, e.g.3*3  at 30 m res)\n",
    "MAJ_THR             = 0.50            # majority threshold (>= 50% neighbors == 1)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid:\n",
    "        return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        tkey = re.sub(r\"\\s+\", \"\", f.get(\"title\", \"\").lower())\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    if res:\n",
    "        return res[0][\"id\"]\n",
    "    nf = drive.CreateFile({\"title\": name, \"parents\":[{\"id\": parent_id}], \"mimeType\":\"application/vnd.google-apps.folder\"})\n",
    "    nf.Upload()\n",
    "    return nf[\"id\"]\n",
    "\n",
    "def download_to_temp(drive_file, suffix):\n",
    "    p = tempfile.NamedTemporaryFile(delete=False, suffix=suffix).name\n",
    "    drive_file.GetContentFile(p)\n",
    "    return p\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\":[{\"id\": parent_id}]})\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "# ---------------------- Raster helpers -------------------\n",
    "def _iter_tiles(H, W, tile=TILE):\n",
    "    for r0 in range(0, H, tile):\n",
    "        for c0 in range(0, W, tile):\n",
    "            h = min(tile, H - r0)\n",
    "            w = min(tile, W - c0)\n",
    "            yield Window(c0, r0, w, h)\n",
    "\n",
    "# ---------------------- Majority smoothing ---------------\n",
    "\n",
    "def _smooth_binary_stream(src_path, dst_path, kernel_px=KERNEL_PX, maj_thr=MAJ_THR):\n",
    "    \"\"\"\n",
    "    Read a float32 0/1/NaN binary raster in tiles and write a smoothed\n",
    "    0/1/NaN raster using a window-based majority filter.\n",
    "\n",
    "    - Input: float32, nodata = NaN, values 0 or 1 elsewhere.\n",
    "    - Output: float32, nodata = NaN, values 0 or 1 elsewhere.\n",
    "    \"\"\"\n",
    "    r = kernel_px // 2\n",
    "    win_area = float(kernel_px * kernel_px)\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        profile = src.profile.copy()\n",
    "        # keep same geo/tiling; ensure float32 + NaN nodata\n",
    "        profile.update(\n",
    "            dtype=\"float32\",\n",
    "            count=1,\n",
    "            nodata=np.nan,\n",
    "            compress=\"LZW\",\n",
    "            tiled=True,\n",
    "            blockxsize=512,\n",
    "            blockysize=512,\n",
    "            BIGTIFF=\"IF_NEEDED\"\n",
    "        )\n",
    "\n",
    "        with rasterio.open(dst_path, \"w\", **profile) as dst:\n",
    "            for _, w in src.block_windows(1):\n",
    "                # expand window by halo for neighborhood support\n",
    "                r0 = max(0, w.row_off - r)\n",
    "                c0 = max(0, w.col_off - r)\n",
    "                r1 = min(src.height, w.row_off + w.height + r)\n",
    "                c1 = min(src.width,  w.col_off + w.width  + r)\n",
    "                if (c1 - c0) <= 0 or (r1 - r0) <= 0:\n",
    "                    # nothing there\n",
    "                    dst.write(np.full((w.height, w.width), np.nan, dtype=np.float32), 1, window=w)\n",
    "                    continue\n",
    "                win_pad = Window(c0, r0, c1 - c0, r1 - r0)\n",
    "\n",
    "                a = src.read(1, window=win_pad, masked=True)\n",
    "                data = a.data\n",
    "                # valid where not masked AND finite\n",
    "                valid = (~a.mask) & np.isfinite(data)\n",
    "                if not valid.any():\n",
    "                    dst.write(np.full((w.height, w.width), np.nan, dtype=np.float32), 1, window=w)\n",
    "                    continue\n",
    "\n",
    "                # current binary: anything > 0.5 treated as 1\n",
    "                base = valid & (data > 0.5)\n",
    "                base_f = base.astype(np.float32)\n",
    "                valf   = valid.astype(np.float32)\n",
    "\n",
    "                # neighborhood counts via uniform_filter\n",
    "                sum_ones  = uniform_filter(base_f, size=kernel_px, mode=\"constant\", cval=0.0) * win_area\n",
    "                cnt_valid = uniform_filter(valf,   size=kernel_px, mode=\"constant\", cval=0.0) * win_area\n",
    "                frac = np.divide(sum_ones, cnt_valid, out=np.zeros_like(sum_ones), where=(cnt_valid > 0))\n",
    "\n",
    "                # majority decision (only where we have valid neighbors)\n",
    "                smoothed = np.zeros_like(base, dtype=bool)\n",
    "                has_nb = (cnt_valid > 0)\n",
    "                smoothed[has_nb] = frac[has_nb] >= maj_thr\n",
    "\n",
    "                # crop back to original tile window\n",
    "                rs = w.row_off - r0\n",
    "                cs = w.col_off - c0\n",
    "                re = rs + w.height\n",
    "                ce = cs + w.width\n",
    "\n",
    "                valid_core    = valid[rs:re, cs:ce]\n",
    "                smooth_core   = smoothed[rs:re, cs:ce]\n",
    "\n",
    "                out_block = np.full((w.height, w.width), np.nan, dtype=np.float32)\n",
    "                out_block[valid_core & smooth_core]  = 1.0\n",
    "                out_block[valid_core & ~smooth_core] = 0.0\n",
    "\n",
    "                dst.write(out_block, 1, window=w)\n",
    "\n",
    "# ---------------------- Main driver ----------------------\n",
    "\n",
    "def smooth_aei_binary_maps(drive):\n",
    "    \"\"\"\n",
    "    Entry point: locate Binary folder, smooth each AEI binary map,\n",
    "    and write to Binary_MAJ sibling folder.\n",
    "    \"\"\"\n",
    "    # Locate folders\n",
    "    cmp_id = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id:\n",
    "        raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "\n",
    "    prob_id = get_subfolder_fuzzy(drive, cmp_id, PROB_SUBFOLDER_NAME)\n",
    "    if not prob_id:\n",
    "        raise RuntimeError(\"Probability folder not found (tried fuzzy match).\")\n",
    "\n",
    "    binary_id = get_subfolder_fuzzy(drive, prob_id, BINARY_FOLDER_NAME)\n",
    "    if not binary_id:\n",
    "        raise RuntimeError(\"Binary folder (with AEI binaries) not found under Probability.\")\n",
    "\n",
    "    smooth_id = get_or_create_folder(drive, prob_id, SMOOTH_FOLDER_NAME)\n",
    "\n",
    "    # List binary TIFFs\n",
    "    files = [\n",
    "        it for it in list_files(drive, binary_id)\n",
    "        if isinstance(it, dict)\n",
    "        and it.get(\"mimeType\") != \"application/vnd.google-apps.folder\"\n",
    "        and it.get(\"title\", \"\").lower().endswith((\".tif\", \".tiff\"))\n",
    "    ]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No binary TIFFs in Binary folder.\")\n",
    "\n",
    "    print(f\"Found {len(files)} binary rasters in Binary/\")\n",
    "\n",
    "    for it in files:\n",
    "        title = it.get(\"title\", \"\")\n",
    "        base  = re.sub(r\"\\.tif(f)?$\", \"\", title, flags=re.IGNORECASE)\n",
    "        out_name = f\"{base}_MAJ_k{KERNEL_PX}_p{int(MAJ_THR*100)}.tif\"\n",
    "\n",
    "        print(f\"\\n=== Smoothing {title} → {out_name} ===\")\n",
    "        tmp_in  = download_to_temp(it, \".tif\")\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\") as tmp_out:\n",
    "            out_local = tmp_out.name\n",
    "\n",
    "        _smooth_binary_stream(tmp_in, out_local, kernel_px=KERNEL_PX, maj_thr=MAJ_THR)\n",
    "        upload_path(drive, out_local, smooth_id, title=out_name)\n",
    "\n",
    "        # cleanup\n",
    "        try:\n",
    "            os.remove(tmp_in)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(out_local)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(f\"   ✓ Wrote {SMOOTH_FOLDER_NAME}/{out_name}\")\n",
    "\n",
    "    print(\"\\n✅ Done smoothing all AEI binary maps (majority filter).\")\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\")\n",
    "    smooth_aei_binary_maps(drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d5924-12c2-4b58-99ad-3f0c7b343e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/4158/.local/lib/python3.11/site-packages/rasterio/merge.py:217: RasterioDeprecationWarning: The precision parameter is unused, deprecated, and will be removed in 2.0.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged 48 rasters → Binary/US_binary.tif\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Merge all per-country AEI binary rasters in Drive → CountryModelPredicted/Probability/Binary/\n",
    "into a single global binary mosaic (union = max), preserving NaNs.\n",
    "\n",
    "Output: Merged_AEI_binary_0_1.tif in the same Binary folder.\n",
    "\"\"\"\n",
    "\n",
    "import os, re, tempfile, warnings\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.merge import merge as rio_merge\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "ROOT_FOLDER_ID      = os.environ.get(\"ROOT_FOLDER_ID\", \"1hqMIyDYEFKnpS8KLxC4bqHmF_9dHXImG\")\n",
    "PARENT_FOLDER_NAME  = \"CountryModelPredicted_Cropland\"\n",
    "PROB_SUBFOLDER_NAME = \"Probability\"   \n",
    "BINARY_SUBFOLDER    = \"Binary_MAJ\"\n",
    "OUT_NAME            = \"US_binary.tif\"\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ---------------------- Drive helpers ---------------------\n",
    "def _dq(drive, q):\n",
    "    return drive.ListFile({\n",
    "        \"q\": q,\n",
    "        \"supportsAllDrives\": True,\n",
    "        \"includeItemsFromAllDrives\": True,\n",
    "        \"maxResults\": 1000\n",
    "    }).GetList()\n",
    "\n",
    "def list_files(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false\")\n",
    "\n",
    "def child_folders(drive, parent_id):\n",
    "    return _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder'\")\n",
    "\n",
    "def get_subfolder_exact(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    return res[0][\"id\"] if res else None\n",
    "\n",
    "def get_subfolder_fuzzy(drive, parent_id, desired):\n",
    "    eid = get_subfolder_exact(drive, parent_id, desired)\n",
    "    if eid: return eid\n",
    "    key = re.sub(r\"\\s+\", \"\", desired.lower())\n",
    "    for f in child_folders(drive, parent_id):\n",
    "        tkey = re.sub(r\"\\s+\", \"\", f.get(\"title\",\"\").lower())\n",
    "        if key in tkey or (\"prob\" in key and \"prob\" in tkey):\n",
    "            return f[\"id\"]\n",
    "    return None\n",
    "\n",
    "def get_or_create_folder(drive, parent_id, name):\n",
    "    res = _dq(drive, f\"'{parent_id}' in parents and trashed=false and mimeType='application/vnd.google-apps.folder' and title='{name}'\")\n",
    "    if res: return res[0][\"id\"]\n",
    "    nf = drive.CreateFile({\"title\": name, \"parents\":[{\"id\": parent_id}], \"mimeType\":\"application/vnd.google-apps.folder\"})\n",
    "    nf.Upload()\n",
    "    return nf[\"id\"]\n",
    "\n",
    "def download_to_temp(drive_file, suffix):\n",
    "    p = tempfile.NamedTemporaryFile(delete=False, suffix=suffix).name\n",
    "    drive_file.GetContentFile(p)\n",
    "    return p\n",
    "\n",
    "def upload_path(drive, local_path, parent_id, title=None):\n",
    "    f = drive.CreateFile({\"title\": title or os.path.basename(local_path), \"parents\":[{\"id\": parent_id}]})\n",
    "    f.SetContentFile(local_path)\n",
    "    f.Upload()\n",
    "    return f[\"id\"]\n",
    "\n",
    "# ---------------------- Merge helpers ---------------------\n",
    "def _try_merge_with_max(srcs):\n",
    "    \"\"\"Preferred path (newer rasterio): method='max', nodata=np.nan.\"\"\"\n",
    "    return rio_merge(\n",
    "        srcs,\n",
    "        nodata=np.nan,\n",
    "        dtype=\"float32\",\n",
    "        precision=7,\n",
    "        resampling=Resampling.nearest,\n",
    "        method=\"max\",\n",
    "    )\n",
    "\n",
    "def _try_merge_basic(srcs):\n",
    "    \"\"\"Older rasterio: no 'method'. We'll nanmax ourselves after merging.\"\"\"\n",
    "    mosaic, out_transform = rio_merge(\n",
    "        srcs,\n",
    "        nodata=np.nan,\n",
    "        dtype=\"float32\",\n",
    "        precision=7,\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "    # emulate union across sources (elementwise maximum, ignoring NaNs)\n",
    "    mosaic = np.nanmax(mosaic, axis=0, keepdims=True).astype(\"float32\")\n",
    "    return mosaic, out_transform\n",
    "\n",
    "def _try_merge_basic_sentinel(srcs, sentinel=-9999.0):\n",
    "    \"\"\"Very old rasterio: nodata cannot be NaN. Use sentinel then convert and nanmax.\"\"\"\n",
    "    mosaic, out_transform = rio_merge(\n",
    "        srcs,\n",
    "        nodata=sentinel,\n",
    "        dtype=\"float32\",\n",
    "        precision=7,\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "    # convert sentinel to NaN\n",
    "    mosaic = mosaic.astype(\"float32\", copy=False)\n",
    "    mosaic[mosaic == sentinel] = np.nan\n",
    "    mosaic = np.nanmax(mosaic, axis=0, keepdims=True).astype(\"float32\")\n",
    "    return mosaic, out_transform\n",
    "\n",
    "# ---------------------- Main merge ------------------------\n",
    "def merge_all_binary_rasters(drive):\n",
    "    # Locate folders\n",
    "    cmp_id     = get_subfolder_fuzzy(drive, ROOT_FOLDER_ID, PARENT_FOLDER_NAME)\n",
    "    if not cmp_id: raise RuntimeError(f\"Folder '{PARENT_FOLDER_NAME}' not found under ROOT.\")\n",
    "    prob_id    = get_subfolder_fuzzy(drive, cmp_id, PROB_SUBFOLDER_NAME)\n",
    "    if not prob_id: raise RuntimeError(\"Probability folder not found (tried fuzzy match).\")\n",
    "    binary_id  = get_subfolder_fuzzy(drive, prob_id, BINARY_SUBFOLDER)\n",
    "    if not binary_id:\n",
    "        binary_id = get_or_create_folder(drive, prob_id, BINARY_SUBFOLDER)\n",
    "\n",
    "    # Find all binary GeoTIFFs (skip the mosaic itself if re-running)\n",
    "    tifs = [it for it in list_files(drive, binary_id)\n",
    "            if isinstance(it, dict)\n",
    "            and it.get(\"mimeType\") != \"application/vnd.google-apps.folder\"\n",
    "            and it.get(\"title\",\"\").lower().endswith((\".tif\",\".tiff\"))\n",
    "            and OUT_NAME.lower() not in it.get(\"title\",\"\").lower()]\n",
    "    if not tifs:\n",
    "        raise FileNotFoundError(\"No binary .tif files found in Binary/.\")\n",
    "\n",
    "    # Download & open datasets\n",
    "    local_paths, srcs = [], []\n",
    "    try:\n",
    "        for it in tifs:\n",
    "            p = download_to_temp(it, \".tif\")\n",
    "            local_paths.append(p)\n",
    "            srcs.append(rasterio.open(p))\n",
    "\n",
    "        # Try modern merge with 'method=max' → else fallback strategies\n",
    "        try:\n",
    "            mosaic, out_transform = _try_merge_with_max(srcs)\n",
    "        except TypeError:\n",
    "            # 'method' not supported\n",
    "            try:\n",
    "                mosaic, out_transform = _try_merge_basic(srcs)\n",
    "            except Exception:\n",
    "                mosaic, out_transform = _try_merge_basic_sentinel(srcs)\n",
    "        except Exception:\n",
    "            # Any other unexpected error → robust fallback\n",
    "            try:\n",
    "                mosaic, out_transform = _try_merge_basic(srcs)\n",
    "            except Exception:\n",
    "                mosaic, out_transform = _try_merge_basic_sentinel(srcs)\n",
    "\n",
    "        # Build output profile from first raster\n",
    "        ref = srcs[0]\n",
    "        out_profile = ref.profile.copy()\n",
    "        out_profile.update(\n",
    "            driver=\"GTiff\",\n",
    "            height=mosaic.shape[1],\n",
    "            width=mosaic.shape[2],\n",
    "            transform=out_transform,\n",
    "            count=1,\n",
    "            dtype=\"float32\",\n",
    "            nodata=np.nan,\n",
    "            compress=\"LZW\",\n",
    "            tiled=True,\n",
    "            blockxsize=512,\n",
    "            blockysize=512,\n",
    "            BIGTIFF=\"IF_NEEDED\"\n",
    "        )\n",
    "\n",
    "        # Write to temp, then upload\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".tif\") as tmp_out:\n",
    "            out_local = tmp_out.name\n",
    "\n",
    "        with rasterio.open(out_local, \"w\", **out_profile) as dst:\n",
    "            dst.write(mosaic[0], 1)\n",
    "\n",
    "        upload_path(drive, out_local, binary_id, title=OUT_NAME)\n",
    "        try: os.remove(out_local)\n",
    "        except: pass\n",
    "\n",
    "        print(f\"✅ Merged {len(srcs)} rasters → Binary/{OUT_NAME}\")\n",
    "\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        for s in srcs:\n",
    "            try: s.close()\n",
    "            except: pass\n",
    "        for p in local_paths:\n",
    "            try: os.remove(p)\n",
    "            except: pass\n",
    "\n",
    "# ---------------------- CLI ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        drive  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"PyDrive2 'drive' not found. Authenticate and expose a global `drive` before running.\")\n",
    "    merge_all_binary_rasters(drive)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
